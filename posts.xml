<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>TMCF 2024</title>
<link>https://theory4ida.github.io/tmcf/posts.html</link>
<atom:link href="https://theory4ida.github.io/tmcf/posts.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.554</generator>
<lastBuildDate>Mon, 24 Jun 2024 23:00:00 GMT</lastBuildDate>
<item>
  <title>Some risks and opportunities of automated data analysis</title>
  <dc:creator>Daniel Archambault</dc:creator>
  <dc:creator>Roger Beecham</dc:creator>
  <dc:creator>Andrew Gelman</dc:creator>
  <dc:creator>Jessica Hullman</dc:creator>
  <dc:creator>Edwin Pos</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/01-models/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>We have all been there: you have your research question, you’ve gathered your dataset, and you are ready to start digging. Or perhaps you only have a question and are wondering how you could answer it in the first place given all the options available. Ever since the invention of data analysis, the human analyst has held the primary role of thinking, deciding and analyzing. At the same time, there has been a steady progression toward automating parts of statistical workflow that are better done by machines, such as calculation. With impressive recent advancements in general purpose intelligent assistants like chatbots, we are in a better position than ever to imagine what an intelligent assistant who acts as a helpful collaborator during data analysis might look like. These new possibilities provide an opportunity to reflect on the ideal interaction between statistical tools and human knowledge. What are the risks involved with increasing amounts of automation in data analysis steps like problem specification, data collection, model specification and selection, and interpretation of results? What are the opportunities? How can we ensure that the sum of human knowledge and AI prediction in data analysis will be more than its parts?</p>
<p>To formalize the problem slightly, one could say that data analysis has traditionally involved applying some sequence of operations on data (e.g., reductions or visualizations) which we shall express as functions <img src="https://latex.codecogs.com/png.latex?f%20=%20(f_1,%20f_2,%20%E2%80%A6,%20f_n)"> to data <img src="https://latex.codecogs.com/png.latex?D">, each of which generates some intermediate output <img src="https://latex.codecogs.com/png.latex?y_i%20=%20f_i(D)"> to produce some ultimate knowledge output or interpretation <img src="https://latex.codecogs.com/png.latex?K%20=%20f"><img src="https://latex.codecogs.com/png.latex?_%7Bhuman%7D(y)">, where <img src="https://latex.codecogs.com/png.latex?f_%7Bhuman%7D"> represents a human processing <img src="https://latex.codecogs.com/png.latex?y%20=%20(y_1,%20y_2,%20%E2%80%A6,%20y_n)">. For example, a conventional visual analytics workflow involves a human selecting and applying to data some set of queries or operations <img src="https://latex.codecogs.com/png.latex?f(D)"> (which might consist of filters, aggregations, regressions, etc.), then interpreting the output of these functions to produce some interpretation or decision <img src="https://latex.codecogs.com/png.latex?K">.</p>
<p>As we consider how this simple formulation changes with increasing levels of automation, we propose this gives rise to a spectrum. On the one end you have a human-directed extreme, such as in conventional visual analytics (Keim et al.&nbsp;2008), where the human has chosen the specific functional form of the members of <img src="https://latex.codecogs.com/png.latex?f"> to apply to the data given their prior knowledge to find <img src="https://latex.codecogs.com/png.latex?K">, and automation is involved primarily to perform what would otherwise be tedious calculations.</p>
<p>In the current age of technological development, where products like ChatGPT are <a href="https://help.openai.com/en/articles/8437071-data-analysis-with-chatgpt">promoted for data analysis</a>, from insight discovery to communication, we are closer to being able to imagine the other extreme of this spectrum: fully automated data analysis. In this case, the human only asks the question or provides a goal and optionally some data <img src="https://latex.codecogs.com/png.latex?D">. The remainder of the process is automated, concluding with the machine presenting its interpretation <img src="https://latex.codecogs.com/png.latex?K"> (i.e., <img src="https://latex.codecogs.com/png.latex?K=f_%7Bassistent%7D%20(y)">). The human is not directly involved in selecting <img src="https://latex.codecogs.com/png.latex?f"> nor in processing the intermediate outputs of <img src="https://latex.codecogs.com/png.latex?y">. Consequently, the selected f may not be interpretable to a human in the sense of being explainable via human-meaningful parameters.</p>
<p>Although it may seem to some readers to be immediately apparent that removing the human from data science is a bad idea, it is not at all clear that the current extent of automation in data analysis workflows is sufficient. Integrating more machine intelligence may help address human limitations we have had no choice but to accept in data analysis. For example, the human will be limited by their own experience, and may only be able to see a limited set of all the possible operations or functions that could be applied. Or, information processing biases may lead them to misperceive <img src="https://latex.codecogs.com/png.latex?y">, resulting in a non-optimal <img src="https://latex.codecogs.com/png.latex?K"> even if their selection of <img src="https://latex.codecogs.com/png.latex?f"> was in fact optimal. There would seem to be many ways that using AI could improve analysis outputs in light of the limited knowledge and experience that any human analyst might have.</p>
<p>We propose that thinking about the spectrum from fully human-driven to fully machine-driven analysis is helpful in several ways. First, by explicitly reflecting on what the ideal role of the human is in data analysis, we gain insight into what aspects of data analysis we think are truly human or non-automatable, and which are better given to the machine. Second, as research and practice increasingly makes use of predictive modeling, we are in a better position to apply our expertise on statistical modeling to design better futures for interactive data analysis. Below we explore this spectrum from its two extremes.</p>
</section>
<section id="the-human-directed-extreme" class="level2">
<h2 class="anchored" data-anchor-id="the-human-directed-extreme">The human-directed extreme</h2>
<p>At one end of the spectrum, the human selects the functions to be applied, processes the outputs of those functions more or less manually (i.e., without necessarily using formal decision rules), and maps the beliefs that they form to an interpretation. We assume that the human has access to visual analytics processes and technology (Keim et al., 2008). By this we mean that the human does not perform the calculation themselves (we aren’t worried about mathematical errors in generating the individual <img src="https://latex.codecogs.com/png.latex?i">), and they have access to visual analytics systems that perform the calculations and present the answers in a visual form for interpretation through <img src="https://latex.codecogs.com/png.latex?f_%7Bhuman%7D">. We assume the human has some level of understanding of how the particular set of functions work and what they mean. In other words, <img src="https://latex.codecogs.com/png.latex?f"> consists of interpretable operations, such as parameterized functions <img src="https://latex.codecogs.com/png.latex?f_%7Btheta%7D"> where theta is considered meaningful to the human, or simple deterministic functions like finding the maximum of data series. It is worth noting that despite the interpretability of the operations applied, how exactly the human processes the available information (i.e., <img src="https://latex.codecogs.com/png.latex?f_%7Bhuman%7D">) can be thought of as a black box of sorts, as the generation of <img src="https://latex.codecogs.com/png.latex?y"> is guided implicitly by the previous experience and domain specific knowledge of the human in addition to the data. It is assumed that the human knows how to best apply their domain knowledge in this scenario.</p>
<p>We can loosely analogize the learning problem to Bayesian decision theory for the purpose of identifying failure points. Assume some high level learning goal given some observed dataset; for example, an analyst hired by a large school district might be investigating the question, What factors explain the dip in high school math performance between 2018 and 2020? While normally in applied Bayesian analysis we would focus on how posterior beliefs about the some parameters deemed meaningful are arrived at within the context of some particular model specification, here we will instead think of the knowledge gained from the entire analysis workflow as resulting from some combination of the analyst’s prior knowledge and beliefs, the data at hand, and modeling assumptions. Assume for example that what the analyst finds is intended to inform the school district’s decision about what to invest next year’s budget in. The human analyst will likely bring some relevant domain knowledge (prior beliefs) on which variables are likely to matter and how much, how they should model the data to achieve their goals, what aspects of context should influence their interpretation, etc. For example, maybe they have prior experience on factors that predict high school test scores, biases that exist in available data, contextual knowledge about the specific school district, etc. What they know about such problems will influence the statistical modeling approach they select (e.g., a multiple regression predicting 11th grade standardized math test performance from some particular set of covariates). The output of the models and operations they apply inform their posterior beliefs about which factors influence student performance and how this information should inform a decision.</p>
<p>If the analyst were fully Bayesian, they would optimally combine the new information they learn with their prior beliefs to arrive at posterior beliefs, then select the utility maximizing interpretation or decision from a space of possible decisions. But a number of things can go wrong when dealing with boundedly rational agents. The analyst’s prior knowledge itself might be biased or incomplete, leading them to analyze the wrong data, fail to select an appropriate set of operations <img src="https://latex.codecogs.com/png.latex?f">, etc. They might make bad assumptions in model specification or selection, leading to misleading modeling outputs. They might overconstrain their analysis based on their priors (or discount them too easily) by taking advantage of degrees of freedom, for example if they are predisposed to prefer a certain interpretation leading to biased beliefs they might settle on a particular fitted model because it aligns with these preferences. They might arrive at the wrong interpretation or decision in light of their beliefs.</p>
<p>On the other hand, there may be advantages to the fully human case. The human understands exactly what was applied in <img src="https://latex.codecogs.com/png.latex?f"> and drives the interpretation of <img src="https://latex.codecogs.com/png.latex?y">, enabling them to apply domain knowledge in a flexible, unscripted way. Their understanding and experience with the functions or models they apply makes it easier for them to debug issues, and they may feel more confident about the interpretation they ultimately arrive at. They have the ability to reformulate the goals and problem upon viewing results at any point, for example if they originally failed to consider some important data. More broadly, if the purpose of most data science is to generate insight from data for human purposes, in the fully human-driven case we are 100% sure that data science is happening as every step was done by and consumed by the human, providing an opportunity for understanding, critique, and reconsideration at each step.</p>
</section>
<section id="the-fully-automated-extreme" class="level2">
<h2 class="anchored" data-anchor-id="the-fully-automated-extreme">The fully automated extreme</h2>
<p>At the fully automated extreme, the human provides an analysis goal and optionally data. When considering the Keim et al.&nbsp;model of visual analytics (Keim et al.&nbsp;2008), the assistant has complete control over all stages, including automated processing of knowledge. In the extreme, the assistant simply returns the answer, <img src="https://latex.codecogs.com/png.latex?K">, without any justification or provenance of the analysis. For the purposes of this post, we assume the assistant optimally combines what can be gleaned from all available prior statistical and scientific modeling examples in an autoregressive framework in which its task is to predict the most appropriate sequence of steps given the human’s prompt.</p>
<p>Several risks arise from the training constraints on the assistant. For example, one risk arises from how the assistant’s suggestions will be constrained by what it has seen, which in the best case consists of all prior observed statistical analyses. The model cannot necessarily suggest an altogether new form of analysis unless it in some way represents a combination of previous observed analyses. This raises the question of whether and when we might expect completely new analysis paradigms to emerge which could not somehow be reconstitutions of existing ideas.</p>
<p>Relatedly, the assistant is constrained to suggesting what is most probable. Ask any statistician if they expect frequencies within a corpus representing all historical examples of statistical analysis to capture the appropriateness of a given analysis path and they will answer with a resounding no. If ritualistic choice of models and interpretation of results is a problem in the fully human directed case, then any agent restricted to optimizing an autoregressive objective over some corpus of training data has the potential to miss something more appropriate but less prevalent in the training data. To some extent modern machine learning pipelines can overcome these biases through processes like fine-tuning, where a small amount of preference data is collected after training an unsupervised foundation model and used to adjust the conditional distribution of the model’s output. However, fine-tuning for particular cases implies a form of interaction that would seem to contradict the extreme fully-automated case: if we expect analyses to improve through interaction to obtain further context- or expert-specific input, we would seem to be advocating for an interactive dialogue between the human user and the assistant.</p>
<p>Relatedly, the fully automated scenario assumes that the human, at the time of prompting, is able to specify their true analysis goal, and in doing so will know how to customize the prompt to contain any relevant domain knowledge they have. But this begins to sound a lot like our trust that the human knows best how to apply their knowledge in the fully human-driven case. Is this reasonable? Or does the real human analyst require intermediate outputs along the way in order to cue lessons from their prior knowledge and experience? Consider how often real analysis workflows involve a shifting of direction. Upon viewing representations of our data in early Exploratory Data Analysis, we might realize our misconceptions of what it contained. Perhaps the fidelity of information we thought we could capture (and which we need to achieve our analysis goals) is simply not possible, and we must rethink our questions altogether. Or, we might realize upon reviewing model diagnostics that there is little signal in the variables we have collected, but have an idea of what other features we could collect. Can we imagine data analysis without such flexibility? Is it possible that an assistant could anticipate such changes in direction? The answer in the extreme case is simply returned without any reason or process to derive the answer, hence no obvious recourse for the human to take further action. Thus, knowledge is gained in some sense if they choose to trust it, but that might be hard.</p>
</section>
<section id="charting-a-generalized-path-through-extremes-of-y" class="level2">
<h2 class="anchored" data-anchor-id="charting-a-generalized-path-through-extremes-of-y">Charting a generalized path through extremes of <img src="https://latex.codecogs.com/png.latex?y"></h2>
<p>The above reflections suggest the ideal amount of machine assistance in data analysis will lie somewhere between the two extremes, and may differ depending on the expertise of the particular human and the specific context. However, we could say that the goal in finding the sweet spot is to identify the points where the human’s imagination holds them back from realizing a better path. If good statistical practice involves judicious use of computation to extend the human’s ability to imagine possible outcomes (e.g., alternative values for a statistic resulting from bias or sampling error, counterfactuals in causal inference, equivalently performing models in machine learning, etc.), a computational assistant who can entertain many models or theories simultaneously provides opportunities to “amplify” human cognition, a stated goal of visualization.</p>
<p>But how much should an assistant prompt the human to imagine outside their comfort zone? Attempting to design the optimal human-machine pairing naturally motivates reflection on the extent to which we want data-driven science to be pluralistic, allowing for different beliefs and conventions when it comes to how to best learn from data. Consider a familiar tension in the field of data analysis between inference, theory and explanation, and prediction. Researchers and practitioners in different fields and domains vary in how much they value each. It would seem that an optimal assistant would need to adjust to the specific problem in ways congruent with domain-specific values. In social and natural science applications, the assistant might, for example, behave like a scientist from those traditions, where the generated y (and consequently <img src="https://latex.codecogs.com/png.latex?K"> derived from <img src="https://latex.codecogs.com/png.latex?y">) is grounded in substantive theory. In Operations research where prediction is typically the goal, we might judge possible outputs purely in terms of best out-of-sample performance, leading to a much more quantitative (minimal) inspection of <img src="https://latex.codecogs.com/png.latex?y"> to arrive at <img src="https://latex.codecogs.com/png.latex?K">.</p>
<p>Paradigmatic differences leave us with many questions around how the assistant should be constrained or configured to realize the goals of scientific knowledge development. We should think about what level of constraints are tolerable and ultimately desirable for learning in a domain, which is a question about how comfortable we are with letting an assistant push us outside our comfort zones as scientists. If the social or natural scientist fine-tunes or otherwise configures the assistant only in a way that reproduces existing practice, the risk is simply reproducing the status quo. What level of “expanded” imagination is desirable for the purpose of scientific progress? For example, should researchers working in domains that prioritize explanatory approaches be given recommendations informed by predictive modeling, as proposed by Hofman et al.&nbsp;(2021) in advocating for integrative modeling? Are there unified goals, constraints or features of analysis – ways of evaluatin and thinking about the y that gives rise to <img src="https://latex.codecogs.com/png.latex?K"> – that are independent of domain, or are we ultimately constrained to work only within a particular modeling paradigm because historically boundaries have existed? To what extent is explicit design toward achieving specific features/goals of scientific analysis and different standards of evidence (Hofman et al., 2021) useful in constraining the assistant?</p>
<p>For example, to what extent should the ideal pairing of the human and machine for data science seek to prioritize:</p>
<ul>
<li><strong>Substantive theory</strong> – Extent to which the <img src="https://latex.codecogs.com/png.latex?K"> that is derived supports, validates and extends knowledge.</li>
<li><strong>Reproducibility and transfer</strong> – Extent to which <img src="https://latex.codecogs.com/png.latex?K"> is reproducible - for example, is it invariant under perturbations we do not believe should substantively change results, such as slight variations in how the assistant is called? Notions of replicability may also matter: when should a closely related analysis on similar datasets in the same domain produce an analogous result?<br>
</li>
<li><strong>Transparency</strong> – Extent to which <img src="https://latex.codecogs.com/png.latex?K"> that can be easily understood and interrogated.</li>
<li><strong>Coverage and generalizability</strong> – Extent to which <img src="https://latex.codecogs.com/png.latex?K"> encompasses a narrow or wide range of settings (contexts, scales, etc.).</li>
<li><strong>Expansiveness</strong> – Extent to which <img src="https://latex.codecogs.com/png.latex?K"> extends imagination, or that enables the analyst to push beyond bounds of inherited modeling and data paradigms.</li>
</ul>
<p>Different domains and use cases might naturally have different weightings on each of these features, leading to different modeling paradigms. Where do we find ourselves on the spectrum, and to what extent might we expect an intelligent assistant to be informed by knowledge of other points along it, so as to push analysts in a given domain outside of their comfort zone?</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>There are many other risks and benefits of the human-directed and automated extremes than those discussed above. However, several themes arise even from our partial treatment. One is that it seems unlikely that the optimal analysis approach is either fully human-directed (with the primary form of automation being calculation as assumed above) or nearly fully automated, with the human providing only the high level goal and optionally, some data. Without any visibility into how <img src="https://latex.codecogs.com/png.latex?K"> was produced, the human has no opportunity to apply knowledge that is not contained in the training data to debug operations chosen by the machine. They may not feel confident in their results, and their lack of insight into how they were reached may prevent them from applying the knowledge that is output, leading to a question of whether it is knowledge at all.</p>
<p>At the same time, a machine that can run many analyses simultaneously, including approaches the human may not know of or be familiar with, has the potential to result in much more informed interpretations of data. There are many blindspots in human analysis. Forms of model multiplicity (the fact that we can get the same model fit or performance from models that imply very different interpretations of a phenomena) and sources of uncertainty (e.g., about how good our assumptions are) are routinely overlooked. In general, it is unlikely that either extreme will be sufficient, prompting a number of questions about how explicitly an analysis assistant should prompt a human analyst to think more expansively than they otherwise might. Key challenges lie in identifying when, how, and why to elicit human knowledge, so as to show the analyst what they may miss, and how to communicate results from potentially non-human interpretable operations in ways that humans can understand. In the current age of technological advancement with increasingly advanced automated methods we expect the sum will indeed be more than its parts.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Hofman, J.M., Watts, D.J., Athey, S. <em>et al.</em> Integrating explanation and prediction in computational social science. <em>Nature</em> 595, 181–188 (2021). doi: <a href="https://doi.org/10.1038/s41586-021-03659-0">10.1038/s41586-021-03659-0</a></p>
<p>Hullman, J., and Gelman, A. (2021). Comparing human to automated statistics. Section 6 of Designing for interactive exploratory data analysis requires theories of graphical inference, Harvard Data Science Review 3 (3).</p>
<p>Hullman, J., Holtzman, A., and Gelman, A. (2023). Artificial intelligence and aesthetic judgment. http://stat.columbia.edu/~gelman/research/unpublished/AI_aesthetic_judgment.pdf</p>
<p>Keim, D., Andrienko, G., Fekete, JD., Görg, C., Kohlhammer, J., Melançon, G. (2008). Visual Analytics: Definition, Process, and Challenges. In: Kerren, A., Stasko, J.T., Fekete, JD., North, C. (eds) Information Visualization. Lecture Notes in Computer Science, vol 4950. Springer, Berlin, Heidelberg. doi: <a href="https://doi.org/10.1007/978-3-540-70956-5_7">10.1007/978-3-540-70956-5_7</a></p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{archambault2024,
  author = {Archambault, Daniel and Beecham, Roger and Gelman, Andrew
    and Hullman, Jessica and Pos, Edwin},
  title = {Some Risks and Opportunities of Automated Data Analysis},
  date = {2024-06-25},
  url = {https://theory4ida.github.io/tmcf/posts/01-models/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-archambault2024" class="csl-entry quarto-appendix-citeas">
Archambault, Daniel, Roger Beecham, Andrew Gelman, Jessica Hullman, and
Edwin Pos. 2024. <span>“Some Risks and Opportunities of Automated Data
Analysis.”</span> June 25, 2024. <a href="https://theory4ida.github.io/tmcf/posts/01-models/">https://theory4ida.github.io/tmcf/posts/01-models/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/01-models/</guid>
  <pubDate>Mon, 24 Jun 2024 23:00:00 GMT</pubDate>
  <media:content url="https://blog.sciencemuseum.org.uk/wp-content/uploads/2013/12/Alan-Turing-29-March-1951-picture-credit-NPL-Archive-Science-Museum1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Navigating the Foggy Garden of Forking Paths</title>
  <dc:creator>Benjamin Bach</dc:creator>
  <dc:creator>Hadley  Wickham</dc:creator>
  <dc:creator>Jo Wood</dc:creator>
  <dc:creator>Kai Xu</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/02-tools/</link>
  <description><![CDATA[ 





<p><img src="https://jwolondon.github.io/forkingPaths/images/gardenPhoto.jpg" class="img-fluid" alt="A garden of forking paths"></p>
<blockquote class="blockquote">
<p>Under the trees of England I meditated on this lost and perhaps mythical labyrinth. I imagined it untouched and perfect on the secret summit of some mountain; I imagined it drowned under rice paddies or beneath the sea; I imagined it infinite, made not only of eight-sided pavilions and of twisting paths but also of rivers, provinces and kingdoms … I thought of a maze of mazes, of a sinuous, ever growing maze which would take in both past and future and would somehow involve the stars.</p>
</blockquote>
<p>— <em>The Garden Of Forking Paths, Jorge Luis Borges, 1941</em></p>
<p><strong>What is a good metaphor for data analysis?</strong> In the spirit of Borges, Pólya, Gelman and many others since, we’re going to run with this one: <em>navigating the forking paths of a foggy garden.</em> It captures our belief that analysis involves deciding between (navigating) a set of discrete decisions (the forks) across different analysis options (the paths) in a continuous landscape of possibility (the garden) that becomes incrementally revealed as the analysis progresses (the fog).</p>
<p>Analysis is not a murder mystery or a detective challenge. Typically there is not one objective Truth to be discovered, but many potential possible discoveries. When presenting a final analysis the paths taken can be just as important as the final destination.</p>
<p>At best, a metaphor is a tool for thinking. We don’t believe that this metaphor captures every aspect of the problem at hand, but perhaps it illuminates components to guide the analyst through their workflow, encouraging critical thinking, suggesting where future tools may assist. We are also cautious: a danger with any metaphor is that it replaces something we don’t fully understand with something we think we do. It may lead to hubris. Nevertheless, stick with us as we dig a little deeper and consider what it might bring to our understanding of the data science process.</p>
<section id="interrogating-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="interrogating-the-metaphor">Interrogating the metaphor</h2>
<p>We’ll begin by exploring five components of the metaphor (navigation, the fog, the garden, the forks, and the paths), how it ties to the practice of data analysis, where it suggests interesting angles to consider, and where it might constrain our thinking.</p>
<section id="navigating" class="level3">
<h3 class="anchored" data-anchor-id="navigating">Navigating</h3>
<p>To perform an analysis we navigate the pathways, making decisions as we explore the problem. We navigate in different ways at different times. Often we will be walking an analysis, plodding from one foot to the other as each step reveals a little more of the path. But at times we may be running in as we enter a flow state or repeat a well-used protocol. Or we may take intuitive leaps that transport us to quite different parts of the garden. Or perhaps we are <em>flaneurs</em>, wandering for its own sake, with no purpose other than to subject ourselves to the whims of serendipity.</p>
<p>Regardless of how we travel, it is a rare analysis that proceeds in a straight line along a pre-determined route. Much real analysis involves exploration – winding paths with many wrong turns and dead ends. Even decisions late in the analysis can provide surprising insights that lead us to reconsider our initial steps.</p>
<p>Once we have done our analysis, getting to a point of interest after taking many wrong turns, we may choose to describe our journey to others. Perhaps we focus only on the start and destination. Or perhaps we write a guided tour (for example as an academic publication), quickly leading the reader to the points of interest while gliding past the less interesting scenery. Scientific honesty may compel us to provide more of a travelogue: did we know what we were looking for before we set out, and how long did we spend hunting for something of interest? Or we might reflect on our own journey as a reflexive exercise; would we take the same paths if we were to start again?</p>
</section>
<section id="fog" class="level3">
<h3 class="anchored" data-anchor-id="fog">Fog</h3>
<p>A fog implies we can’t see much of the garden from our current location: we have to make decisions and choose paths in order to reveal future choices. Our decisions are guided by incomplete information leading us frequently to backtrack; often a path that looked promising at first glance is later revealed to be a dead end. We might need to map our passage as we proceed to avoid getting lost. Or perhaps we rely on maps to unseen destinations left by others.</p>
<p>The fog is not constant; in some places it is thicker, giving us little clue about where to travel next. Experience may give our eyes more power to penetrate the fog, as past analyses help us better predict which paths are likely to lead to bountiful places.</p>
<p>Some destinations will be obvious from a distance; tall trees reaching above the fog. Others will be serendipitous, stumbled upon, fallen from the sky, and discovered among the bushes.</p>
</section>
<section id="garden" class="level3">
<h3 class="anchored" data-anchor-id="garden">Garden</h3>
<p>The garden implies a space that’s a mix of the human and natural, tempting us with bounty to be discovered. Depending on what you are picturing, that bounty might be a succulent tomato in a vegetable garden; an attractive fountain in a formal garden; the symmetrical layout of Italianate beds; or a prize statue at the centre of a hedge maze. Do these metaphorical objects help frame and evaluate our objectives and discoveries?</p>
<p>What role does the topography of the garden play? Perhaps ‘height’ is the optimality of the analysis, where the shape of the garden landscape captures the effectiveness of the underlying analysis. Is the garden flat with a multitude of minor undulations? Is it tended on a mountain, where no matter where you start, there’s an obvious destination at the summit? Or does this garden span a mountain range with multiple peaks connected by ridges and passes, separated by deep ravines.</p>
<p>But in a foggy landscape we may not have a clear notion of ‘optimal’. Perhaps height could mean something else. Leaning on the metaphor, gradient could represent the ease with which we can progress. Moving downhill follows established good practice, gathering momentum from the familiar. Perhaps the tools we use lead us downhill, affordances that encourage some approaches at the expense of others. Or perhaps the fog is less dense in the heights so that travelling uphill allows us to see further.</p>
<p>The notion of landscape emphasises continuity. Perhaps it is unhelpful to think of discrete decision-making; instead we have at our disposal choices from an infinite range of possibilities. We might continuously vary some model parameters and describe the effect on our journeys. Sensitivity analysis might describe how critically dependent our location in the garden is on how we move.</p>
</section>
<section id="forking" class="level3">
<h3 class="anchored" data-anchor-id="forking">Forking</h3>
<p>A forking path in garden of possibility implies there are choices to be made, and that those choices are discrete and finite. Let’s put aside for the moment that we could freely wander about the garden and instead we will keep off the grass.</p>
<p>The existence of a network of paths implies they have some topology. We suggest that understanding that topology is itself instructive. For example, those paths might form a <em>tree</em>: A single start point leading to multiple possible outcomes. At each stage in the decision-making process, more choices are opened up. Alternatively, that tree might be rooted in a single outcome where multiple start points lead to the same destination. Such a topology might give us confidence that our conclusions are robust; that even with changes in methodological decision-making, all paths lead to the same result. But perhaps such an arrangement should signal caution; that our methods are too restrictive, leading only to a limited set of possible outcomes regardless of the data that informs them.</p>
<p>The network of forking paths may form non tree-like patterns. For example, cycles that take us back to previous decision points as part of an iterative process. Or perhaps we have unconnected paths, ones that exist in the same garden of possibility but where we fail to make connections between them.</p>
<p>An understanding of the network of forking paths may lead to insight not just about the outcomes, but the process followed and the way it shapes the validity of findings. Perhaps these are oversimplistic or poorly fitting characterisations of different forking paths. We are intrigued by the possibility that the structure of these networks of choices are instructive in reflecting on the data science workflow by providing a rich and shareable account of the analytic process.</p>
</section>
<section id="paths" class="level3">
<h3 class="anchored" data-anchor-id="paths">Paths</h3>
<p>If a decision-point is a fork in a network of data science activity, what is the path? Does the metaphor suggest different types of path whose characterisations result in insight or meaningful consequence?</p>
<p>We take a path to represent some chosen activity, perhaps the cleaning of a dataset, the specification of a visualization design, or the populating of a simulation model. It is what we might traditionally document when explaining our work.</p>
<p>What might the ‘width’ of a path mean? Consider the popularity of a well-trodden path. Perhaps we are following a process adopted by many others, following established good (or even poor) practice. A narrow path might represent a break from convention - activity without precedence, <em>novelty</em> taking us somewhere few have visited before. Or perhaps we are pioneering a new shortcut to a place many others have visited. Characterising path width might help signal to us where we exercise methodological scrutiny. Examining wide paths might prompt us to question convention, or at least its fit to the problem at hand.</p>
<p>Our paths move us through the garden of possibility. A <em>perilous</em> path might be one that traverses a precipitous edge in that garden - the slightest misstep rapidly takes us somewhere quite different to our intended destination. Should we avoid peril? Or can it take us to places otherwise inaccessible?</p>
<p>In contrast, a wide road at the bottom of a ravine might be difficult to leave. Any deviations from convention are quickly corrected. When should we stick to these safe routes, embracing the confidence given to us by others? When should we worry that are we being unnecessarily constrained by convention and denying ourselves the opportunities of novelty and new perspectives.</p>
</section>
</section>
<section id="working-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="working-with-the-metaphor">Working with the metaphor</h2>
<p>What might the metaphor mean in the data science workflow carried out by Jan – an imaginary data scientist looking to predict the outcome of a forthcoming national election?</p>
<p>Jan has at her disposal the results of a several national opinion polls that ask a sample of voters how they would vote in the election. She has a choice: Which of the polls does she use for her analysis? Acknowledging the existence of that choice helps to signal the possible impact of data sources on eventual conclusions.</p>
<p>She chooses one of the polls and <em>signposts</em> this was just one of several choices at this point. She wishes to visualize the distributions of the polling data. Again she has a choice - should she produce a map of the results emphasising geographical patterns, or a sequence of histograms emphasising the shapes of the distributions. Or she might choose both and compare what they tell her.</p>
<p>But what if she’d chosen a different opinion poll? Would that have led ultimately to different conclusions? Or what if she had chosen to represent the results in tabular format and not a map? Or what is the effect of choosing a choropleth over a cartogram?</p>
<p>Her path through the garden of possibilities are captured as a tree – from a single starting point (a) through the choice of polling data to use (b, c or d) and choices of output maps, charts and tables (e, f, g or h).</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/tree1.png" width="400" alt="A simple tree"></p>
<p>The characterisation of the topology of these choices tells us (and her) something about the process she is following. Depending on how different her interpretations e, f, g and h are, this might indicate the robustness of her conclusions.</p>
<p>Jan isn’t the only person working with election data. Perhaps her team started with different objectives, one emphasising the polling analysis capabilities of the team to market their services. Another focussed on assessing the accuracy of their prediction results. Another in providing commentary for a national news outlet. Perhaps their decision paths looked like this:</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/tree2.png" width="400" alt="Inverted tree"></p>
<p>It suggests a degree of <em>equifinality</em> – different paths lead to the same outcome (g) even when starting from different places. Different polling data or analytical processes lead to the same predicted result. It might suggest greater confidence in the prediction. But it also signals the possiblity that results are an artefact of misapplied analytical approaches that led to the same arbitrary result.</p>
<p>Jan decides to build a more sophisticated <a href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">MRP model</a> to predict election results. She stratifies opinion poll results by the demographics of the respondents, runs the model to generate some output distributions and visualises the output of the model emphasising the uncertainty captured by the model. In parallel her team-mate, Arun, runs his own MRP based on a different set of data and with different parameters weighting demographics. Even though this too generates distributions of possible predictive outcomes, they are quite distinct from Jan’s MRP results.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/forest4.png" width="400" alt="Unconnected networks"></p>
<p>It allows the team to question why their results are so distinct. Even Arun’s exploration of model parameterisation (<em>h</em> and <em>i</em>) that had initially given him a sense of the variance in the model outputs is challenged by the contrast with Jan’s MRP outcome (<em>d</em>).</p>
<p>For a later project, under less time constraint, Jan uses intermediate results from her analysis to revisit earlier assumptions and choices in a more iterative and reflexive process. It takes place in a foggier garden where she can’t see her destination until it emerges after several iterations.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/graph3.png" width="300" alt="Directed graph with loops"></p>
<p>By examining this topology Jan is able to see the effect of iterating over her process, to examine how her workflow is shaping the conclusions she draws, where iteration is more or less effective.</p>
</section>
<section id="thinking-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="thinking-with-the-metaphor">Thinking with the metaphor</h2>
<p>Our primary goal here is to present a metaphor that we find useful for framing data analysis. But the metaphor is only as useful as the tools and thinking it provokes, and while we don’t (yet) offer any concrete tools inspired by this metaphor, our discussion included many ideas that offer promise.</p>
<p>We’ve grouped our ideas into two broad categories which we explore below: navigating and documenting the journey.</p>
<section id="navigating-1" class="level3">
<h3 class="anchored" data-anchor-id="navigating-1">Navigating</h3>
<p>Our metaphor is physical and geographical: we are navigating forking paths in a garden. That physicality leads us to contemplate a set of questions:</p>
<ul>
<li><p>What areas have we visited and what areas have we missed? Where should we go and what areas should we avoid? We probably don’t start our journey with a map, but we do want to build one as we go. Such a map might be detailed, surveying the terrain and recording landmarks and findings. Or perhaps the map is schematic, carefully documenting the topology of paths taken, ignoring their length and distance between them. Or perhaps it is more experiential: “here be dragons”.</p></li>
<li><p>Am I walking alone or as a team? Do we walk the same path together or do different members take a turn at leading the navigation? Do we divide and conquer, starting from different locations and walking different paths? How do we share information on where we have been and where we’re contemplating next?</p></li>
<li><p>As we travel, we might notice interesting pathways that we don’t have the time to take right now. How can we leave analysis trail markers or note analysis landmarks so that when we inevitably need to backtrack, we can easily find a new path to take?</p></li>
<li><p>What does it mean to take a break, relaxing from the work, pausing, contemplating, and refining our plans? Can we build analysis ‘viewpoints’ that encourage us to pause and consider where we’ve been and where we’re going?</p></li>
<li><p>How much time do we take? Do we walk for a fixed distance, a fixed amount of time, or until we see a specific landmark? What tools might hint at when to stop our excursions?</p></li>
<li><p>When we hit a river or crevice do we backtrack to find an alternative path and when do we build a bridge? We may build bridges and create solid roads, tread new paths as we go.</p></li>
</ul>
</section>
<section id="documenting-the-journey" class="level3">
<h3 class="anchored" data-anchor-id="documenting-the-journey">Documenting the journey</h3>
<p>As we navigate the garden, we don’t want to forget why we’re journeying and what our end goal is (and sometimes, for the <em>flaneur</em>, the journey itself is the goal). We need to see, collect, observe, discover, understand, and describe, documenting our journey for ourselves and others.</p>
<ul>
<li><p>When should we take photos, highly realistic but low-dimensional snapshots of our observations? When should we make a sketch, a rough, quick, drawing emphasising features curated by the hand and mind of the observer. When should we a capture a specimen, a tiny snippet of reality, that we can later examine in the lab?</p></li>
<li><p>If our analysis takes multiple days, how do we remember what we did each day? Should we be journalling our journey each night, reflecting on the paths we have taken, missed, ignored, explored, and flagged for future exploration?</p></li>
<li><p>The analysis garden is large and complex, and it’s easy to lose our way and forget where we have been. As we proceed, should we be dropping breadcrumbs to remind us where we’ve been before? Breadcrumbs also allow us to backtrack, reversing from dead ends and examine the fork we have missed.</p></li>
</ul>
</section>
</section>
<section id="designing-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="designing-with-the-metaphor">Designing with the metaphor</h2>
<p>How does a metaphor help us inform workflows, attitudes, and tools for data analysis? We created the sketch below while exploring how metaphors can help (and hinder!) tool-building.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/sketch.png" width="700" alt="A simple tree"></p>
<p>In this diagram, the metaphor (M1) surfaces items, actions, relationships, challenges and questions (circles inside M1). By extending the metaphor to its widest extent, by playing with its elements and implications, we create a rich description of the actual problem domain (data analysis, in our case). Those are the lines to the circles inside the dashed circle, the problem space. By projecting concepts from the metaphor space to concepts in the problem space, we discover those concepts in the problem space and create explicit analogies to these concepts in the problem space (links between circles in M1 and the dashed circle.)</p>
<p>We may not uncover all concepts in the problem space but instead of trying to understand the problem domain “out of itself” and “through itself”, our chosen metaphor helps do the work for us. We use to the metaphor to create <em>a</em> specific understanding of the problem space. Each element in the metaphor, fog, bridges, maps, paths etc, maps to one or more elements in the problem domain: uncertainty, data, methodology, findings, actions, etc.</p>
<p>A tool designer wishing to support analysts in their work can now evaluate each of those mappings:</p>
<ul>
<li><p>Some mappings might be a <strong>high fidelity representation</strong> of the problem. For example, the sensation of deciding between multiple forking paths is similar to picking from a small set of possible analysis decisions.</p></li>
<li><p>Some mappings might be <strong>pluralistic</strong>. There might be many possible mappings and explanations for a given element of the metaphor. For example, we identified three possible interpretations of the height of the landscape above: the “optimality” of the analysis, the difficulty of analysis, or the ability to see above the fog.</p></li>
<li><p>Other mappings might <strong>not be useful</strong>: for example, it may be uninstructive to imagine a meaningful mapping of ‘weather’ to any concept in data analysis. Or mappings might be confusing and might lead to misunderstandings or false implications. Likewise, the notion of ‘relaxing’ or ‘recreation’ might not help at all with understanding data analysis. A designer can choose to discard those mappings.</p></li>
</ul>
<p>There might be concepts in the problem space that are not explained or “discovered” by the metaphor. We accept that but it shows that the choice of metaphor is important. We based our investigation on one possible metaphor, but there are many others.</p>
<p>Our sketch shows another metaphor labelled M2. It prompts us to wonder how it might influence our understanding of the problem (the links and concepts discovered in the problem space) and the potential tools we could develop to assist our analysis. Does it widen or deepen our understanding, or simply reiterate what we already know?</p>
<p>For example, we could look at other metaphors for data analysis.</p>
<ul>
<li>The multiverse.</li>
<li>A detective mystery.</li>
<li>The needle in a haystack.</li>
<li>Water and fishing (data dredging, fishing expeditions, data lakes).</li>
<li>Prospecting for oil (data is the new oil) and mining.</li>
</ul>
<p>Once a metaphor has been chosen and has been mapped to a problem space, a designer can start creating metaphor-informed solutions. In our sketch, those solutions are the rectangles S1-S3 on the very left. A design or tool does not have to cover all the concepts in the metaphor. Rather, it can focus on some specific elements and relations of that metaphor.</p>
<p>While metaphorical thinking can provide <em>affordances</em>, there may be is no benefit in making the metaphor explicit in the tools, workflows, activities or interfaces we create. Our tools don’t need to be skeuomorphic, and literally incorporate elements of the garden, fog, or forking paths into our user interfaces. We want to recreate the function of the metaphor only as far as it provides utility.</p>
</section>
<section id="speculating-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="speculating-with-the-metaphor">Speculating with the metaphor</h2>
<p>We have discussed a metaphor—“navigating the foggy garden of forking paths”—that we think provides a useful framing for the process of data analysis.</p>
<p>What do you think? Where does this metaphor resonate with your experience and where does it feel like a poor fit? Do you ever feel lost during the process of analysis and wish you had a map, compass, binoculars, trail markers, or a GPS? Does it prompt you to consider what new tools a data analyst might benefit from?</p>
<p>We also believe there is general value in framing workflows, actions, and interface design around the construction of metaphors. Metaphors provide us with a means to reduce the complexity of a problem but also surface the elements we might value. We are excited by the possibility metaphor can help to understand pain points, to ease critical thinking and to shape better tools for data analysis.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bach2024,
  author = {Bach, Benjamin and Wickham, Hadley and Wood, Jo and Xu, Kai},
  title = {Navigating the {Foggy} {Garden} of {Forking} {Paths}},
  date = {2024-06-25},
  url = {https://theory4ida.github.io/tmcf/posts/02-tools/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bach2024" class="csl-entry quarto-appendix-citeas">
Bach, Benjamin, Hadley Wickham, Jo Wood, and Kai Xu. 2024.
<span>“Navigating the Foggy Garden of Forking Paths.”</span> June 25,
2024. <a href="https://theory4ida.github.io/tmcf/posts/02-tools/">https://theory4ida.github.io/tmcf/posts/02-tools/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/02-tools/</guid>
  <pubDate>Mon, 24 Jun 2024 23:00:00 GMT</pubDate>
  <media:content url="https://jwolondon.github.io/forkingPaths/images/gardenPhoto.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Humans all the way down: statistical reflection and self-critique for interactive data analysis</title>
  <dc:creator>Di Cook</dc:creator>
  <dc:creator>Rachel Franklin</dc:creator>
  <dc:creator>Cagatay Turkay</dc:creator>
  <dc:creator>Mari-Cruz Villa-Uriol</dc:creator>
  <dc:creator>Levi Wolf</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/03-cultures/</link>
  <description><![CDATA[ 





<p><img src="https://theory4ida.github.io/tmcf/posts/03-cultures/archives.png" class="img-fluid"></p>
<blockquote class="blockquote">
<p>The Census Bureau tabulates same-sex couples in both the American Community Survey (ACS) and the Decennial Census. Two questions are used to identify same-sex couples: relationship and sex. The agency follows edit rules that are used to change data values for seemingly contradictory answers. The edit rules for combining information from relationship and sex have evolved since the category of unmarried partner was added in 1990. <strong>In that census, if a household consisted of a married couple and both spouses reported the same sex, the relationship category remained husband or wife, but the sex of the partner who reported being a spouse to the householder was changed</strong> [our emphasis].</p>
<p><a href="https://academic.oup.com/poq/article/77/S1/145/1877806">DeMaio et al., 2023</a></p>
</blockquote>
<p>What does data-driven science that is critical, reflexive, and methodologically robust look like? How can we operationalise such a mindset through structured methods and practices to foster different expectations for data-driven science and scientists? Interactive data analysis<sup>1</sup> by definition offers approaches that encourage an iterative dialogue between analyst, models, and data, making the most of machine capability and human participation and facilitating the interrogation of context and situatedness of data and analytical workflows. Used mindfully and deliberately, interactivity is a strength—not a threat—for validity, robustness, and value. In this blog, we highlight reasons why analysis cannot be left solely to the machines and provide concrete suggestions for guided interactivity: ways in which analysts can probe themselves and their analytical findings to become habituated to asking: How can I be sure I have found something? How can I be more certain what I have found is notable?</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Becker, R. and Chambers, J. (1984) S: An Interactive Environment for Data Analysis and Graphics, Chapman and Hall/CRC.</p></div></div><section id="interaction-taking-time-to-look-around" class="level2">
<h2 class="anchored" data-anchor-id="interaction-taking-time-to-look-around">Interaction — Taking time to look around</h2>
<p>Within a broader data analysis framework, <em>interaction</em> is a moment of communication—a touchpoint—that happens via a graphic, table, metric, summary statistic, an estimated parameter or a prediction. It’s usually facilitated by coding (as in literate programming), direct manipulation using graphics (as in multiple coordinated views), or through a user-interface. As such, <em>interactive data analysis</em> encourages an ongoing dialogue between analyst, analysis, and data. Each touchpoint presents an opportunity to stop and reflect: What have we learned? What might we still learn? How does this extend our imagination, understanding, and vision of the world we inhabit? Amidst pressure to find answers and make decisions, interaction prompts deliberation, curiosity, and reflection.</p>
</section>
<section id="data-driven-science-starts-with-data" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="data-driven-science-starts-with-data">Data-driven science starts with data</h2>
<p>Data is a powerful but imperfect lens for making sense, and increasing our understanding, of the world around us<sup>2</sup><sup>3</sup>. Human decisions determine (and constrain) the who, what, when, where, and why<sup>4</sup> of data—as well as the mechanics of how phenomena and characteristics are measured. The way data is collected substantially determines what inferences can be made, which is sometimes ignored by the human analyst. Humans occasionally apply subjective filters to collected data, as well. For example, the widely publicized gender gap in math, measured with PISA data, is not universal among 15 year olds in numerous countries (however a <em>reversed reading gap</em> is observed)—and the effect is exaggerated from an average gap of 0–20 points out of a full numerical range of 0–1000. The effect is miniscule, and individual-to-individual variation dominates<sup>5</sup>.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Donoho, D. (2017). 50 Years of Data Science. Journal of Computational and Graphical Statistics, 26(4), 745–766. https://doi.org/10.1080/10618600.2017.1384734.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Wickham, H., Çetinkaya-Rundel, M. &amp; Grolemund, G. (2023). R for Data Science (2e). O’Reilly Media.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;Çetinkaya-Rundel, M., Dogucu, M. &amp; Rummerfield, W. (2022) The 5Ws and 1H of Term Projects in the Introductory Data Science Classroom, https://doi.org/10.52041/serj.v21i2.37.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;https://www.oecd-ilibrary.org/sites/f56f8c26-en/index.html?itemId=/content/component/f56f8c26-en</p></div><div id="fn6"><p><sup>6</sup>&nbsp;Broman, K. W., &amp; Woo, K. H. (2018). Data Organization in Spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;Swinscow, T. D. V. (2009) Statistics at Square One (11e), BMJ Books. (Chapter 3).</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Belbin L, Wallis E, Hobern D, Zerger A (2021) The Atlas of Living Australia: History, current state and future directions. Biodiversity Data Journal 9: e65023. https://doi.org/10.3897/BDJ.9.e65023</p></div><div id="fn9"><p><sup>9</sup>&nbsp;MacColl, C., Leseberg, N. P., Seaton, R., Murphy, S. A., &amp; Watson, J. E. M. (2023). Rapid and recent range collapse of Australia’s Red Goshawk Erythrotriorchis radiatus. Emu - Austral Ornithology, 123(2), 93–104. https://doi.org/10.1080/01584197.2023.2172735</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Malički M, Aalbersberg IJ, Bouter L, Mulligan A, Ter Riet G. (2023) Transparency in Conducting and Reporting Research: A survey of authors, reviewers, and editors across scholarly disciplines. PLoS One. 18(3):e0270054. doi: 10.1371/journal.pone.0270054.</p></div></div><p>Automated validation<sup>6</sup> is commonly implemented to prevent mistakes, but, at the root, human decisions guide what to check and what constitutes a mistake. Moreover, omissions in the original data collection can inhibit inference. For example, making population-level references from data requires specifying the population<sup>7</sup>. It’s not easy, and it may be uncomfortable, to spell out the population. It may also result in awkward realizations, like that half the population was not measured. Other times, for example with animal sightings, values are only recorded when a human is present even though most events happened with no humans looking<sup>8</sup> and substantial adjusting is required to provide some assurance of reliability of the inferences made<sup>9</sup>. Transparency<sup>10</sup> in data collection and application helps illuminate these issues.</p>
<p>There are multiple analytical starting points when working with data. A data scientist or analyst might commence directly from the data<sup>11</sup>, exploring potential relationships and patterns to determine what stories or narratives can be uncovered. More traditionally, one might come to the data with hypotheses in hand, seeking to test whether theory-based expectations can be supported. To infer from data and analysis, self-critique encourages questioning. Who and what is captured by the data and what is missing? How do method choices and presentation of results reinforce, obfuscate, or illuminate findings? When are results robust and ready to be shared? When should we stop?</p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Wickham, H., Lawrence, M., Cook, D. et al.&nbsp;(2009) The Plumbing of Interactive Graphics. Computational Statistics 24, 207–215, https://doi.org/10.1007/s00180-008-0116-x</p></div></div></section>
<section id="humans-all-the-way-down" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="humans-all-the-way-down">Humans all the way down</h2>
<p>Human-in-the-loop interactive data analysis is a helpful framework for data science- and AI-driven research, encouraging those explicit touchpoints between researcher and computer in which expert knowledge can shape and feed back into an iterative analytical process guided by real-world understanding. Although an appealing solution to existing problems, the human-in-the-loop proposition relies on an implicit null model of human-<em>not</em>-in-the-loop: that without deliberate touchpoints, data-driven research is somehow insulated from human fallibility (and expertise). This is patently not the case. Human fingerprints are all over data science applications, even when machines have guided the process from start to finish.</p>
<section id="humans-are-in-the-data-loop" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="humans-are-in-the-data-loop">Humans are in the data loop</h3>
<p>Although some shortcomings of data are commonly acknowledged, like bias, representation, or missingness, there is less acknowledgement that, particularly for surveys or administrative data, data are created by humans. Humans decide what questions to ask, what the answer space looks like, how to convert raw answers into variables, measures, and indices, and what will be the unit of observation. The example at the top of this post illustrates how a massive and ongoing data collection mechanism like the US decennial census failed to count same-sex households in the late 20th century. The option not only didn’t appear as an element in the answer space, but <a href="https://academic.oup.com/poq/article/77/S1/145/1877806">responses were changed for those households where both adults reported the same sex, on the assumption that error was involved</a><sup>12</sup>. In this case, human researchers not only failed to conceptualise the full space of real household behaviours, but also amended the data to fit a set of assumptions. Similar blinders or constraints exist for household structures that assume marriage, or ethnicity and race categories that force respondents into predefined categories, or sex/gender questions that fail to reflect actual lived identities.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;DeMaio, T., Bates, N., and O’Connell, M. (2013). Exploring Measurement Error Issues In the Reporting of Same-Sex Couples. <em>Public Opinion Quarterly</em> 77(S1): 145-158. <a href="https://doi.org/10.1093/poq/nfs066">https://doi.org/10.1093/poq/nfs066</a></p></div></div></section>
<section id="humans-are-in-the-theory-loop" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="humans-are-in-the-theory-loop">Humans are in the theory loop</h3>
<p>Theory is often assumed to be less relevant for data-driven research—the idea that big data (and big code) renders theory obsolete<sup>13</sup>. This can also be read as another form of blindness to humans in the loop, as theory is, without argument, created by humans. Wait, you say, how can theory impact data-driven research in which theory plays no explicit role? First, theory is what determines the content of our data. We know to collect data about race, gender, age, educational attainment, and geographical location (as just a few examples) <em>precisely because</em> theory tells us these are important for understanding outcomes, behaviors, and preferences. Second, the questions we ask of our data—even in exploration—may not be testing theories, but are certainly theory-driven. When handed wage or income data, it is theory that whispers in our ear, “how does this compare across genders?”</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;https://www.wired.com/2008/06/pb-theory/</p></div></div></section>
<section id="and-humans-are-in-the-methods-loop-too" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="and-humans-are-in-the-methods-loop-too">And humans are in the methods loop, too</h3>
<p>Ok, but surely the methods we employ—software packages, models, and statistics—are free of humans? Not quite. Methods are created and implemented by humans in order to solve problems and answer questions generated by humans. This necessarily means that the types of analysis (and therefore eventual answers or findings) are constrained by the tools we have at our disposal. From the development of statistics to compare groups to innovative types of data visualisation (like hexagon maps<sup>14</sup> over traditional choropleth maps), what at first glance might appear to be an off-the-shelf machine procedure is actually a human-in-the-loop solution.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;https://srkobakian.github.io/sugarbag/index.html</p></div></div></section>
</section>
<section id="why-human-involvement-matters" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="why-human-involvement-matters">Why human involvement matters</h2>
<p>In a nutshell, robust inference is difficult. One explanation for this is the “garden of forking paths” metaphor<sup>15</sup> which illustrates the challenges of interactive hypothesis testing and the blurry boundaries between explanatory and confirmatory analysis. The challenge is, however, much more extensive: implicit choices embedded in theory, data, and methods also hamper inference. Moreover, the wider contexts in which data analysis takes place, whether academic, governmental, industry, or media, privilege speed and certainty of findings. Academic systems, including publication and promotion, reward quantity, innovation, and conformity, meaning there is little if any incentive for introspection, replication, or any other approach that might reinforce confidence in findings.</p>
<div class="no-row-height column-margin column-container"><div id="fn15"><p><sup>15</sup>&nbsp;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf</p></div></div><p>The assumption is that humans get in the way of robust inference—that, consciously and subconsciously, we make choices that facilitate the (occasionally spurious) detection of patterns, signals, and answers. We know to shine a spotlight on model specification and interpretation, whether exploratory or confirmatory—obvious touchpoints where humans and machines are in contact—as areas vulnerable to analyst bias. To go further and argue that human touchpoints exist at every stage of research, from theory to data to methods, is to invite capitulation. Perhaps robust analysis is impossible. Surrender is not what we are proposing, however! We are confident analysts can respond creatively to challenges, once they’re identified. To do this, though, we need to become better at academic self-reflection and also take greater advantage of a wide range of inference support approaches. We have some suggestions:</p>
<section id="be-a-social-scientist-talk-to-people" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="be-a-social-scientist-talk-to-people">Be a “<em>social</em>” scientist: talk to people!</h3>
<p>One of the easiest ways to stress-test data analysis is <em>to talk to people about it</em>! Getting feedback is central to improvement. Write a working paper, post it on <a href="https://arxiv.org">ArXiV</a>/<a href="https://osf.io">OSF</a>, publish code as an open-source software package, and talk about the work at conferences and on social media. Indeed, the things that we worry about in analysis or research are often quite different from the stuff that other people might find concerning. It’s useful to note here that just <em>posting</em> (or <em>talking</em>) about the work is often not enough to actually attract feedback; teaching people how to use one’s open source code might be necessary in order to get feedback on the ideas it executes. More informal “brown-bag” or “workshop”-style events are often designed for this kind of structured feedback, and tools like Margins/Librarian from Fermats Library<sup>16</sup> or Hypothesis<sup>17</sup> can help facilitate online input.</p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;<a href="https://fermatslibrary.com/">https://fermatslibrary.com/</a></p></div><div id="fn17"><p><sup>17</sup>&nbsp;<a href="https://web.hypothes.is/">https://web.hypothes.is/</a></p></div><div id="fn18"><p><sup>18</sup>&nbsp;<a href="https://thesiswhisperer.com/2023/07/10/academicenshittification/">https://thesiswhisperer.com/2023/07/10/academicenshittification/</a></p></div><div id="fn19"><p><sup>19</sup>&nbsp;Whitmer, J. M. You are your brand: self-branding and the marketization of self. <em>Sociology Compass</em> 13(3): e12662. <a href="https://doi.org/10.1111/soc4.12662">https://doi.org/10.1111/soc4.12662</a></p></div></div><p>At the same time, it is important to acknowledge that social media and conferences are not a cure-all for gaining perspective. Travel to conferences can be expensive and time-consuming, with clear disparities in access. Furthermore, conferences can be competitive, high-stakes environments for some, making it difficult to both give and receive effective and constructive critique. Social media also has some serious drawbacks<sup>18</sup>, in part due to the imperative to develop and maintain a ‘personal brand’ on those platforms.<sup>19</sup> At a higher level, seeking input from others can make it difficult to find a balance between cultivating an individual voice and learning to adopt the perspectives and norms within a research community. This is especially the case for early career researchers. Thus, it can be helpful to cultivate methods to support internal self-reflection.</p>
</section>
<section id="internalize-a-critical-perspective" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="internalize-a-critical-perspective">Internalize a critical perspective</h3>
<p>One common practice is to imagine how someone else might react to our work. This might be a PhD or work supervisor (or another mentor) voicing common questions or routine self-checks: <em>Have you plotted the data first? Have you tried to solve the problem yourself before reaching for something off the shelf? Has someone else solved it differently? Why did the previous work on this topic miss what you’re proposing?</em> This also might come in the form of conceptualizing a researcher <em>persona</em>, a fictional person who works from a given research background (like an “experimentalist” or a “data scientist”), who critiques work from a particular angle. This can help identify weaknesses in research design, or identify places where analytical choices might merit more justification. This can be quite useful as a mode of self-reflection, and has its place in developing an analytical voice and sense of self.</p>
<p>Not everybody has had such a mentor or, if they have one, it might be hard to internalize their voice positively. Mentoring is quite important, but it can be difficult to cultivate good mentor-mentee relationships.<sup>20</sup> When thinking about research personas, critiques might be too <em>total</em> to effectively improve a draft. An old joke comes to mind:</p>
<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;https://www.science.org/content/article/improving-mentoring-academia-requires-collective-effort.</p></div></div><p>A driver, tired and lost, pulls over to the side of the road and asks a shepherd:</p>
<blockquote class="blockquote">
<p>“How do I get to town?” _</p>
</blockquote>
<p>The shepherd responds,</p>
<blockquote class="blockquote">
<p>“Well, I wouldn’t start from here!”</p>
</blockquote>
<p>Indeed, if an analyst <em>knew</em> that analysis was problematic, they might ostensibly change how they executed that part of the research in the first place. Of course, the analyst <em>doesn’t</em> know what they don’t know until they try to gain perspective. This suggests that <em>surprise</em> is essential to effective self-reflection—learning about the ways you can improve will involve some kind of realization, a recognition of something previously unknown, and is fundamental to effective self-reflection.</p>
</section>
<section id="leverage-randomness-and-randomization" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="leverage-randomness-and-randomization">Leverage randomness and randomization</h3>
<p>Another way to elicit new perspectives is to invoke randomness<sup>21</sup>, like an <em>Oblique Strategies</em><sup>22</sup> for researchers. In its original form, this is a deck of cards intended to help artists get over creative blocks. Each card has a different short statement used as a prompt for engagement<sup>23</sup>. The card might suggest to “Discover the recipes you are using and abandon them” or “Honor thy error as a hidden intention”. These are useful provocations to escape a creative block. They are unlikely to directly help determine a best course of action during a data analysis, or decide on a better model, but are an effective technique to imagine what this relationship might look like if there was really nothing going on<sup>24</sup><sup>25</sup>, or vary if a different sample was taken. Invoking randomness includes permuting values<sup>26</sup>, bootstrapping<sup>27</sup> the sample, creating cross-validation folds<sup>28</sup><sup>29</sup>, simulating<sup>30</sup> from a distribution—all mindsets and tools that enable the development of perspective on data patterns and provide confidence in interpretations.</p>
<div class="no-row-height column-margin column-container"><div id="fn21"><p><sup>21</sup>&nbsp;http://library.isical.ac.in:8080/jspui/bitstream/10263/5519/1/C%20R%20Rao%20speech.pdf.</p></div><div id="fn22"><p><sup>22</sup>&nbsp;https://en.wikipedia.org/wiki/Oblique_Strategies.</p></div><div id="fn23"><p><sup>23</sup>&nbsp;You can try drawing a few of these yourself here: http://stoney.sb.org/eno/oblique.html.</p></div><div id="fn24"><p><sup>24</sup>&nbsp;Buja A., Cook D., Hofmann H., Lawrence M., Lee E.-K., Swayne D. F. and Wickham H. (2009) Statistical inference for exploratory data analysis and model diagnostics, Phil. Trans. R. Soc. A.3674361–4383 http://doi.org/10.1098/rsta.2009.0120.</p></div><div id="fn25"><p><sup>25</sup>&nbsp;Wickham, H., Cook, D., Hofmann, H. and Buja, A., “Graphical inference for infovis,” in IEEE Transactions on Visualization and Computer Graphics, vol.&nbsp;16, no. 6, pp.&nbsp;973-979, Nov.-Dec.&nbsp;2010, doi: 10.1109/TVCG.2010.161.</p></div><div id="fn26"><p><sup>26</sup>&nbsp;Good, P. (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses, Springer.</p></div><div id="fn27"><p><sup>27</sup>&nbsp;Efron, B. (1979) Bootstrap Methods: Another Look at the Jackknife. Ann. Statist. 7 (1) 1 - 26, January, 1979. https://doi.org/10.1214/aos/1176344552.</p></div><div id="fn28"><p><sup>28</sup>&nbsp;Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and a Method for Prediction, Technometrics. 16 (1): 125–127. doi:10.2307/1267500.</p></div><div id="fn29"><p><sup>29</sup>&nbsp;Stone, M. (1974). Cross-Validatory Choice and Assessment of Statistical Predictions, Journal of the Royal Statistical Society, Series B (Methodological). 36 (2): 111–147. doi:10.1111/j.2517-6161.1974.tb00994.x.</p></div><div id="fn30"><p><sup>30</sup>&nbsp;Sokolowski, J. A., Banks, C. M. (2009). Principles of Modeling and Simulation. John Wiley &amp; Son. p.&nbsp;6. ISBN 978-0-470-28943-3.</p></div></div></section>
<section id="consider-granularity" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="consider-granularity">Consider granularity</h3>
<p>One pattern of thought might be to use more fine-grained explanations: what specific observations are necessary for a result to hold? There are a few new statistical approaches that can help identify, for example, how much of a dataset is needed in order to preserve a core finding<sup>31</sup> or how “serious” an omitted confounder would have to be before an effect disappears<sup>32</sup>. In either case, these statistical tools help identify <em>why</em> a result arises from a given dataset, characterize the conditions under which they might not, and find what new data might be needed (or analyses conducted) in order to make the work more robust.</p>
<div class="no-row-height column-margin column-container"><div id="fn31"><p><sup>31</sup>&nbsp;Broderick, T., Giordano, R., and Meager, R. (2023). An automatic finite-sample robustness metric: when can dropping a little data make a big difference. ArXiV preprint: https://arxiv.org/abs/2011.14999</p></div><div id="fn32"><p><sup>32</sup>&nbsp;Cinelli, C. and Hazlett, C. (2019). Making sense of sensitivity: extending omitted variable bias. <em>Journal of the Royal Statistical Society, Series B: Statistical Methodology</em> 82(1), 39-67. <a href="https://doi.org/10.1111/rssb.12348">https://doi.org/10.1111/rssb.12348</a></p></div></div></section>
<section id="identify-saturation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="identify-saturation">Identify saturation</h3>
<p>How much analysis is enough? The question of when to stop collecting data, when to stop estimating models, or when a question has been answered—when to stop fiddling around with analytical details, in short—is one faced across the entire research continuum. An approach widely adopted in qualitative research is that of saturation<sup>33</sup>: research can stop when no new information or insight is elicited from additional data collection or analysis. This framework is also generally (but cautiously!) applicable to data-driven science. Here the idea is less that analysis stops as soon as a satisfactory answer is found (bad; don’t do this), but rather that a variety of approaches is tried until broadly the same findings emerge across the board. At that point saturation is reached.</p>
<div class="no-row-height column-margin column-container"><div id="fn33"><p><sup>33</sup>&nbsp;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5993836/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5993836/</a></p></div></div></section>
<section id="use-counterfactuals-and-what-if-scenarios" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="use-counterfactuals-and-what-if-scenarios">Use counterfactuals and what-if scenarios</h3>
<p>Robust inference depends on stress-testing analytical approaches, or theoretical assumptions<sup>34</sup>, and the ability to interpret black-box models<sup>35</sup>. Across research domains, the observed analytical relationships between variables can result from a range of underlying processes. Student test scores that increase as class size shrinks<sup>36</sup>, for example, may result from a direct relationship between the two variables, but might also be attributable to the wealth of the school district, characteristics of students, or a host of other intervening/underlying factors. From a policy standpoint, this is a problem. Where and how should interventions be designed? A counterfactual<sup>37</sup> mindset encourages the analyst to consider alternative stories or narratives and to assess whether, and how, analytical findings change when parts of the story are changed. This also relates to explainable artificial intelligence (XAI) methods for complex models<sup>38</sup>, which articulate how a model makes an individual prediction.</p>
<div class="no-row-height column-margin column-container"><div id="fn34"><p><sup>34</sup>&nbsp;Greco, L., Ventura, L. (2011) Robust inference for the stress–strength reliability. Stat Papers 52, 773–788, https://doi.org/10.1007/s00362-009-0286-9.</p></div><div id="fn35"><p><sup>35</sup>&nbsp;Molnar, C. (2024) Interpretable Machine Learning, https://christophm.github.io/interpretable-ml-book/</p></div><div id="fn36"><p><sup>36</sup>&nbsp;Cho, H., Glewwe, P., Whitler, M. (2012) Do reductions in class size raise students’ test scores? Economics of Education Review, Volume 31, Issue 3, Pages 77-95, https://doi.org/10.1016/j.econedurev.2012.01.004.</p></div><div id="fn37"><p><sup>37</sup>&nbsp;DeMartno, G.F. (2021) The specter of irreparable ignorance: counterfactuals and causality in economics. Review of Evolutionary Political Economy 2: 253-276. https://doi.org/10.1007/s43253-020-00029-w</p></div><div id="fn38"><p><sup>38</sup>&nbsp;Biecek P (2018). “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research, 19(84), 1-5. https://jmlr.org/papers/v19/18-416.html.</p></div></div></section>
<section id="see-it-to-believe-it" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="see-it-to-believe-it">See it to believe it</h3>
<p>Data visualisation employs the human visual system to interpret relationships, enabling summaries that are more elaborate than is capable from simple numerical statistics. It is useful in every stage of interactive data analysis, from data exploration to diagnosing and summarising model fit. New statistical thinking in visualisation methods lifts this area into the domain of making inference for unconventional data scenarios and evaluating robustness of findings. As a baseline, the grammar of graphics<sup>39</sup><sup>40</sup> provides a framework that bridges statistical thinking and data graphics. From this basis, new methods are emerging. Line-ups<sup>41</sup><sup>42</sup> nudge the viewer to check whether observed relationships are really there. Causal quartets<sup>43</sup>, which provide a visual “check” on the mechanics of average treatment effects, can help analysts understand the different combinations of individual effects that produce average effects. Hypothetical outcome plots<sup>44</sup> enable the analyst to assess what the plot might look like under different samples. Rashomon sets<sup>45</sup> are useful for instances in which predictive models offer similar performance, but alternative underlying explanations. One element common to these visual approaches is that they serve to encourage reflection, multiplicity of perspectives, and nuanced interpretation, supporting and cautioning interpretation of results and aiding inference where classical statistical methods do not apply.</p>
<div class="no-row-height column-margin column-container"><div id="fn39"><p><sup>39</sup>&nbsp;Wilkinson, L. (2010), The grammar of graphics. WIREs Comp Stat, 2: 673-677. https://doi.org/10.1002/wics.118</p></div><div id="fn40"><p><sup>40</sup>&nbsp;Hadley Wickham. (2010) A layered grammar of graphics.Journal of Computational and Graphical Statistics, vol.&nbsp;19, no. 1, pp.&nbsp;3–28.</p></div><div id="fn41"><p><sup>41</sup>&nbsp;Buja A., Cook D., Hofmann H., Lawrence M., Lee E.-K., Swayne D. F. and Wickham H. (2009) Statistical inference for exploratory data analysis and model diagnostics, Phil. Trans. R. Soc. A.3674361–4383 http://doi.org/10.1098/rsta.2009.0120.</p></div><div id="fn42"><p><sup>42</sup>&nbsp;Wickham, H., Cook, D., Hofmann, H. and Buja, A., “Graphical inference for infovis,” in IEEE Transactions on Visualization and Computer Graphics, vol.&nbsp;16, no. 6, pp.&nbsp;973-979, Nov.-Dec.&nbsp;2010, doi: 10.1109/TVCG.2010.161.</p></div><div id="fn43"><p><sup>43</sup>&nbsp;Gelman, A., Hullman, J. and Kennedy, L. (2023) Causal quartets: Different ways to attain the same average treatment effect, <a href="https://arxiv.org/abs/2302.12878">https://arxiv.org/abs/2302.12878</a>.</p></div><div id="fn44"><p><sup>44</sup>&nbsp;A. Kale, F. Nguyen, M. Kay and J. Hullman, Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data in IEEE Transactions on Visualization &amp; Computer Graphics, vol.&nbsp;25, no. 01, pp.&nbsp;892-902, 2019. doi: 10.1109/TVCG.2018.2864909.</p></div><div id="fn45"><p><sup>45</sup>&nbsp;Biecek, P., Baniecki, H., Krzyziński, M., &amp; Cook, D. (2024). Performance Is Not Enough: The Story Told by a Rashomon Quartet. Journal of Computational and Graphical Statistics, 1–4. https://doi.org/10.1080/10618600.2024.2344616</p></div></div></section>
<section id="embrace-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="embrace-uncertainty">Embrace uncertainty</h3>
<p>Modelling is an uncertain endeavour: data are uncertain; models provide (uncertain) estimates of one potential version of reality; and individual point estimates are uncertain. Researchers and analysts, however, work in truths and certainties, generating inevitable tension between data-driven analysis and real-world applications. What to do? Formally, it’s important to embrace opportunities to foreground uncertainty of findings and this can be done visually, numerically, and narratively. More informally, practicing talking about findings as one plausible version of a story <em>out of many</em> performs a wider service to all communities producing and consuming data-driven analysis.</p>
</section>
<section id="record-provenance-and-document-analytic-journeys" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="record-provenance-and-document-analytic-journeys">Record provenance and document analytic journeys</h3>
<p>The human-machine partnership underlying interactive data analysis needs to be documented as a means to ensure reproducibility, facilitate validation, and increase trust—not only in the findings but in the journey itself<sup>46</sup>. The documentation and provenance of these interactions can be performed at different levels of detail, including with data and methods, as well as how results are validated and reported. The documentation of the analytic journey needs to happen at multiple levels of detail, from coarse high-level representations of the journey to fine-grained, low-level, representations of the analyst’s journey. Ensuring that these are explored in context is also important.</p>
<div class="no-row-height column-margin column-container"><div id="fn46"><p><sup>46</sup>&nbsp;Becker, G., Barr, C., Gentleman, R., &amp; Lawrence, M. (2017). Enhancing Reproducibility and Collaboration via Management of R Package Cohorts. Journal of Statistical Software, 82(1), 1–18. https://doi.org/10.18637/jss.v082.i01</p></div></div></section>
</section>
<section id="closing-reflections" class="level2">
<h2 class="anchored" data-anchor-id="closing-reflections">Closing reflections</h2>
<p>Despite the rise of artificial intelligence, human intelligence (and fallibility) are evergreen. Human actions are ubiquitous in every part of data analysis, and humans are active players in this game. With the increasing prevalence of open-source tools and open research, everyone has the power to do data analysis. Under these conditions making reliable inferences presents ongoing challenges. Creating spaces in which to try out alternatives facilitates creativity, interactivity, and surprise in data analysis—and, in the end, more robust inference. Building in techniques for self-reflection and critique is more necessary than ever to help ensure confidence in data analysis results, allowing the data to say what it means.</p>
<!-- Footnotes themselves at the bottom. -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cook2024,
  author = {Cook, Di and Franklin, Rachel and Turkay, Cagatay and
    Villa-Uriol, Mari-Cruz and Wolf, Levi},
  title = {Humans All the Way down: Statistical Reflection and
    Self-Critique for Interactive Data Analysis},
  date = {2024-06-25},
  url = {https://theory4ida.github.io/tmcf/posts/03-cultures/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cook2024" class="csl-entry quarto-appendix-citeas">
Cook, Di, Rachel Franklin, Cagatay Turkay, Mari-Cruz Villa-Uriol, and
Levi Wolf. 2024. <span>“Humans All the Way down: Statistical Reflection
and Self-Critique for Interactive Data Analysis.”</span> June 25, 2024.
<a href="https://theory4ida.github.io/tmcf/posts/03-cultures/">https://theory4ida.github.io/tmcf/posts/03-cultures/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/03-cultures/</guid>
  <pubDate>Mon, 24 Jun 2024 23:00:00 GMT</pubDate>
  <media:content url="https://theory4ida.github.io/tmcf/posts/03-cultures/archives.png" medium="image" type="image/png" height="66" width="144"/>
</item>
<item>
  <title>Forking paths and workflow in statistical practice and communication</title>
  <dc:creator>Andrew Gelman</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/04-forkingpaths/</link>
  <description><![CDATA[ 





<p><img src="https://theory4ida.github.io/tmcf/posts/04-forkingpaths/brecon.jpeg" class="img-fluid"> To note <sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Some of the material here came from posts on our blog, Statistical Modeling, Causal Inference, and Social Science.</p>
<p>We thank Merlin Heidemanns for the analysis of uncertainty in election forecasting and the participants in the Turing Institute workshop, Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science, https://www.turing.ac.uk/research/theory-and-method-challenge-fortnights/navigating-garden-forking-paths-theoretical</p></div></div><p>I recently participated in a workshop on theoretical foundations for interactive data analysis in data-driven science with the theme, "Navigating the garden of forking paths." The issue here was not the impact of forking paths on p-values (Gelman and Loken, 2013) but rather how to better understand the open-ended nature of exploratory analysis and discovery, a topic that we've also been thinking about regarding statistical modeling workflow (Gelman et al., 2020).</p>
<p>As a contribution to this workshop, I have compiled here some thoughts, which I've divided into two categories: statistical practice and communication. "Statistical practice" includes graphical exploration as well as more traditional model-based inference, and "communication" includes the sociological processes of science.</p>
<p>A recurring theme here is the connection between research goals, scientific discovery, and mathematical/computational tools.</p>
<section id="some-thoughts-on-forking-paths-in-statistical-practice" class="level2">
<h2 class="anchored" data-anchor-id="some-thoughts-on-forking-paths-in-statistical-practice">Some thoughts on forking paths in statistical practice:</h2>
<ul>
<li><p><em>Statistical practice as scientific exploration.</em> When you do applied statistics (more generally, "interactive data analysis"), you’re acting like a scientist. You form hypotheses, gather data, run experiments, modify your theories, etc. Here, we're not talking about hypotheses of the form “theta = 0” or whatever; we're talking about hypotheses such as, “N = 200 will be enough for this study” or "A parallel coordinates plot might reveal an unexpected pattern in these data" or “Instrumental variables should work on this problem” or “We can safely use the normal approximation here” or “We really need to include a measurement-error model here” or “The research question of interest is unanswerable from the data we have here; what we really need to do is . . .”, etc. Existing treatments of statistical practice and workflow (including in our own textbooks) do not fully capture the way that the steps of statistical design, data collection, analysis, and decision making feel like science.</p></li>
<li><p><em>The trail of breadcrumbs.</em> To understand and trust such an analysis it is helpful to have a “trail of breadcrumbs” connecting data, theory, and conclusions. Here's a story to illustrate this point. Gartzke (2007) performed an analysis to distinguish between two theories in international relations: the "democratic peace" (which postulates that democratic countries do not go to war) and the "capitalist peace" (under which the key factor is trade, not political deliberation). As Gartzke puts it, "both democracies and capitalist dyads appear never to fight wars. Still, determining more about these relationships, and their relative impact on war, requires that we move beyond cross tabs." Based on his regression analysis, he concludes that the evidence suggests that "capitalism, and not democracy, leads to peace." The question then arises: Where in the data can this distinction be made? In regression analysis predicting war (more generally, "militarized interstate disputes") from numerical measures of democracy, capitalism, and various other characteristic dyads of countries over time. Capitalism and democracy are highly correlated in the data, so for the regression to untangle their predictive effects, there should be some warring dyads that were democratic but not capitalistic. The decisive data perhaps come from the wars in 1990s in the former Yugoslavia, when Serbia, Bosnia, and Croatia were democracies but did not yet have capitalist economic systems. The point of this story for our purposes here is that when a data-driven analysis leads to a discovery, the logical next step is to open the black box and understand what in the data led to this conclusion. Some analysis and visualization tools are well-suited to this process; with other methods, such as regression analysis, this opening-up process is not so easy, and this represents an important path for future research.</p></li>
<li><p><em>Moving beyond the push-a-button, take-a-pill model of science.</em> There is a replication crisis in much of science, and the resulting discussion has focused on issues of procedure (preregistration, publication incentives, and so forth) and statistical concepts such as p-values and statistical significance. But what about the scientific theories that were propped up by these unreplicable findings—what can we say about them? Many of these theories correspond to a simplistic view of the world, with push-button interventions that are summarized by their "treatment effects." Real-world effects vary among people and over time, and estimates of localized effects will typically be very noisy. As a consequence, it's unrealistic to expect theory-free inference to yield stable estimates. Statistical significance and forking paths are the least of our problems here. Instead we recommend considering mechanistic or process-based modeling, where possible measuring and modeling intermediate outcomes. A simple example is to model tumor sizes in a cancer drug rather than just looking at a binary success/failure outcome.</p></li>
<li><p><em>Exploratory data analysis and implicit models.</em> Data visualization and exploratory analysis have often been thought to be unrelated to or in competition with statistical modeling. When thought of in terms of workflow, though, exploration and modeling can be seen as closely related. Start with the idea that exploratory analysis is for discovering unexpected patterns in data: as Tukey (1972) put it, “graphs intended to let us see what may be happening over and above what we have already described." Lurking behind the unexpected is <em>the expected</em>, and indeed the better we can model our data, the more we can learn from our data graphics. Models guide our explanations; conversely, exploratory discoveries can be viewed as model checks (Gelman, 2004, Hullman and Gelman, 2021).</p></li>
<li><p>Here's a standard paradigm of data analysis, which we do not like because we prefer to think of all data analysis as exploratory:</p></li>
</ul>
<blockquote class="blockquote">
<p>- Step 1: "Exploratory data analysis." Some plots of raw data, possibly used to determine a transformation.</p>
<p>- Step 2: The "main analysis"—maybe model-based, maybe non-parametric, whatever. It is typically focused, not always recognized as exploratory.</p>
<p>- Step 3: That’s it.</p>
</blockquote>
<p>We can do better than Step 3 by integrating Steps 1 and 2. A good model can make exploratory data analysis much more effective and, conversely, we'll understand and trust a model a lot more after seeing it displayed graphically along with data.</p>
<ul>
<li><p><em>The fractal nature of scientific revolutions.</em> Scientific progress is self-similar (that is, fractal): each level of abstraction, from local problem solving to big-picture science, features progress of the “normal science” type, punctuated by occasional revolutions. The revolutions themselves have a fractal time scale, with small revolutions occurring fairly frequently (every few minutes for an exam-type problem, up to every few years or decades for a major scientific consensus). At the largest level, human inquiry has perhaps moved from a magical to a scientific paradigm. Within science, the dominant paradigm has moved from Newtonian billiard balls, to quantum, to evolution and population genetics, to neural computation. Within, say, psychology, the paradigm has moved from behaviorism to cognitive psychology. On smaller scales, too, we see paradigm shifts. For example, in working on an applied problem, we typically will start in a certain direction, then suddenly realize we were thinking about it wrong, then move forward, etc etc. In a consulting setting, this reevaluation can happen several times in a couple of hours. At a slightly longer time scale, we might reassess our approach to an applied problem after a few months, realizing there was some key feature we were misunderstanding. This normal-science and revolution pattern ties into a Bayesian workflow cycling between model building, inference, and model checking.</p></li>
<li><p><em>The multiverse.</em> The point of the "forking paths" metaphor in statistics is that multiple comparisons can be a problem, even when there is no "fishing expedition" or "p-hacking" and the research hypothesis was posited ahead of time. Indeed, often we can look at existing literature or even a single published article containing multiple studies to get a sense of the "multiverse" spanned by possible choices of data coding and analysis. Steegen et al.&nbsp;(2016) give an example of a literature in evolutionary psychology in which fertility was assessed in five different ways, menstrual onset was defined in three different ways, relationships were categorized in three different ways, and so forth, leading to 168 different options.</p></li>
<li><p><em>Forking paths are a good thing.</em> It is good to analyze data in different ways! The mistake is to choose just one. Rather than reporting the best result and then adjusting the analysis for multiple comparisons, we recommend performing all of some set of comparisons of interest and then using multilevel modeling to learn from the ensemble. This is what we mean when we say that we usually don't have to worry about multiple comparisons (Gelman, Hill, and Yajima, 2012). "Forking paths" can be taken as a criticism of naive interpretations of p-values; it is not a criticism of flexible data analysis and exploration in science.</p></li>
<li><p><em>Visualization of uncertainty.</em> Just as the individual beliefs and behaviors are best understood in a social context, probabilities are best understood in relation to the probabilities of other events. For example, in the 2020 U.S. election, Joe Biden was far ahead in national and state polls, but the probabilistic forecast needed to account for the possibility of systematic polling error. These graphs, which show probabilistic forecasts of Biden's electoral vote conditional on polling error, are more informative than unconditional distributions.</p></li>
</ul>
<p><img src="https://theory4ida.github.io/tmcf/posts/04-forkingpaths/uncertainty.png" class="" style="width:6.05933in;height:1.82557in"></p>
<p>In the event, the polling error was about 2.5 percentage points, and the final election was close.</p>
<ul>
<li><p><em>Variation.</em> Often what we learn from interactive data analysis are patterns of variation: a treatment that works in some settings but not others, geographic variation, behavioral differences between young and old people, and so forth. This is not about "forking paths" in the sense of different approaches to a single problem, but rather that data-driven science can lead us to see complexity, and this can be facilitated by modern workflows. To the extent that data graphics is automated and systematized (as with the grammar of graphics and the tidyverse in R), analysts can make graphs with less friction and will be more able to discover interesting and important variation.</p></li>
<li><p><em>Statistics as the science of defaults.</em> Applied statistics is sometimes concerned with one-of-a-kind problems, but statistical methods are typically intended to be used in routine practice. This is recognized in classical theory (where statistical properties are evaluated based on their long-run frequency distributions) and in Bayesian statistics (averaging over the prior distribution). In computer science, machine learning algorithms are compared using cross-validation on benchmark corpuses, which is another sort of reference distribution.</p>
<p>Statisticians have standard estimates for all sorts of models, books of statistical tests, and default settings for everything. Statistical software has default settings, and even the choice of package to be used could be considered a default. More generally, much of the job of statisticians is to devise, evaluate, and codify methods that will be used by others in routine practice.</p>
<p>Automatic behavior is not a bad thing! When we make things automatic, users can think at the next level of abstraction. For example, push-button linear regression allows researchers to focus on the model rather than on how to solve a matrix equation, and it can even take them to the next level of abstraction and think about prediction without even thinking about the model. As teachers and users of research, we then are (rightly) concerned that lack of understanding can be a problem, but it’s hard to go back. We might as well complain that the vast majority of people drive their cars with no understanding of how those little explosions inside the engine make the car go round.</p></li>
<li><p><em>Dense data and sparse model, or sparse data and dense model.</em> Tibshirani (2014) writes of the "bet on sparsity" principle: "The l1 methods assume that the truth is sparse, in some basis. If the assumption holds true, then the parameters can be efficiently estimated using l1 penalties. If the assumption does not hold—so that the truth is dense—then no method will be able to recover the underlying model without a large amount of data per parameter." This reasoning applies to a world in which data are dense and underlying reality is sparse, a setting that arises in many areas of science and engineering. For example, a surveillance video has a huge amount of information which can be summarized in very few dimensions as the motion of a few people over time; or a long gene sequence can be studied with the goal of classifying people into a small number of disease-risk categories.</p>
<p>In other applications, data are sparse and the underlying reality is dense. In social and environmental sciences, pretty much no effects being studied are zero—but many of these effects will be lost in the noise if we attempt to learn them from data. For such problems, we do not want to assume or bet on sparsity; rather, we should accept complexity and variation while recognizing the limitations of our data and models. If we do use regularization techniques that induce sparsity when working in social science and policy, we should not kid ourselves that we have discovered fundamental sparse structures. It is helpful here to consider the thought experiment, "What would happen if we got tons and tons more data?" In that case we would surely discover further structure in the world. At its best, data-driven science tells us what we can learn right now, not what can be discovered in the future.</p></li>
<li><p><em>Simulation-based experimentation</em>. Data analysis can be expensive in time and effort, and this can lead to us thinking that if a project took a lot of work then it has to be good. To state that belief is to mock it, yet it persists.</p>
<p>How can we avoid what might be called the "fallacy of effort"? We recommend simulation-based experimentation, which requires the following steps: (1) create a fake world, (2) simulate parameters and data from this world, (3) analyze the simulated data and get inference for the underlying parameters, (4) compare those inferences to the parameter values simulated in step 2. This can be done systematically in a Bayesian context (Modrák et al., 2024) but in practice informal checking can work just fine, in that problems will often show up in a simple simulation.</p>
<p>Creating a fake world is not easy—if analyzing a dataset is like playing Sim City, simulating fake data is like writing Sim City—but this effort can be well worth it, not just for the benefit of uncovering problems and the increased confidence arising from successful recovery, but also because constructing a simulation experiment is a way to clarify our thinking. Indeed, we often recommend simulating fake data before embarking on any real-world data collection process, to get a sense of what can realistically be learned from a proposed design.</p></li>
</ul>
</section>
<section id="some-thoughts-on-forking-paths-in-communication" class="level2">
<h2 class="anchored" data-anchor-id="some-thoughts-on-forking-paths-in-communication">Some thoughts on forking paths in communication:</h2>
<ul>
<li><p><em>Your most important collaborator</em>. Your most important collaborator is you, six months ago—and she doesn’t answer email. One implication of this principle is that presentation graphics should not be so different from exploratory research graphics. When graphing data just for yourself, you want to make the patterns as clear as possible, which is also what you want for other audiences. A clear message and purpose, a crisp and transparent design, readability—these things are important for you too.</p>
<p>More generally, the collaboration principle points to the value of understanding the paths of our interactive data analyses: this includes keeping some record of what is being done, along with the development of software that facilitates a workflow with parallel analyses.</p></li>
<li><p><em>What is the purpose of the methods section?</em> A frustrating aspect of science papers is that the methods section doesn't fully describe what was actually done. It can take a lot of sleuthing to figure out how to reconstruct published results---and that doesn't even get into all the things that got tried that didn't get written up! Even when you include any published supplementary information, you still typically don't see key details such as the wording and ordering of survey questions. Even if you set aside the possibility of scientific misconduct, people have difficulty writing up exactly what they did. With masters or doctoral thesis, you'll often find that the bulk of the thesis is review material: students are writing up the book they wish they'd been given to read at the outset of the project. Then when you get to the parts of the thesis that describe the new material, you won't see the data you need.</p>
<p>Why is it that researchers have such difficulty writing up exactly what they did? Setting aside fraud, writing up what you did should be the easiest thing to do! We have a couple of theories on this: (1) Students are used to reading textbooks and other materials written in general terms. It's natural for them to imitate that style when they start to write for publication; (2) The ultimate goal of science writing is to increase collective understanding, but the immediate goal is acceptance (by the journal editors, the thesis committee, the boss, or whoever decides whether the report goes forward). And, for various reasons, it doesn't seem that this acceptance requires or is even facilitated by a full and clear description of what you actually did.</p></li>
<li><p><em>Preregistration as a floor, not a ceiling.</em> There is a concern that preregistration stifles innovation: if Fleming had preregistered his study, he never would’ve noticed the penicillin mold, etc. Our response is that preregistration is a floor, not a ceiling. Preregistration is a list of things you plan to do, that’s all; it does not stop you from doing more. If Fleming had followed a pre-analysis protocol, that would’ve been fine: there would have been nothing stopping him from continuing to look at his bacterial cultures. It can be really valuable to preregister, to formulate hypotheses and simulate fake data before gathering any real data. To do this requires assumptions—it takes work!—and we think it’s work that’s well spent. And then, when the data arrive, do everything you’d planned to do, along with whatever else you want to do.</p></li>
<li><p><em>Honesty and transparency are not enough.</em> Reproducibility is great, but if a study is too noisy (with the bias and variance of measurements being large compared to any persistent underlying effects), that making it reproducible won’t solve those problems. Reproducibility (or, more generally, “honesty and transparency”) has been oversold, and we don't want researchers to think that, just because they drink the reproducibility elixir, that their studies will then be good. Reproducibility makes it harder to fool yourself and others, but it does not turn a hopelessly noisy study into good science. We want to be able to say that a particular project is hopeless without implying that the researchers involved are being dishonest. Lots of people do research that’s honest, transparent, and useless! That’s one reason we prefer to speak of “forking paths” rather than “p-hacking”: it’s less of an accusation and more of a description.</p></li>
<li><p><em>"Rigor" as a slogan and the Chestertonian principle.</em> Extreme skepticism is a form of credulity. This principle arises in politics, as with conspiracy theorists, and also in scientific method, where concerns of rigor can lead to a conceptual vacuum that is filled by something closer to pure speculation. Statistics textbooks will sometimes imply that causal inference is impossible without randomized experimentation and that population inference is impossible without random sampling—a position that is ridiculous given that real-world surveys of humans are almost never random samples or even close to that.</p>
<p>Rigor is important, though! Rigorous reasoning connects our analyses and conclusions to our theories (the trail of breadcrumbs mentioned earlier in this document). Understanding how our samples are <em>not</em> random is the first step toward adjusting for biases and quantifying possible errors. We should not think of rigor as being opposed to interactive data analysis.</p></li>
<li><p><em>Feeling disrespected</em>. Those of us who work in data visualization and data-analysis workflow have long felt disrespected by theoreticians and proponents of often-spurious rigor. This can be annoying. For example, a theoretical statistician once wrote, "The particle physicists have left a trail of such confidence intervals in their wake. Many of these parameters will eventually be known (that is, measured to great precision). Someday we can count how many of their intervals trapped the true parameter values and assess the coverage. The 95 percent frequentist intervals will live up to their advertised coverage claims." Maybe not! Based on the historical record, physicists’ intervals have not lived up to their advertised coverage (see Wasserman, 2008, and Gelman, 2008). Conversely, theorists can feel dissed by practitioners who don't recognize the ways in which applied work has benefited from theoretical understanding. The relevance to the present discussion is that when considering communication we need to consider some of the social background context of specific scholarly disputes. There is a sort of Escher stairway, in which visualization experts feel disrespected by theorists, and theorists feel disrespected by applied practitioners.</p></li>
<li><p><em>The conditions leading to the replication crisis in psychology and other fields.</em> Just as it is said that our modern megafires arise from having forests full of trees, all ready to ignite and preserved in that kindling-like state by firefighting policies that have favored preservation of existing trees over all else, so has the replication crisis been fueled by a decades-long supply of highly vulnerable research articles, kept in their pristine state through an active effort of leaders of the academic psychology establishment to suppress criticism of any published work. We are not claiming that psychology is worse than other fields; rather, psychology has lots of experiments which are easy to replicate (unlike in fundamentally observational fields such as economics and political science) and which are inexpensive in time, money, and lives (unlike in medicine or education research). Other fields also have woods that are ready to burst into flames, but the matches have not yet been struck in sufficient quantity.</p></li>
</ul>
<!-- -->
<ul>
<li><p><em>Goals/audience, solutions, self-criticism</em>. It is easy when working on a problem to jump in the middle. In our workflow we should remember to step back and consider our ultimate and proximate goals. The ultimate goal might be to make some policy decision or to crack some scientific problem; the proximate goal might be to bring a project to a conclusion—maybe better to say a stable intermediate state—so that it is publishable. And that is not a cynical goal: if research is worth doing, it's worth sharing. Also relevant is the audience. When writing, you should choose your target audience, while realizing that others may read your document too. And you should also criticize the solutions you are offering. Even in a purely positive presentation, criticisms can take the form of delineating the boundaries outside of which your solutions will not work.</p></li>
<li><p><em>The politics of the science reform movement</em>. The core of the science reform movement (the Open Science Framework, etc.) has had to make all sorts of compromises with conservative forces in the science establishment in order to keep them on board. Within academic psychology, the science reform movement arose from a coalition between radical reformers (who viewed replications as a way to definitely debunk prominent work in social psychology they believed to be fatally flawed) and conservatives (who viewed replications as a way to definitively confirm findings that they considered to have been unfairly questioned on methodological grounds). As often in politics, this alliance was unstable and has in turn led to "science reform reform" movements from the "left" (viewing current reform proposals as too focused on method and procedure rather than scientific substance) and from the "right" (arguing that the balance has tipped too far in favor of skepticism).</p></li>
<li><p><em>The importance of stories.</em> Storytelling is central to science, not just as a tool for broadcasting scientific findings to the outside world, but also as a way that we as scientists understand and evaluate theories. For this purpose, a story should be anomalous and immutable; that is, it should be surprising, representing some aspect of reality that is not well explained by existing models of the world, and have details that stand up to scrutiny.</p>
<p>This raises a paradox: learning from anomalies seems to contradict usual principles of science and statistics where we seek representative or unbiased samples. We resolve this paradox by placing learning-within-stories into a hypothetico-deductive (Popperian) framework, in which storytelling is a form of exploration of the implications of a hypothesis. This back-and forth connects to the above-discussed idea of the fractal nature of scientific revolutions and, more generally, to the forking paths of interactive data exploration.</p></li>
<li><p><em>The foxhole fallacy and the pluralist's dilemma</em>. In an article entitled, "No Bayesians in foxholes," the statistician Leo Breiman (1997) made the confident and false statement that, "when big, real, tough problems need to be solved, there are no Bayesians." It would be more accurate to say that Breiman was not aware of any such examples and indeed seemed to put in some effort to avoid finding them. What's funny is that he couldn't just say that he had made great contributions to statistics, and others had made important contributions to applied problems using Bayesian methods. He had to go beyond his expertise and exhibit the "foxhole fallacy," whereby someone does not seem to be able to believe that other people can legitimately hold views different from theirs. Related to this is the pluralist's dilemma: how to recognize that our approach is just one among many, that our own embrace of this approach is contingent on many things beyond our control, while still expressing the reasons why we prefer our approach to the alternatives (at least for the problems we work on). When considering scientific exploration and communication, we keep returning to this issue.</p></li>
<li><p><em>Taking political attitudes seriously</em>. A challenge in science communication is when people have preconceived notions and are not open to following the data or willing to accept empirical results. This is related to the "law of small numbers" fallacy identified by Tversky and Kahneman (1971), that there is an expectation that all evidence on a topic should go in the same direction.</p>
<p>When it comes to policy analysis, there are two ways to resolve this problem. From one direction, we want to develop tools for better communication of research results so that strong findings can be persuasive to skeptics (while continuing the work of science reform that is focused on assuring that weak evidence is not overstated). From the other direction, we have to accept that some stakeholders are not about to change their policy positions, perhaps because of legitimate external reasons. For example, a study could be performed estimating the economic effects of some social policy, but a policy maker might already favor (or oppose) the policy because of concerns of cost, ethics, or other outcomes. Even in a pure science context, a researcher might have a prior commitment to a line of research that is too strong to be shaken by any single study.</p>
<p>What to do when working with people who are expected to hold a fixed position? We propose to avoid the usual frustrations by accepting this position and flipping it around, asking the question: Given that this larger policy or theoretical position is fixed, how would these people incorporate new evidence into their understanding? The point is to avoid painting people into a corner. For example, suppose someone is committing to thinking that a certain drug treatment is a good idea, and then data come in showing no effect. Allow the believer to say something like, "Even if this drug does not work in this particular setting, I believe it works elsewhere," or "Even if this drug is ineffective, I support that a general policy of approving more treatments will on average lead to improvements by encouraging innovation," or whatever. There is no need to agree with such a position; the point is that this kind of exchange moves the discussion forward, rather than everything getting stalled on a refusal to accept new evidence or a refusal to discount a discredited evidential claim.</p></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Leo Brieman (1997). No Bayesians in foxholes. <em>IEEE Expert</em> 12 (6), 21-24.</p>
<p>Erik Gartzke (2007). The capitalist peace. <em>American Political Science Review</em> 51, 166-191.</p>
<p>Andrew Gelman (2004). Exploratory data analysis for complex models (with discussion). Journal of Computational and Graphical Statistics 13, 755-779.</p>
<p>Andrew Gelman (2008). Objections to Bayesian statistics (with discussion and rejoinder). <em>Bayesian Analysis</em> 3, 445-477.</p>
<p>Andrew Gelman, Jennifer Hill, and Masanao Yajima (2012). Why we (usually) don't have to worry about multiple comparisons. Journal of Research on Educational Effectiveness 5, 189-211.</p>
<p>Andrew Gelman and Eric Loken (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no "fishing expedition" or "p-hacking" and the research hypothesis was posited ahead of time. <a href="http://stat.columbia.edu/~gelman/research/unpublished/forking.pdf">link to pdf</a></p>
<p>Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Paul-Christian Bürkner, Lauren Kennedy, Jonah Gabry, and Martin Modrák (2020). Bayesian workflow. <a href="http://stat.columbia.edu/~gelman/research/unpublished/Bayesian_Workflow_article.pdf">link to pdf</a></p>
<p>Jessica Hullman and Andrew Gelman (2021). Designing for interactive exploratory data analysis requires theories of graphical inference (with discussion). Harvard Data Science Review 3 (3).</p>
<p>Martin Modrák, Angie H. Moon, Shinyoung Kim, Paul Bürkner, Niko Huurre, Kateřina Faltejsková, Andrew Gelman, and Aki Vehtari (2024). Simulation-based calibration checking for Bayesian computation: The choice of test quantities shapes sensitivity. Bayesian Analysis.</p>
<p>Robert Tibshirani (2014). In praise of sparsity and convexity. In <em>Past, Present, and Future of Statistical Science</em>, ed.&nbsp;Xihong Lin, Christian Genest, David Banks, Geert Molenberghs, David Scott, and Jane-Ling Wang. CRC Press.</p>
<p>John W. Tukey (1972). Some graphic and semigraphic displays. In <em>Statistical Papers in Honor of George W. Snedecor</em>, ed.&nbsp;T. A. Bancroft. Iowa State University Press.</p>
<p>Amos Tversky and Daniel Kahneman (1971). Belief in the law of small numbers. <em>Psychological Bulletin</em> 76, 105-110.</p>
<p>Larry Wasserman (2008). Comment on "Objections to Bayesian statistics," by Andrew Gelman. <em>Bayesian Analysis</em> 3, 463-465.</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{gelman2024,
  author = {Gelman, Andrew},
  title = {Forking Paths and Workflow in Statistical Practice and
    Communication},
  date = {2024-06-23},
  url = {https://theory4ida.github.io/tmcf/posts/04-forkingpaths/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-gelman2024" class="csl-entry quarto-appendix-citeas">
Gelman, Andrew. 2024. <span>“Forking Paths and Workflow in Statistical
Practice and Communication.”</span> June 23, 2024. <a href="https://theory4ida.github.io/tmcf/posts/04-forkingpaths/">https://theory4ida.github.io/tmcf/posts/04-forkingpaths/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/04-forkingpaths/</guid>
  <pubDate>Sat, 22 Jun 2024 23:00:00 GMT</pubDate>
  <media:content url="https://theory4ida.github.io/tmcf/posts/04-forkingpaths/brecon.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
