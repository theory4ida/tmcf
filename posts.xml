<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>TMCF 2024</title>
<link>https://theory4ida.github.io/tmcf/posts.html</link>
<atom:link href="https://theory4ida.github.io/tmcf/posts.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.555</generator>
<lastBuildDate>Tue, 25 Jun 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Some risks and opportunities of automated data analysis</title>
  <dc:creator>Daniel Archambault</dc:creator>
  <dc:creator>Roger Beecham</dc:creator>
  <dc:creator>Andrew Gelman</dc:creator>
  <dc:creator>Jessica Hullman</dc:creator>
  <dc:creator>Edwin Pos</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/models/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>We have all been there: you have your research question, you’ve gathered your dataset, and you are ready to start digging. Or perhaps you only have a question and are wondering how you could answer it in the first place given all the options available. Ever since the invention of data analysis, the human analyst has held the primary role of thinking, deciding and analyzing. At the same time, there has been a steady progression toward automating parts of statistical workflow that are better done by machines, such as calculation. With impressive recent advancements in general purpose intelligent assistants like chatbots, we are in a better position than ever to imagine what an intelligent assistant who acts as a helpful collaborator during data analysis might look like. These new possibilities provide an opportunity to reflect on the ideal interaction between statistical tools and human knowledge. What are the risks involved with increasing amounts of automation in data analysis steps like problem specification, data collection, model specification and selection, and interpretation of results? What are the opportunities? How can we ensure that the sum of human knowledge and AI prediction in data analysis will be more than its parts?</p>
<p>To formalize the problem slightly, one could say that data analysis has traditionally involved applying some sequence of operations on data (e.g., reductions or visualizations) which we shall express as functions f = (f<sub>1</sub>, f<sub>2,</sub>, …, f<sub>n</sub>) to data D, each of which generates some intermediate output y<sub>i</sub> = f<sub>i</sub>(D) to produce some ultimate knowledge output or interpretation K = f<sub>human</sub>(y), where f<sub>human </sub>represents a human processing y = (y<sub>1</sub>, y<sub>2</sub>, …, y<sub>n</sub>). For example, a conventional visual analytics workflow involves a human selecting and applying to data some set of queries or operations f(D) (which might consist of filters, aggregations, regressions, etc.), then interpreting the output of these functions to produce some interpretation or decision K.</p>
<p>As we consider how this simple formulation changes with increasing levels of automation, we propose this gives rise to a spectrum. On the one end you have a human-directed extreme, such as in conventional visual analytics (Keim et al.&nbsp;2008), where the human has chosen the specific functional form of the members of f to apply to the data given their prior knowledge to find K, and automation is involved primarily to perform what would otherwise be tedious calculations.</p>
<p>In the current age of technological development, where products like ChatGPT are <a href="https://help.openai.com/en/articles/8437071-data-analysis-with-chatgpt">promoted for data analysis</a>, from insight discovery to communication, we are closer to being able to imagine the other extreme of this spectrum: fully automated data analysis. In this case, the human only asks the question or provides a goal and optionally some data D. The remainder of the process is automated, concluding with the machine presenting its interpretation K (i.e., K=f<sub>assistent</sub>(y)). The human is not directly involved in selecting f<sub> </sub>nor in processing the intermediate outputs of y. Consequently, the selected f may not be interpretable to a human in the sense of being explainable via human-meaningful parameters.</p>
<p>Although it may seem to some readers to be immediately apparent that removing the human from data science is a bad idea, it is not at all clear that the current extent of automation in data analysis workflows is sufficient. Integrating more machine intelligence may help address human limitations we have had no choice but to accept in data analysis. For example, the human will be limited by their own experience, and may only be able to see a limited set of all the possible operations or functions that could be applied. Or, information processing biases may lead them to misperceive y, resulting in a non-optimal K even if their selection of f was in fact optimal. There would seem to be many ways that using AI could improve analysis outputs in light of the limited knowledge and experience that any human analyst might have.</p>
<p>We propose that thinking about the spectrum from fully human-driven to fully machine-driven analysis is helpful in several ways. First, by explicitly reflecting on what the ideal role of the human is in data analysis, we gain insight into what aspects of data analysis we think are truly human or non-automatable, and which are better given to the machine. Second, as research and practice increasingly makes use of predictive modeling, we are in a better position to apply our expertise on statistical modeling to design better futures for interactive data analysis. Below we explore this spectrum from its two extremes.</p>
</section>
<section id="the-human-directed-extreme" class="level2">
<h2 class="anchored" data-anchor-id="the-human-directed-extreme">The human-directed extreme</h2>
<p>At one end of the spectrum, the human selects the functions to be applied, processes the outputs of those functions more or less manually (i.e., without necessarily using formal decision rules), and maps the beliefs that they form to an interpretation. We assume that the human has access to visual analytics processes and technology (Keim et al., 2008). By this we mean that the human does not perform the calculation themselves (we aren’t worried about mathematical errors in generating the individual y<sub>i</sub>), and they have access to visual analytics systems that perform the calculations and present the answers in a visual form for interpretation through f<sub>human</sub>. We assume the human has some level of understanding of how the particular set of functions work and what they mean. In other words, f consists of interpretable operations, such as parameterized functions f<sub>theta</sub> where theta is considered meaningful to the human, or simple deterministic functions like finding the maximum of data series. It is worth noting that despite the interpretability of the operations applied, how exactly the human processes the available information (i.e., f<sub>human</sub>) can be thought of as a black box of sorts, as the generation of y is guided implicitly by the previous experience and domain specific knowledge of the human in addition to the data. It is assumed that the human knows how to best apply their domain knowledge in this scenario.</p>
<p>We can loosely analogize the learning problem to Bayesian decision theory for the purpose of identifying failure points. Assume some high level learning goal given some observed dataset; for example, an analyst hired by a large school district might be investigating the question, What factors explain the dip in high school math performance between 2018 and 2020? While normally in applied Bayesian analysis we would focus on how posterior beliefs about the some parameters deemed meaningful are arrived at within the context of some particular model specification, here we will instead think of the knowledge gained from the entire analysis workflow as resulting from some combination of the analyst’s prior knowledge and beliefs, the data at hand, and modeling assumptions. Assume for example that what the analyst finds is intended to inform the school district’s decision about what to invest next year’s budget in. The human analyst will likely bring some relevant domain knowledge (prior beliefs) on which variables are likely to matter and how much, how they should model the data to achieve their goals, what aspects of context should influence their interpretation, etc. For example, maybe they have prior experience on factors that predict high school test scores, biases that exist in available data, contextual knowledge about the specific school district, etc. What they know about such problems will influence the statistical modeling approach they select (e.g., a multiple regression predicting 11th grade standardized math test performance from some particular set of covariates). The output of the models and operations they apply inform their posterior beliefs about which factors influence student performance and how this information should inform a decision.</p>
<p>If the analyst were fully Bayesian, they would optimally combine the new information they learn with their prior beliefs to arrive at posterior beliefs, then select the utility maximizing interpretation or decision from a space of possible decisions. But a number of things can go wrong when dealing with boundedly rational agents. The analyst’s prior knowledge itself might be biased or incomplete, leading them to analyze the wrong data, fail to select an appropriate set of operations f, etc. They might make bad assumptions in model specification or selection, leading to misleading modeling outputs. They might overconstrain their analysis based on their priors (or discount them too easily) by taking advantage of degrees of freedom, for example if they are predisposed to prefer a certain interpretation leading to biased beliefs they might settle on a particular fitted model because it aligns with these preferences. They might arrive at the wrong interpretation or decision in light of their beliefs.</p>
<p>On the other hand, there may be advantages to the fully human case. The human understands exactly what was applied in f and drives the interpretation of y, enabling them to apply domain knowledge in a flexible, unscripted way. Their understanding and experience with the functions or models they apply makes it easier for them to debug issues, and they may feel more confident about the interpretation they ultimately arrive at. They have the ability to reformulate the goals and problem upon viewing results at any point, for example if they originally failed to consider some important data. More broadly, if the purpose of most data science is to generate insight from data for human purposes, in the fully human-driven case we are 100% sure that data science is happening as every step was done by and consumed by the human, providing an opportunity for understanding, critique, and reconsideration at each step.</p>
</section>
<section id="the-fully-automated-extreme" class="level2">
<h2 class="anchored" data-anchor-id="the-fully-automated-extreme">The fully automated extreme</h2>
<p>At the fully automated extreme, the human provides an analysis goal and optionally data. When considering the Keim et al.&nbsp;model of visual analytics (Keim et al.&nbsp;2008), the assistant has complete control over all stages, including automated processing of knowledge. In the extreme, the assistant simply returns the answer, K, without any justification or provenance of the analysis. For the purposes of this post, we assume the assistant optimally combines what can be gleaned from all available prior statistical and scientific modeling examples in an autoregressive framework in which its task is to predict the most appropriate sequence of steps given the human’s prompt.</p>
<p>Several risks arise from the training constraints on the assistant. For example, one risk arises from how the assistant’s suggestions will be constrained by what it has seen, which in the best case consists of all prior observed statistical analyses. The model cannot necessarily suggest an altogether new form of analysis unless it in some way represents a combination of previous observed analyses. This raises the question of whether and when we might expect completely new analysis paradigms to emerge which could not somehow be reconstitutions of existing ideas.</p>
<p>Relatedly, the assistant is constrained to suggesting what is most probable. Ask any statistician if they expect frequencies within a corpus representing all historical examples of statistical analysis to capture the appropriateness of a given analysis path and they will answer with a resounding no. If ritualistic choice of models and interpretation of results is a problem in the fully human directed case, then any agent restricted to optimizing an autoregressive objective over some corpus of training data has the potential to miss something more appropriate but less prevalent in the training data. To some extent modern machine learning pipelines can overcome these biases through processes like fine-tuning, where a small amount of preference data is collected after training an unsupervised foundation model and used to adjust the conditional distribution of the model’s output. However, fine-tuning for particular cases implies a form of interaction that would seem to contradict the extreme fully-automated case: if we expect analyses to improve through interaction to obtain further context- or expert-specific input, we would seem to be advocating for an interactive dialogue between the human user and the assistant.</p>
<p>Relatedly, the fully automated scenario assumes that the human, at the time of prompting, is able to specify their true analysis goal, and in doing so will know how to customize the prompt to contain any relevant domain knowledge they have. But this begins to sound a lot like our trust that the human knows best how to apply their knowledge in the fully human-driven case. Is this reasonable? Or does the real human analyst require intermediate outputs along the way in order to cue lessons from their prior knowledge and experience? Consider how often real analysis workflows involve a shifting of direction. Upon viewing representations of our data in early Exploratory Data Analysis, we might realize our misconceptions of what it contained. Perhaps the fidelity of information we thought we could capture (and which we need to achieve our analysis goals) is simply not possible, and we must rethink our questions altogether. Or, we might realize upon reviewing model diagnostics that there is little signal in the variables we have collected, but have an idea of what other features we could collect. Can we imagine data analysis without such flexibility? Is it possible that an assistant could anticipate such changes in direction? The answer in the extreme case is simply returned without any reason or process to derive the answer, hence no obvious recourse for the human to take further action. Thus, knowledge is gained in some sense if they choose to trust it, but that might be hard.</p>
</section>
<section id="charting-a-generalized-path-through-extremes-of-y" class="level2">
<h2 class="anchored" data-anchor-id="charting-a-generalized-path-through-extremes-of-y">Charting a generalized path through extremes of y</h2>
<p>The above reflections suggest the ideal amount of machine assistance in data analysis will lie somewhere between the two extremes, and may differ depending on the expertise of the particular human and the specific context. However, we could say that the goal in finding the sweet spot is to identify the points where the human’s imagination holds them back from realizing a better path. If good statistical practice involves judicious use of computation to extend the human’s ability to imagine possible outcomes (e.g., alternative values for a statistic resulting from bias or sampling error, counterfactuals in causal inference, equivalently performing models in machine learning, etc.), a computational assistant who can entertain many models or theories simultaneously provides opportunities to “amplify” human cognition, a stated goal of visualization.</p>
<p>But how much should an assistant prompt the human to imagine outside their comfort zone? Attempting to design the optimal human-machine pairing naturally motivates reflection on the extent to which we want data-driven science to be pluralistic, allowing for different beliefs and conventions when it comes to how to best learn from data. Consider a familiar tension in the field of data analysis between inference, theory and explanation, and prediction. Researchers and practitioners in different fields and domains vary in how much they value each. It would seem that an optimal assistant would need to adjust to the specific problem in ways congruent with domain-specific values. In social and natural science applications, the assistant might, for example, behave like a scientist from those traditions, where the generated y (and consequently K derived from y) is grounded in substantive theory. In Operations research where prediction is typically the goal, we might judge possible outputs purely in terms of best out-of-sample performance, leading to a much more quantitative (minimal) inspection of y to arrive at K.</p>
<p>Paradigmatic differences leave us with many questions around how the assistant should be constrained or configured to realize the goals of scientific knowledge development. We should think about what level of constraints are tolerable and ultimately desirable for learning in a domain, which is a question about how comfortable we are with letting an assistant push us outside our comfort zones as scientists. If the social or natural scientist fine-tunes or otherwise configures the assistant only in a way that reproduces existing practice, the risk is simply reproducing the status quo. What level of “expanded” imagination is desirable for the purpose of scientific progress? For example, should researchers working in domains that prioritize explanatory approaches be given recommendations informed by predictive modeling, as proposed by Hofman et al.&nbsp;(2021) in advocating for integrative modeling? Are there unified goals, constraints or features of analysis – ways of evaluatin and thinking about the y that gives rise to K – that are independent of domain, or are we ultimately constrained to work only within a particular modeling paradigm because historically boundaries have existed? To what extent is explicit design toward achieving specific features/goals of scientific analysis and different standards of evidence (Hofman et al., 2021) useful in constraining the assistant?</p>
<p>For example, to what extent should the ideal pairing of the human and machine for data science seek to prioritize:</p>
<ul>
<li><strong>Substantive theory </strong>— Extent to which the K that is derived supports, validates and extends knowledge.</li>
<li><strong>Reproducibility and transfer </strong>– Extent to which K is reproducible - for example, is it invariant under perturbations we do not believe should substantively change results, such as slight variations in how the assistant is called? Notions of replicability may also matter: when should a closely related analysis on similar datasets in the same domain produce an analogous result?<br>
</li>
<li><strong>Transparency</strong> – Extent to which K that can be easily understood and interrogated.</li>
<li><strong>Coverage and generalizability</strong>– Extent to which K encompasses a narrow or wide range of settings (contexts, scales, etc.).</li>
<li><strong>Expansiveness </strong>– Extent to which K extends imagination, or that enables the analyst to push beyond bounds of inherited modeling and data paradigms.</li>
</ul>
<p>Different domains and use cases might naturally have different weightings on each of these features, leading to different modeling paradigms. Where do we find ourselves on the spectrum, and to what extent might we expect an intelligent assistant to be informed by knowledge of other points along it, so as to push analysts in a given domain outside of their comfort zone?</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>There are many other risks and benefits of the human-directed and automated extremes than those discussed above. However, several themes arise even from our partial treatment. One is that it seems unlikely that the optimal analysis approach is either fully human-directed (with the primary form of automation being calculation as assumed above) or nearly fully automated, with the human providing only the high level goal and optionally, some data. Without any visibility into how K was produced, the human has no opportunity to apply knowledge that is not contained in the training data to debug operations chosen by the machine. They may not feel confident in their results, and their lack of insight into how they were reached may prevent them from applying the knowledge that is output, leading to a question of whether it is knowledge at all.</p>
<p>At the same time, a machine that can run many analyses simultaneously, including approaches the human may not know of or be familiar with, has the potential to result in much more informed interpretations of data. There are many blindspots in human analysis. Forms of model multiplicity (the fact that we can get the same model fit or performance from models that imply very different interpretations of a phenomena) and sources of uncertainty (e.g., about how good our assumptions are) are routinely overlooked. In general, it is unlikely that either extreme will be sufficient, prompting a number of questions about how explicitly an analysis assistant should prompt a human analyst to think more expansively than they otherwise might. Key challenges lie in identifying when, how, and why to elicit human knowledge, so as to show the analyst what they may miss, and how to communicate results from potentially non-human interpretable operations in ways that humans can understand. In the current age of technological advancement with increasingly advanced automated methods we expect the sum will indeed be more than its parts.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>Hofman, J.M., Watts, D.J., Athey, S. <em>et al.</em> Integrating explanation and prediction in computational social science. <em>Nature</em> 595, 181–188 (2021). https://doi.org/10.1038/s41586-021-03659-0</p>
<p>Hullman, J., and Gelman, A. (2021). Comparing human to automated statistics. Section 6 of Designing for interactive exploratory data analysis requires theories of graphical inference, Harvard Data Science Review 3 (3).</p>
<p>Hullman, J., Holtzman, A., and Gelman, A. (2023). Artificial intelligence and aesthetic judgment. http://stat.columbia.edu/~gelman/research/unpublished/AI_aesthetic_judgment.pdf</p>
<p>Keim, D., Andrienko, G., Fekete, JD., Görg, C., Kohlhammer, J., Melançon, G. (2008). Visual Analytics: Definition, Process, and Challenges. In: Kerren, A., Stasko, J.T., Fekete, JD., North, C. (eds) Information Visualization. Lecture Notes in Computer Science, vol 4950. Springer, Berlin, Heidelberg. <a href="https://doi.org/10.1007/978-3-540-70956-5_7">https://doi.org/10.1007/978-3-540-70956-5_7</a></p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{archambault2024,
  author = {Archambault, Daniel and Beecham, Roger and Gelman, Andrew
    and Hullman, Jessica and Pos, Edwin},
  title = {Some Risks and Opportunities of Automated Data Analysis},
  date = {2024-06-25},
  url = {https://theory4ida.github.io/tmcf/posts/tools/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-archambault2024" class="csl-entry quarto-appendix-citeas">
Archambault, Daniel, Roger Beecham, Andrew Gelman, Jessica Hullman, and
Edwin Pos. 2024. <span>“Some Risks and Opportunities of Automated Data
Analysis.”</span> June 25, 2024. <a href="https://theory4ida.github.io/tmcf/posts/tools/">https://theory4ida.github.io/tmcf/posts/tools/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/models/</guid>
  <pubDate>Tue, 25 Jun 2024 00:00:00 GMT</pubDate>
  <media:content url="https://blog.sciencemuseum.org.uk/wp-content/uploads/2013/12/Alan-Turing-29-March-1951-picture-credit-NPL-Archive-Science-Museum1.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Navigating the Foggy Garden of Forking Paths</title>
  <dc:creator>Benjamin Bach</dc:creator>
  <dc:creator>Hadley  Wickham</dc:creator>
  <dc:creator>Jo Wood</dc:creator>
  <dc:creator>Kai Xu</dc:creator>
  <link>https://theory4ida.github.io/tmcf/posts/tools/</link>
  <description><![CDATA[ 





<p><img src="https://jwolondon.github.io/forkingPaths/images/gardenPhoto.jpg" class="img-fluid" alt="A garden of forking paths"></p>
<blockquote class="blockquote">
<p>Under the trees of England I meditated on this lost and perhaps mythical labyrinth. I imagined it untouched and perfect on the secret summit of some mountain; I imagined it drowned under rice paddies or beneath the sea; I imagined it infinite, made not only of eight-sided pavilions and of twisting paths but also of rivers, provinces and kingdoms … I thought of a maze of mazes, of a sinuous, ever growing maze which would take in both past and future and would somehow involve the stars.</p>
</blockquote>
<p>— <em>The Garden Of Forking Paths, Jorge Luis Borges, 1941</em></p>
<p><strong>What is a good metaphor for data analysis?</strong> In the spirit of Borges, Pólya, Gelman and many others since, we’re going to run with this one: <em>navigating the forking paths of a foggy garden.</em> It captures our belief that analysis involves deciding between (navigating) a set of discrete decisions (the forks) across different analysis options (the paths) in a continuous landscape of possibility (the garden) that becomes incrementally revealed as the analysis progresses (the fog).</p>
<p>Analysis is not a murder mystery or a detective challenge. Typically there is not one objective Truth to be discovered, but many potential possible discoveries. When presenting a final analysis the paths taken can be just as important as the final destination.</p>
<p>At best, a metaphor is a tool for thinking. We don’t believe that this metaphor captures every aspect of the problem at hand, but perhaps it illuminates components to guide the analyst through their workflow, encouraging critical thinking, suggesting where future tools may assist. We are also cautious: a danger with any metaphor is that it replaces something we don’t fully understand with something we think we do. It may lead to hubris. Nevertheless, stick with us as we dig a little deeper and consider what it might bring to our understanding of the data science process.</p>
<section id="interrogating-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="interrogating-the-metaphor">Interrogating the metaphor</h2>
<p>We’ll begin by exploring five components of the metaphor (navigation, the fog, the garden, the forks, and the paths), how it ties to the practice of data analysis, where it suggests interesting angles to consider, and where it might constrain our thinking.</p>
<section id="navigating" class="level3">
<h3 class="anchored" data-anchor-id="navigating">Navigating</h3>
<p>To perform an analysis we navigate the pathways, making decisions as we explore the problem. We navigate in different ways at different times. Often we will be walking an analysis, plodding from one foot to the other as each step reveals a little more of the path. But at times we may be running in as we enter a flow state or repeat a well-used protocol. Or we may take intuitive leaps that transport us to quite different parts of the garden. Or perhaps we are <em>flaneurs</em>, wandering for its own sake, with no purpose other than to subject ourselves to the whims of serendipity.</p>
<p>Regardless of how we travel, it is a rare analysis that proceeds in a straight line along a pre-determined route. Much real analysis involves exploration – winding paths with many wrong turns and dead ends. Even decisions late in the analysis can provide surprising insights that lead us to reconsider our initial steps.</p>
<p>Once we have done our analysis, getting to a point of interest after taking many wrong turns, we may choose to describe our journey to others. Perhaps we focus only on the start and destination. Or perhaps we write a guided tour (for example as an academic publication), quickly leading the reader to the points of interest while gliding past the less interesting scenery. Scientific honesty may compel us to provide more of a travelogue: did we know what we were looking for before we set out, and how long did we spend hunting for something of interest? Or we might reflect on our own journey as a reflexive exercise; would we take the same paths if we were to start again?</p>
</section>
<section id="fog" class="level3">
<h3 class="anchored" data-anchor-id="fog">Fog</h3>
<p>A fog implies we can’t see much of the garden from our current location: we have to make decisions and choose paths in order to reveal future choices. Our decisions are guided by incomplete information leading us frequently to backtrack; often a path that looked promising at first glance is later revealed to be a dead end. We might need to map our passage as we proceed to avoid getting lost. Or perhaps we rely on maps to unseen destinations left by others.</p>
<p>The fog is not constant; in some places it is thicker, giving us little clue about where to travel next. Experience may give our eyes more power to penetrate the fog, as past analyses help us better predict which paths are likely to lead to bountiful places.</p>
<p>Some destinations will be obvious from a distance; tall trees reaching above the fog. Others will be serendipitous, stumbled upon, fallen from the sky, and discovered among the bushes.</p>
</section>
<section id="garden" class="level3">
<h3 class="anchored" data-anchor-id="garden">Garden</h3>
<p>The garden implies a space that’s a mix of the human and natural, tempting us with bounty to be discovered. Depending on what you are picturing, that bounty might be a succulent tomato in a vegetable garden; an attractive fountain in a formal garden; the symmetrical layout of Italianate beds; or a prize statue at the centre of a hedge maze. Do these metaphorical objects help frame and evaluate our objectives and discoveries?</p>
<p>What role does the topography of the garden play? Perhaps ‘height’ is the optimality of the analysis, where the shape of the garden landscape captures the effectiveness of the underlying analysis. Is the garden flat with a multitude of minor undulations? Is it tended on a mountain, where no matter where you start, there’s an obvious destination at the summit? Or does this garden span a mountain range with multiple peaks connected by ridges and passes, separated by deep ravines.</p>
<p>But in a foggy landscape we may not have a clear notion of ‘optimal’. Perhaps height could mean something else. Leaning on the metaphor, gradient could represent the ease with which we can progress. Moving downhill follows established good practice, gathering momentum from the familiar. Perhaps the tools we use lead us downhill, affordances that encourage some approaches at the expense of others. Or perhaps the fog is less dense in the heights so that travelling uphill allows us to see further.</p>
<p>The notion of landscape emphasises continuity. Perhaps it is unhelpful to think of discrete decision-making; instead we have at our disposal choices from an infinite range of possibilities. We might continuously vary some model parameters and describe the effect on our journeys. Sensitivity analysis might describe how critically dependent our location in the garden is on how we move.</p>
</section>
<section id="forking" class="level3">
<h3 class="anchored" data-anchor-id="forking">Forking</h3>
<p>A forking path in garden of possibility implies there are choices to be made, and that those choices are discrete and finite. Let’s put aside for the moment that we could freely wander about the garden and instead we will keep off the grass.</p>
<p>The existence of a network of paths implies they have some topology. We suggest that understanding that topology is itself instructive. For example, those paths might form a <em>tree</em>: A single start point leading to multiple possible outcomes. At each stage in the decision-making process, more choices are opened up. Alternatively, that tree might be rooted in a single outcome where multiple start points lead to the same destination. Such a topology might give us confidence that our conclusions are robust; that even with changes in methodological decision-making, all paths lead to the same result. But perhaps such an arrangement should signal caution; that our methods are too restrictive, leading only to a limited set of possible outcomes regardless of the data that informs them.</p>
<p>The network of forking paths may form non tree-like patterns. For example, cycles that take us back to previous decision points as part of an iterative process. Or perhaps we have unconnected paths, ones that exist in the same garden of possibility but where we fail to make connections between them.</p>
<p>An understanding of the network of forking paths may lead to insight not just about the outcomes, but the process followed and the way it shapes the validity of findings. Perhaps these are oversimplistic or poorly fitting characterisations of different forking paths. We are intrigued by the possibility that the structure of these networks of choices are instructive in reflecting on the data science workflow by providing a rich and shareable account of the analytic process.</p>
</section>
<section id="paths" class="level3">
<h3 class="anchored" data-anchor-id="paths">Paths</h3>
<p>If a decision-point is a fork in a network of data science activity, what is the path? Does the metaphor suggest different types of path whose characterisations result in insight or meaningful consequence?</p>
<p>We take a path to represent some chosen activity, perhaps the cleaning of a dataset, the specification of a visualization design, or the populating of a simulation model. It is what we might traditionally document when explaining our work.</p>
<p>What might the ‘width’ of a path mean? Consider the popularity of a well-trodden path. Perhaps we are following a process adopted by many others, following established good (or even poor) practice. A narrow path might represent a break from convention - activity without precedence, <em>novelty</em> taking us somewhere few have visited before. Or perhaps we are pioneering a new shortcut to a place many others have visited. Characterising path width might help signal to us where we exercise methodological scrutiny. Examining wide paths might prompt us to question convention, or at least its fit to the problem at hand.</p>
<p>Our paths move us through the garden of possibility. A <em>perilous</em> path might be one that traverses a precipitous edge in that garden - the slightest misstep rapidly takes us somewhere quite different to our intended destination. Should we avoid peril? Or can it take us to places otherwise inaccessible?</p>
<p>In contrast, a wide road at the bottom of a ravine might be difficult to leave. Any deviations from convention are quickly corrected. When should we stick to these safe routes, embracing the confidence given to us by others? When should we worry that are we being unnecessarily constrained by convention and denying ourselves the opportunities of novelty and new perspectives.</p>
</section>
</section>
<section id="working-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="working-with-the-metaphor">Working with the metaphor</h2>
<p>What might the metaphor mean in the data science workflow carried out by Jan – an imaginary data scientist looking to predict the outcome of a forthcoming national election?</p>
<p>Jan has at her disposal the results of a several national opinion polls that ask a sample of voters how they would vote in the election. She has a choice: Which of the polls does she use for her analysis? Acknowledging the existence of that choice helps to signal the possible impact of data sources on eventual conclusions.</p>
<p>She chooses one of the polls and <em>signposts</em> this was just one of several choices at this point. She wishes to visualize the distributions of the polling data. Again she has a choice - should she produce a map of the results emphasising geographical patterns, or a sequence of histograms emphasising the shapes of the distributions. Or she might choose both and compare what they tell her.</p>
<p>But what if she’d chosen a different opinion poll? Would that have led ultimately to different conclusions? Or what if she had chosen to represent the results in tabular format and not a map? Or what is the effect of choosing a choropleth over a cartogram?</p>
<p>Her path through the garden of possibilities are captured as a tree – from a single starting point (a) through the choice of polling data to use (b, c or d) and choices of output maps, charts and tables (e, f, g or h).</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/tree1.png" width="400" alt="A simple tree"></p>
<p>The characterisation of the topology of these choices tells us (and her) something about the process she is following. Depending on how different her interpretations e, f, g and h are, this might indicate the robustness of her conclusions.</p>
<p>Jan isn’t the only person working with election data. Perhaps her team started with different objectives, one emphasising the polling analysis capabilities of the team to market their services. Another focussed on assessing the accuracy of their prediction results. Another in providing commentary for a national news outlet. Perhaps their decision paths looked like this:</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/tree2.png" width="400" alt="Inverted tree"></p>
<p>It suggests a degree of <em>equifinality</em> – different paths lead to the same outcome (g) even when starting from different places. Different polling data or analytical processes lead to the same predicted result. It might suggest greater confidence in the prediction. But it also signals the possiblity that results are an artefact of misapplied analytical approaches that led to the same arbitrary result.</p>
<p>Jan decides to build a more sophisticated <a href="https://en.wikipedia.org/wiki/Multilevel_regression_with_poststratification">MRP model</a> to predict election results. She stratifies opinion poll results by the demographics of the respondents, runs the model to generate some output distributions and visualises the output of the model emphasising the uncertainty captured by the model. In parallel her team-mate, Arun, runs his own MRP based on a different set of data and with different parameters weighting demographics. Even though this too generates distributions of possible predictive outcomes, they are quite distinct from Jan’s MRP results.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/forest4.png" width="400" alt="Unconnected networks"></p>
<p>It allows the team to question why their results are so distinct. Even Arun’s exploration of model parameterisation (<em>h</em> and <em>i</em>) that had initially given him a sense of the variance in the model outputs is challenged by the contrast with Jan’s MRP outcome (<em>d</em>).</p>
<p>For a later project, under less time constraint, Jan uses intermediate results from her analysis to revisit earlier assumptions and choices in a more iterative and reflexive process. It takes place in a foggier garden where she can’t see her destination until it emerges after several iterations.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/graph3.png" width="300" alt="Directed graph with loops"></p>
<p>By examining this topology Jan is able to see the effect of iterating over her process, to examine how her workflow is shaping the conclusions she draws, where iteration is more or less effective.</p>
</section>
<section id="thinking-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="thinking-with-the-metaphor">Thinking with the metaphor</h2>
<p>Our primary goal here is to present a metaphor that we find useful for framing data analysis. But the metaphor is only as useful as the tools and thinking it provokes, and while we don’t (yet) offer any concrete tools inspired by this metaphor, our discussion included many ideas that offer promise.</p>
<p>We’ve grouped our ideas into two broad categories which we explore below: navigating and documenting the journey.</p>
<section id="navigating-1" class="level3">
<h3 class="anchored" data-anchor-id="navigating-1">Navigating</h3>
<p>Our metaphor is physical and geographical: we are navigating forking paths in a garden. That physicality leads us to contemplate a set of questions:</p>
<ul>
<li><p>What areas have we visited and what areas have we missed? Where should we go and what areas should we avoid? We probably don’t start our journey with a map, but we do want to build one as we go. Such a map might be detailed, surveying the terrain and recording landmarks and findings. Or perhaps the map is schematic, carefully documenting the topology of paths taken, ignoring their length and distance between them. Or perhaps it is more experiential: “here be dragons”.</p></li>
<li><p>Am I walking alone or as a team? Do we walk the same path together or do different members take a turn at leading the navigation? Do we divide and conquer, starting from different locations and walking different paths? How do we share information on where we have been and where we’re contemplating next?</p></li>
<li><p>As we travel, we might notice interesting pathways that we don’t have the time to take right now. How can we leave analysis trail markers or note analysis landmarks so that when we inevitably need to backtrack, we can easily find a new path to take?</p></li>
<li><p>What does it mean to take a break, relaxing from the work, pausing, contemplating, and refining our plans? Can we build analysis ‘viewpoints’ that encourage us to pause and consider where we’ve been and where we’re going?</p></li>
<li><p>How much time do we take? Do we walk for a fixed distance, a fixed amount of time, or until we see a specific landmark? What tools might hint at when to stop our excursions?</p></li>
<li><p>When we hit a river or crevice do we backtrack to find an alternative path and when do we build a bridge? We may build bridges and create solid roads, tread new paths as we go.</p></li>
</ul>
</section>
<section id="documenting-the-journey" class="level3">
<h3 class="anchored" data-anchor-id="documenting-the-journey">Documenting the journey</h3>
<p>As we navigate the garden, we don’t want to forget why we’re journeying and what our end goal is (and sometimes, for the <em>flaneur</em>, the journey itself is the goal). We need to see, collect, observe, discover, understand, and describe, documenting our journey for ourselves and others.</p>
<ul>
<li><p>When should we take photos, highly realistic but low-dimensional snapshots of our observations? When should we make a sketch, a rough, quick, drawing emphasising features curated by the hand and mind of the observer. When should we a capture a specimen, a tiny snippet of reality, that we can later examine in the lab?</p></li>
<li><p>If our analysis takes multiple days, how do we remember what we did each day? Should we be journalling our journey each night, reflecting on the paths we have taken, missed, ignored, explored, and flagged for future exploration?</p></li>
<li><p>The analysis garden is large and complex, and it’s easy to lose our way and forget where we have been. As we proceed, should we be dropping breadcrumbs to remind us where we’ve been before? Breadcrumbs also allow us to backtrack, reversing from dead ends and examine the fork we have missed.</p></li>
</ul>
</section>
</section>
<section id="designing-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="designing-with-the-metaphor">Designing with the metaphor</h2>
<p>How does a metaphor help us inform workflows, attitudes, and tools for data analysis? We created the sketch below while exploring how metaphors can help (and hinder!) tool-building.</p>
<p><img src="https://jwolondon.github.io/forkingPaths/images/sketch.png" width="700" alt="A simple tree"></p>
<p>In this diagram, the metaphor (M1) surfaces items, actions, relationships, challenges and questions (circles inside M1). By extending the metaphor to its widest extent, by playing with its elements and implications, we create a rich description of the actual problem domain (data analysis, in our case). Those are the lines to the circles inside the dashed circle, the problem space. By projecting concepts from the metaphor space to concepts in the problem space, we discover those concepts in the problem space and create explicit analogies to these concepts in the problem space (links between circles in M1 and the dashed circle.)</p>
<p>We may not uncover all concepts in the problem space but instead of trying to understand the problem domain “out of itself” and “through itself”, our chosen metaphor helps do the work for us. We use to the metaphor to create <em>a</em> specific understanding of the problem space. Each element in the metaphor, fog, bridges, maps, paths etc, maps to one or more elements in the problem domain: uncertainty, data, methodology, findings, actions, etc.</p>
<p>A tool designer wishing to support analysts in their work can now evaluate each of those mappings:</p>
<ul>
<li><p>Some mappings might be a <strong>high fidelity representation</strong> of the problem. For example, the sensation of deciding between multiple forking paths is similar to picking from a small set of possible analysis decisions.</p></li>
<li><p>Some mappings might be <strong>pluralistic</strong>. There might be many possible mappings and explanations for a given element of the metaphor. For example, we identified three possible interpretations of the height of the landscape above: the “optimality” of the analysis, the difficulty of analysis, or the ability to see above the fog.</p></li>
<li><p>Other mappings might <strong>not be useful</strong>: for example, it may be uninstructive to imagine a meaningful mapping of ‘weather’ to any concept in data analysis. Or mappings might be confusing and might lead to misunderstandings or false implications. Likewise, the notion of ‘relaxing’ or ‘recreation’ might not help at all with understanding data analysis. A designer can choose to discard those mappings.</p></li>
</ul>
<p>There might be concepts in the problem space that are not explained or “discovered” by the metaphor. We accept that but it shows that the choice of metaphor is important. We based our investigation on one possible metaphor, but there are many others.</p>
<p>Our sketch shows another metaphor labelled M2. It prompts us to wonder how it might influence our understanding of the problem (the links and concepts discovered in the problem space) and the potential tools we could develop to assist our analysis. Does it widen or deepen our understanding, or simply reiterate what we already know?</p>
<p>For example, we could look at other metaphors for data analysis.</p>
<ul>
<li>The multiverse.</li>
<li>A detective mystery.</li>
<li>The needle in a haystack.</li>
<li>Water and fishing (data dredging, fishing expeditions, data lakes).</li>
<li>Prospecting for oil (data is the new oil) and mining.</li>
</ul>
<p>Once a metaphor has been chosen and has been mapped to a problem space, a designer can start creating metaphor-informed solutions. In our sketch, those solutions are the rectangles S1-S3 on the very left. A design or tool does not have to cover all the concepts in the metaphor. Rather, it can focus on some specific elements and relations of that metaphor.</p>
<p>While metaphorical thinking can provide <em>affordances</em>, there may be is no benefit in making the metaphor explicit in the tools, workflows, activities or interfaces we create. Our tools don’t need to be skeuomorphic, and literally incorporate elements of the garden, fog, or forking paths into our user interfaces. We want to recreate the function of the metaphor only as far as it provides utility.</p>
</section>
<section id="speculating-with-the-metaphor" class="level2">
<h2 class="anchored" data-anchor-id="speculating-with-the-metaphor">Speculating with the metaphor</h2>
<p>We have discussed a metaphor—“navigating the foggy garden of forking paths”—that we think provides a useful framing for the process of data analysis.</p>
<p>What do you think? Where does this metaphor resonate with your experience and where does it feel like a poor fit? Do you ever feel lost during the process of analysis and wish you had a map, compass, binoculars, trail markers, or a GPS? Does it prompt you to consider what new tools a data analyst might benefit from?</p>
<p>We also believe there is general value in framing workflows, actions, and interface design around the construction of metaphors. Metaphors provide us with a means to reduce the complexity of a problem but also surface the elements we might value. We are excited by the possibility metaphor can help to understand pain points, to ease critical thinking and to shape better tools for data analysis.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{bach2024,
  author = {Bach, Benjamin and Wickham, Hadley and Wood, Jo and Xu, Kai},
  title = {Navigating the {Foggy} {Garden} of {Forking} {Paths}},
  date = {2024-06-25},
  url = {https://theory4ida.github.io/tmcf/posts/tools/},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-bach2024" class="csl-entry quarto-appendix-citeas">
Bach, Benjamin, Hadley Wickham, Jo Wood, and Kai Xu. 2024.
<span>“Navigating the Foggy Garden of Forking Paths.”</span> June 25,
2024. <a href="https://theory4ida.github.io/tmcf/posts/tools/">https://theory4ida.github.io/tmcf/posts/tools/</a>.
</div></div></section></div> ]]></description>
  <guid>https://theory4ida.github.io/tmcf/posts/tools/</guid>
  <pubDate>Tue, 25 Jun 2024 00:00:00 GMT</pubDate>
  <media:content url="https://jwolondon.github.io/forkingPaths/images/gardenPhoto.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
