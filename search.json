[
  {
    "objectID": "files/notes.html",
    "href": "files/notes.html",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Commentary paper (e.g. multi-author in RSS Stats and Society)\nAn edited special issue, possibly also RSS Stats and Society)\nSome ambitions for funded activity\nOthers\n\n\n\n\n\nStructured activities rather than free-flow discussion / “talks”.\nTMCF for writing outputs rather than open discussion / networking.\nPre-meeting activities that we ask participants to perform, and communications, should establish above, and enable participants to come with ideas for writing.\n\n\n\n\nNote this is an initial plan to be iterated over.\n\n\n\nGive each participant an extreme position that they have to argue / test the limits of.\nThey elaborate on position, identify what it brings, its consequences and its future. Get people to write provocations relating to position. An example, the famous Anderson quote: “Big Data makes theory obsolete [sic]”\nThey will present these positions on day 1, via 20 minute presentations.\nThey should argue for that position with reference to their own work (this is a way of doing introductions).\nCT and RB to formulate these positions, and slightly informed by the work that individuals do.\n\n\n\n\n\nFrom these positions participants will be allocated into groups. These might fall into disciplinary divides (whether we want to encourage / discourage this is up for discussion).\nAs a group, they write up position with evidence and examples. These documents should be ?? word blog posts.\nWe could impose a common structure on these blog posts (to be discussed)\nBlog posts are shared with participants\n\n\n\n\n\nHaving read/understood positions, each participant identifies consequences for their own research. E.g. what position x,y,z would mean for their work/community.\nThis would be via quick presentations (little prep).\nRest of day, entire group works up a structure for the position paper.\nGroups of individuals attached to sections.\n\n\n\n\n\nGroups of individuals write sections to which they are attached.\n\n\n\n\n\nOpen discussion of Research Agenda, given work done\nPlans for second week of TMCF for local people\n\n\n\n\n\n\nHeuristics trumps theory in data-driven research. Model-building in data-driven research is best approached by consulting data and domain praxis and commonsense rather than isolated theory/ theoretical mechanisms.\nKnowledge claims can only be made through out-of-sample significance tests (extreme pre-registration).\nExploratory analysis induces p-hacking\nData graphics + visual analysis methods induce false discovery via unchecked multiple comparisons\nData graphics + visual analysis methods are free from pitfalls of statistical analysis since they encourage informal calibration and expectation on inferences.\nAutomated analysis is the best hope for doing bias-free research (e.g. removing human from scientific method)\nVisualization as data artefact (e.g. Simulation from null hypothesis – Hops and lineups – for exploring possible outcomes that might have been produced by data, not for marking out confidence intervals)\nFor research to have value, it must be underpinned by crisply stated causal claims.\nThe only interesting/useful hypothesis tests are those where the “null hypothesis” is not rejected (where p&gt;.05).\n\nTensions that above try to expose:\n\nautomation / human-in-loop / human-mediated\npre-registration versus free-form exploration versus heuristics (recipes for doing data-driven research)\nlimits to modelling / diagnostics\npractice of data-driven science / tools\n\n\n\n\n\nPre-registration for Predictive Modeling https://arxiv.org/abs/2311.18807\nWhy We (Usually) Don’t Have to Worry About Multiple Comparisons https://doi.org/10.1080/19345747.2011.618213"
  },
  {
    "objectID": "files/notes.html#challenge-outcomes",
    "href": "files/notes.html#challenge-outcomes",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Commentary paper (e.g. multi-author in RSS Stats and Society)\nAn edited special issue, possibly also RSS Stats and Society)\nSome ambitions for funded activity\nOthers"
  },
  {
    "objectID": "files/notes.html#constraints-checks-for-planning",
    "href": "files/notes.html#constraints-checks-for-planning",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Structured activities rather than free-flow discussion / “talks”.\nTMCF for writing outputs rather than open discussion / networking.\nPre-meeting activities that we ask participants to perform, and communications, should establish above, and enable participants to come with ideas for writing."
  },
  {
    "objectID": "files/notes.html#structure-for-5-days",
    "href": "files/notes.html#structure-for-5-days",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Note this is an initial plan to be iterated over.\n\n\n\nGive each participant an extreme position that they have to argue / test the limits of.\nThey elaborate on position, identify what it brings, its consequences and its future. Get people to write provocations relating to position. An example, the famous Anderson quote: “Big Data makes theory obsolete [sic]”\nThey will present these positions on day 1, via 20 minute presentations.\nThey should argue for that position with reference to their own work (this is a way of doing introductions).\nCT and RB to formulate these positions, and slightly informed by the work that individuals do.\n\n\n\n\n\nFrom these positions participants will be allocated into groups. These might fall into disciplinary divides (whether we want to encourage / discourage this is up for discussion).\nAs a group, they write up position with evidence and examples. These documents should be ?? word blog posts.\nWe could impose a common structure on these blog posts (to be discussed)\nBlog posts are shared with participants\n\n\n\n\n\nHaving read/understood positions, each participant identifies consequences for their own research. E.g. what position x,y,z would mean for their work/community.\nThis would be via quick presentations (little prep).\nRest of day, entire group works up a structure for the position paper.\nGroups of individuals attached to sections.\n\n\n\n\n\nGroups of individuals write sections to which they are attached.\n\n\n\n\n\nOpen discussion of Research Agenda, given work done\nPlans for second week of TMCF for local people"
  },
  {
    "objectID": "files/notes.html#some-themes-that-might-form-positions",
    "href": "files/notes.html#some-themes-that-might-form-positions",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Heuristics trumps theory in data-driven research. Model-building in data-driven research is best approached by consulting data and domain praxis and commonsense rather than isolated theory/ theoretical mechanisms.\nKnowledge claims can only be made through out-of-sample significance tests (extreme pre-registration).\nExploratory analysis induces p-hacking\nData graphics + visual analysis methods induce false discovery via unchecked multiple comparisons\nData graphics + visual analysis methods are free from pitfalls of statistical analysis since they encourage informal calibration and expectation on inferences.\nAutomated analysis is the best hope for doing bias-free research (e.g. removing human from scientific method)\nVisualization as data artefact (e.g. Simulation from null hypothesis – Hops and lineups – for exploring possible outcomes that might have been produced by data, not for marking out confidence intervals)\nFor research to have value, it must be underpinned by crisply stated causal claims.\nThe only interesting/useful hypothesis tests are those where the “null hypothesis” is not rejected (where p&gt;.05).\n\nTensions that above try to expose:\n\nautomation / human-in-loop / human-mediated\npre-registration versus free-form exploration versus heuristics (recipes for doing data-driven research)\nlimits to modelling / diagnostics\npractice of data-driven science / tools"
  },
  {
    "objectID": "files/notes.html#some-additional-reading",
    "href": "files/notes.html#some-additional-reading",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Pre-registration for Predictive Modeling https://arxiv.org/abs/2311.18807\nWhy We (Usually) Don’t Have to Worry About Multiple Comparisons https://doi.org/10.1080/19345747.2011.618213"
  },
  {
    "objectID": "programme/index.html",
    "href": "programme/index.html",
    "title": "Conference Programme",
    "section": "",
    "text": "Monday 17th June\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n1100-1115\n\n Arrival and networking\n\n\n\n1115-1130\n\n TMCF and Turing\n– Ioannis Kosmidis, TMCF Lead\n\n\n\n1130-1145\n\n Turing Overview\n– Daniel Lovelock, Head of Academic Engagement\n\n\n\n1145--1200\n\n This TMCF\n– Cagatay Turkay, Roger Beecham\n\n\n\nProvocations and positions\n\n\n1200--1300\n\n Participants present their provocations, w/ group review\n– All\n\n\n\n1300-1400\n\n Lunch\n\n\n\n1400--1530\n\n Participants present their provocations, w/ group review\n– All\n\n\n\n1530-1700\n\n Affinity mapping exercise\n– All\n\n\n\n\n\n\n\n\n\n\nTuesday 18th June\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n\n Arrival and networking\n\n\n\n0930-0945\n\n Update and plans for day\n– Cagatay Turkay, Roger Beecham\n\n\n\nDocumenting positions\n\n\n0945-1230\n\n Groups co-write 1500-word blog w/ example and evidence\n– All\n\n\n\n1230-1330\n\n Lunch\n\n\n\n1330-1600\n\n Groupwork on commentaries\n– All\n\n\n\n1600-1630\n\n De-brief and planning\n– Cagatay Turkay, Roger Beecham\n\n\n\n\n\n\n\n\n\n\nWednesday 19th Jun\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n\n Arrival and networking\n\n\n\n0930-0945\n\n Update and plans for day\n– Cagatay Turkay, Roger Beecham\n\n\n\n0945-1100\n\n Lightning talks from Turing PIs on probabilistic programming\n– TBC\n\n\n\nGrounding positions\n\n\n1100-1230\n\n Groupwork on blogs\n– All\n\n\n\n1230-1330\n\n Lunch\n\n\n\n1330-1600\n\n Groupwork on blogs\n– All\n\n\n\n\n\n\n\n\n\n\nThursday 20th June\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n\n Arrival and networking\n\n\n\n0930-0945\n\n Update and plans for day\n– Cagatay Turkay, Roger Beecham\n\n\n\nConsolidate\n\n\n0945-1230\n\n Prioritising and grounding activity\n– All\n\n\n\n1130-1230\n\n Paper planning\n– All\n\n\n\n1230-1330\n\n Lunch\n\n\n\n1330-1530\n\n Paper planning + material gathering\n– All\n\n\n\n1545-1600\n\n Progress and next steps\n– All\n\n\n\nPublic lecture\n\n\n1730-2100\n\n Public lectures + drinks reception\n– Andrew Gelman, Jess Hullman, Hadley Wickham\n\n\n\n\n\n\n\n\n\n\nFriday 21st June\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n\n Arrival and networking\n\n\n\n0930-0945\n\n Briefing from Turing Director for Fundamental DS&AI\n– Andrew Duncan \n\n\n\nOutput planning\n\n\n0945-1230\n\n Commentary + SI paper mapping, software, toolkits, etc.\n– All\n\n\n\n1230-1400\n\n Lunch and depart"
  },
  {
    "objectID": "posts/02-tools/index.html",
    "href": "posts/02-tools/index.html",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "",
    "text": "— The Garden Of Forking Paths, Jorge Luis Borges, 1941\nWhat is a good metaphor for data analysis? In the spirit of Borges, Pólya, Gelman and many others since, we’re going to run with this one: navigating the forking paths of a foggy garden. It captures our belief that analysis involves deciding between (navigating) a set of discrete decisions (the forks) across different analysis options (the paths) in a continuous landscape of possibility (the garden) that becomes incrementally revealed as the analysis progresses (the fog).\nAnalysis is not a murder mystery or a detective challenge. Typically there is not one objective Truth to be discovered, but many potential possible discoveries. When presenting a final analysis the paths taken can be just as important as the final destination.\nAt best, a metaphor is a tool for thinking. We don’t believe that this metaphor captures every aspect of the problem at hand, but perhaps it illuminates components to guide the analyst through their workflow, encouraging critical thinking, suggesting where future tools may assist. We are also cautious: a danger with any metaphor is that it replaces something we don’t fully understand with something we think we do. It may lead to hubris. Nevertheless, stick with us as we dig a little deeper and consider what it might bring to our understanding of the data science process."
  },
  {
    "objectID": "posts/02-tools/index.html#interrogating-the-metaphor",
    "href": "posts/02-tools/index.html#interrogating-the-metaphor",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "Interrogating the metaphor",
    "text": "Interrogating the metaphor\nWe’ll begin by exploring five components of the metaphor (navigation, the fog, the garden, the forks, and the paths), how it ties to the practice of data analysis, where it suggests interesting angles to consider, and where it might constrain our thinking.\n\nNavigating\nTo perform an analysis we navigate the pathways, making decisions as we explore the problem. We navigate in different ways at different times. Often we will be walking an analysis, plodding from one foot to the other as each step reveals a little more of the path. But at times we may be running in as we enter a flow state or repeat a well-used protocol. Or we may take intuitive leaps that transport us to quite different parts of the garden. Or perhaps we are flaneurs, wandering for its own sake, with no purpose other than to subject ourselves to the whims of serendipity.\nRegardless of how we travel, it is a rare analysis that proceeds in a straight line along a pre-determined route. Much real analysis involves exploration – winding paths with many wrong turns and dead ends. Even decisions late in the analysis can provide surprising insights that lead us to reconsider our initial steps.\nOnce we have done our analysis, getting to a point of interest after taking many wrong turns, we may choose to describe our journey to others. Perhaps we focus only on the start and destination. Or perhaps we write a guided tour (for example as an academic publication), quickly leading the reader to the points of interest while gliding past the less interesting scenery. Scientific honesty may compel us to provide more of a travelogue: did we know what we were looking for before we set out, and how long did we spend hunting for something of interest? Or we might reflect on our own journey as a reflexive exercise; would we take the same paths if we were to start again?\n\n\nFog\nA fog implies we can’t see much of the garden from our current location: we have to make decisions and choose paths in order to reveal future choices. Our decisions are guided by incomplete information leading us frequently to backtrack; often a path that looked promising at first glance is later revealed to be a dead end. We might need to map our passage as we proceed to avoid getting lost. Or perhaps we rely on maps to unseen destinations left by others.\nThe fog is not constant; in some places it is thicker, giving us little clue about where to travel next. Experience may give our eyes more power to penetrate the fog, as past analyses help us better predict which paths are likely to lead to bountiful places.\nSome destinations will be obvious from a distance; tall trees reaching above the fog. Others will be serendipitous, stumbled upon, fallen from the sky, and discovered among the bushes.\n\n\nGarden\nThe garden implies a space that’s a mix of the human and natural, tempting us with bounty to be discovered. Depending on what you are picturing, that bounty might be a succulent tomato in a vegetable garden; an attractive fountain in a formal garden; the symmetrical layout of Italianate beds; or a prize statue at the centre of a hedge maze. Do these metaphorical objects help frame and evaluate our objectives and discoveries?\nWhat role does the topography of the garden play? Perhaps ‘height’ is the optimality of the analysis, where the shape of the garden landscape captures the effectiveness of the underlying analysis. Is the garden flat with a multitude of minor undulations? Is it tended on a mountain, where no matter where you start, there’s an obvious destination at the summit? Or does this garden span a mountain range with multiple peaks connected by ridges and passes, separated by deep ravines.\nBut in a foggy landscape we may not have a clear notion of ‘optimal’. Perhaps height could mean something else. Leaning on the metaphor, gradient could represent the ease with which we can progress. Moving downhill follows established good practice, gathering momentum from the familiar. Perhaps the tools we use lead us downhill, affordances that encourage some approaches at the expense of others. Or perhaps the fog is less dense in the heights so that travelling uphill allows us to see further.\nThe notion of landscape emphasises continuity. Perhaps it is unhelpful to think of discrete decision-making; instead we have at our disposal choices from an infinite range of possibilities. We might continuously vary some model parameters and describe the effect on our journeys. Sensitivity analysis might describe how critically dependent our location in the garden is on how we move.\n\n\nForking\nA forking path in garden of possibility implies there are choices to be made, and that those choices are discrete and finite. Let’s put aside for the moment that we could freely wander about the garden and instead we will keep off the grass.\nThe existence of a network of paths implies they have some topology. We suggest that understanding that topology is itself instructive. For example, those paths might form a tree: A single start point leading to multiple possible outcomes. At each stage in the decision-making process, more choices are opened up. Alternatively, that tree might be rooted in a single outcome where multiple start points lead to the same destination. Such a topology might give us confidence that our conclusions are robust; that even with changes in methodological decision-making, all paths lead to the same result. But perhaps such an arrangement should signal caution; that our methods are too restrictive, leading only to a limited set of possible outcomes regardless of the data that informs them.\nThe network of forking paths may form non tree-like patterns. For example, cycles that take us back to previous decision points as part of an iterative process. Or perhaps we have unconnected paths, ones that exist in the same garden of possibility but where we fail to make connections between them.\nAn understanding of the network of forking paths may lead to insight not just about the outcomes, but the process followed and the way it shapes the validity of findings. Perhaps these are oversimplistic or poorly fitting characterisations of different forking paths. We are intrigued by the possibility that the structure of these networks of choices are instructive in reflecting on the data science workflow by providing a rich and shareable account of the analytic process.\n\n\nPaths\nIf a decision-point is a fork in a network of data science activity, what is the path? Does the metaphor suggest different types of path whose characterisations result in insight or meaningful consequence?\nWe take a path to represent some chosen activity, perhaps the cleaning of a dataset, the specification of a visualization design, or the populating of a simulation model. It is what we might traditionally document when explaining our work.\nWhat might the ‘width’ of a path mean? Consider the popularity of a well-trodden path. Perhaps we are following a process adopted by many others, following established good (or even poor) practice. A narrow path might represent a break from convention - activity without precedence, novelty taking us somewhere few have visited before. Or perhaps we are pioneering a new shortcut to a place many others have visited. Characterising path width might help signal to us where we exercise methodological scrutiny. Examining wide paths might prompt us to question convention, or at least its fit to the problem at hand.\nOur paths move us through the garden of possibility. A perilous path might be one that traverses a precipitous edge in that garden - the slightest misstep rapidly takes us somewhere quite different to our intended destination. Should we avoid peril? Or can it take us to places otherwise inaccessible?\nIn contrast, a wide road at the bottom of a ravine might be difficult to leave. Any deviations from convention are quickly corrected. When should we stick to these safe routes, embracing the confidence given to us by others? When should we worry that are we being unnecessarily constrained by convention and denying ourselves the opportunities of novelty and new perspectives."
  },
  {
    "objectID": "posts/02-tools/index.html#working-with-the-metaphor",
    "href": "posts/02-tools/index.html#working-with-the-metaphor",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "Working with the metaphor",
    "text": "Working with the metaphor\nWhat might the metaphor mean in the data science workflow carried out by Jan – an imaginary data scientist looking to predict the outcome of a forthcoming national election?\nJan has at her disposal the results of a several national opinion polls that ask a sample of voters how they would vote in the election. She has a choice: Which of the polls does she use for her analysis? Acknowledging the existence of that choice helps to signal the possible impact of data sources on eventual conclusions.\nShe chooses one of the polls and signposts this was just one of several choices at this point. She wishes to visualize the distributions of the polling data. Again she has a choice - should she produce a map of the results emphasising geographical patterns, or a sequence of histograms emphasising the shapes of the distributions. Or she might choose both and compare what they tell her.\nBut what if she’d chosen a different opinion poll? Would that have led ultimately to different conclusions? Or what if she had chosen to represent the results in tabular format and not a map? Or what is the effect of choosing a choropleth over a cartogram?\nHer path through the garden of possibilities are captured as a tree – from a single starting point (a) through the choice of polling data to use (b, c or d) and choices of output maps, charts and tables (e, f, g or h).\n\nThe characterisation of the topology of these choices tells us (and her) something about the process she is following. Depending on how different her interpretations e, f, g and h are, this might indicate the robustness of her conclusions.\nJan isn’t the only person working with election data. Perhaps her team started with different objectives, one emphasising the polling analysis capabilities of the team to market their services. Another focussed on assessing the accuracy of their prediction results. Another in providing commentary for a national news outlet. Perhaps their decision paths looked like this:\n\nIt suggests a degree of equifinality – different paths lead to the same outcome (g) even when starting from different places. Different polling data or analytical processes lead to the same predicted result. It might suggest greater confidence in the prediction. But it also signals the possiblity that results are an artefact of misapplied analytical approaches that led to the same arbitrary result.\nJan decides to build a more sophisticated MRP model to predict election results. She stratifies opinion poll results by the demographics of the respondents, runs the model to generate some output distributions and visualises the output of the model emphasising the uncertainty captured by the model. In parallel her team-mate, Arun, runs his own MRP based on a different set of data and with different parameters weighting demographics. Even though this too generates distributions of possible predictive outcomes, they are quite distinct from Jan’s MRP results.\n\nIt allows the team to question why their results are so distinct. Even Arun’s exploration of model parameterisation (h and i) that had initially given him a sense of the variance in the model outputs is challenged by the contrast with Jan’s MRP outcome (d).\nFor a later project, under less time constraint, Jan uses intermediate results from her analysis to revisit earlier assumptions and choices in a more iterative and reflexive process. It takes place in a foggier garden where she can’t see her destination until it emerges after several iterations.\n\nBy examining this topology Jan is able to see the effect of iterating over her process, to examine how her workflow is shaping the conclusions she draws, where iteration is more or less effective."
  },
  {
    "objectID": "posts/02-tools/index.html#thinking-with-the-metaphor",
    "href": "posts/02-tools/index.html#thinking-with-the-metaphor",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "Thinking with the metaphor",
    "text": "Thinking with the metaphor\nOur primary goal here is to present a metaphor that we find useful for framing data analysis. But the metaphor is only as useful as the tools and thinking it provokes, and while we don’t (yet) offer any concrete tools inspired by this metaphor, our discussion included many ideas that offer promise.\nWe’ve grouped our ideas into two broad categories which we explore below: navigating and documenting the journey.\n\nNavigating\nOur metaphor is physical and geographical: we are navigating forking paths in a garden. That physicality leads us to contemplate a set of questions:\n\nWhat areas have we visited and what areas have we missed? Where should we go and what areas should we avoid? We probably don’t start our journey with a map, but we do want to build one as we go. Such a map might be detailed, surveying the terrain and recording landmarks and findings. Or perhaps the map is schematic, carefully documenting the topology of paths taken, ignoring their length and distance between them. Or perhaps it is more experiential: “here be dragons”.\nAm I walking alone or as a team? Do we walk the same path together or do different members take a turn at leading the navigation? Do we divide and conquer, starting from different locations and walking different paths? How do we share information on where we have been and where we’re contemplating next?\nAs we travel, we might notice interesting pathways that we don’t have the time to take right now. How can we leave analysis trail markers or note analysis landmarks so that when we inevitably need to backtrack, we can easily find a new path to take?\nWhat does it mean to take a break, relaxing from the work, pausing, contemplating, and refining our plans? Can we build analysis ‘viewpoints’ that encourage us to pause and consider where we’ve been and where we’re going?\nHow much time do we take? Do we walk for a fixed distance, a fixed amount of time, or until we see a specific landmark? What tools might hint at when to stop our excursions?\nWhen we hit a river or crevice do we backtrack to find an alternative path and when do we build a bridge? We may build bridges and create solid roads, tread new paths as we go.\n\n\n\nDocumenting the journey\nAs we navigate the garden, we don’t want to forget why we’re journeying and what our end goal is (and sometimes, for the flaneur, the journey itself is the goal). We need to see, collect, observe, discover, understand, and describe, documenting our journey for ourselves and others.\n\nWhen should we take photos, highly realistic but low-dimensional snapshots of our observations? When should we make a sketch, a rough, quick, drawing emphasising features curated by the hand and mind of the observer. When should we a capture a specimen, a tiny snippet of reality, that we can later examine in the lab?\nIf our analysis takes multiple days, how do we remember what we did each day? Should we be journalling our journey each night, reflecting on the paths we have taken, missed, ignored, explored, and flagged for future exploration?\nThe analysis garden is large and complex, and it’s easy to lose our way and forget where we have been. As we proceed, should we be dropping breadcrumbs to remind us where we’ve been before? Breadcrumbs also allow us to backtrack, reversing from dead ends and examine the fork we have missed."
  },
  {
    "objectID": "posts/02-tools/index.html#designing-with-the-metaphor",
    "href": "posts/02-tools/index.html#designing-with-the-metaphor",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "Designing with the metaphor",
    "text": "Designing with the metaphor\nHow does a metaphor help us inform workflows, attitudes, and tools for data analysis? We created the sketch below while exploring how metaphors can help (and hinder!) tool-building.\n\nIn this diagram, the metaphor (M1) surfaces items, actions, relationships, challenges and questions (circles inside M1). By extending the metaphor to its widest extent, by playing with its elements and implications, we create a rich description of the actual problem domain (data analysis, in our case). Those are the lines to the circles inside the dashed circle, the problem space. By projecting concepts from the metaphor space to concepts in the problem space, we discover those concepts in the problem space and create explicit analogies to these concepts in the problem space (links between circles in M1 and the dashed circle.)\nWe may not uncover all concepts in the problem space but instead of trying to understand the problem domain “out of itself” and “through itself”, our chosen metaphor helps do the work for us. We use to the metaphor to create a specific understanding of the problem space. Each element in the metaphor, fog, bridges, maps, paths etc, maps to one or more elements in the problem domain: uncertainty, data, methodology, findings, actions, etc.\nA tool designer wishing to support analysts in their work can now evaluate each of those mappings:\n\nSome mappings might be a high fidelity representation of the problem. For example, the sensation of deciding between multiple forking paths is similar to picking from a small set of possible analysis decisions.\nSome mappings might be pluralistic. There might be many possible mappings and explanations for a given element of the metaphor. For example, we identified three possible interpretations of the height of the landscape above: the “optimality” of the analysis, the difficulty of analysis, or the ability to see above the fog.\nOther mappings might not be useful: for example, it may be uninstructive to imagine a meaningful mapping of ‘weather’ to any concept in data analysis. Or mappings might be confusing and might lead to misunderstandings or false implications. Likewise, the notion of ‘relaxing’ or ‘recreation’ might not help at all with understanding data analysis. A designer can choose to discard those mappings.\n\nThere might be concepts in the problem space that are not explained or “discovered” by the metaphor. We accept that but it shows that the choice of metaphor is important. We based our investigation on one possible metaphor, but there are many others.\nOur sketch shows another metaphor labelled M2. It prompts us to wonder how it might influence our understanding of the problem (the links and concepts discovered in the problem space) and the potential tools we could develop to assist our analysis. Does it widen or deepen our understanding, or simply reiterate what we already know?\nFor example, we could look at other metaphors for data analysis.\n\nThe multiverse.\nA detective mystery.\nThe needle in a haystack.\nWater and fishing (data dredging, fishing expeditions, data lakes).\nProspecting for oil (data is the new oil) and mining.\n\nOnce a metaphor has been chosen and has been mapped to a problem space, a designer can start creating metaphor-informed solutions. In our sketch, those solutions are the rectangles S1-S3 on the very left. A design or tool does not have to cover all the concepts in the metaphor. Rather, it can focus on some specific elements and relations of that metaphor.\nWhile metaphorical thinking can provide affordances, there may be is no benefit in making the metaphor explicit in the tools, workflows, activities or interfaces we create. Our tools don’t need to be skeuomorphic, and literally incorporate elements of the garden, fog, or forking paths into our user interfaces. We want to recreate the function of the metaphor only as far as it provides utility."
  },
  {
    "objectID": "posts/02-tools/index.html#speculating-with-the-metaphor",
    "href": "posts/02-tools/index.html#speculating-with-the-metaphor",
    "title": "Navigating the Foggy Garden of Forking Paths",
    "section": "Speculating with the metaphor",
    "text": "Speculating with the metaphor\nWe have discussed a metaphor—“navigating the foggy garden of forking paths”—that we think provides a useful framing for the process of data analysis.\nWhat do you think? Where does this metaphor resonate with your experience and where does it feel like a poor fit? Do you ever feel lost during the process of analysis and wish you had a map, compass, binoculars, trail markers, or a GPS? Does it prompt you to consider what new tools a data analyst might benefit from?\nWe also believe there is general value in framing workflows, actions, and interface design around the construction of metaphors. Metaphors provide us with a means to reduce the complexity of a problem but also surface the elements we might value. We are excited by the possibility metaphor can help to understand pain points, to ease critical thinking and to shape better tools for data analysis."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Some risks and opportunities of automated data analysis\n\n\n\n\n\nSome thoughts arising from TMCF 2024 workshop at the Turing Institute.\n\n\n\n\n\nJun 25, 2024\n\n\nDaniel Archambault, Roger Beecham, Andrew Gelman, Jessica Hullman, Edwin Pos\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Foggy Garden of Forking Paths\n\n\n\n\n\nSome thoughts arising from TMCF 2024 workshop at the Turing Institute.\n\n\n\n\n\nJun 25, 2024\n\n\nBenjamin Bach, Hadley Wickham, Jo Wood, Kai Xu\n\n\n\n\n\n\n\n\n\n\n\n\nHumans all the way down: statistical reflection and self-critique for interactive data analysis\n\n\n\n\n\nSome thoughts arising from TMCF 2024 workshop at the Turing Institute.\n\n\n\n\n\nJun 25, 2024\n\n\nDi Cook, Rachel Franklin, Cagatay Turkay, Mari-Cruz Villa-Uriol, Levi Wolf\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "",
    "text": "Theory & Methods Challenge Fortnights are intensive, two-week events where teams of 12 experts from across the Turing university partner network and external institutions worldwide collaborate to initiate work on tackling a foundational challenge in data science and artificial intelligence. Each event focuses on a specific theoretical or methodological challenge that has the potential to impact data science and artificial intelligence at a fundamental level.\nvia turing.ac.uk.\n\nWelcome to our upcoming Turing Theory & Methods Challenge Fortnight (TMCF). Our challenge will take place in the British Library in summer 2024.\nThe ambition behind TMCFs is nicely laid out on the Alan Turing Institute’s website (above). Ours seeks to a address a problem familiar to data-driven research: the garden of forking paths fallacy (Gelman and Loken 2013). We have assembled a stellar team to work on the challenge – world-leaders in data science engineering and practice. Alongside the challenge we intend to host public lectures and seminars – more news on this to follow.\nWe will update these pages through 2024 as planning and other preparatory work develops.\n\n\nCagatay Turkay and Roger Beecham, Challenge Leads"
  },
  {
    "objectID": "index.html#our-tmcf",
    "href": "index.html#our-tmcf",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "",
    "text": "Theory & Methods Challenge Fortnights are intensive, two-week events where teams of 12 experts from across the Turing university partner network and external institutions worldwide collaborate to initiate work on tackling a foundational challenge in data science and artificial intelligence. Each event focuses on a specific theoretical or methodological challenge that has the potential to impact data science and artificial intelligence at a fundamental level.\nvia turing.ac.uk.\n\nWelcome to our upcoming Turing Theory & Methods Challenge Fortnight (TMCF). Our challenge will take place in the British Library in summer 2024.\nThe ambition behind TMCFs is nicely laid out on the Alan Turing Institute’s website (above). Ours seeks to a address a problem familiar to data-driven research: the garden of forking paths fallacy (Gelman and Loken 2013). We have assembled a stellar team to work on the challenge – world-leaders in data science engineering and practice. Alongside the challenge we intend to host public lectures and seminars – more news on this to follow.\nWe will update these pages through 2024 as planning and other preparatory work develops.\n\n\nCagatay Turkay and Roger Beecham, Challenge Leads"
  },
  {
    "objectID": "index.html#key-dates",
    "href": "index.html#key-dates",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "Key Dates",
    "text": "Key Dates\n\nWeek 1 (full team) : Monday 17th June – Friday 21st June 2024\nWeek 2 (UK-based team) : Monday 8th July – Friday 12th July 2024"
  },
  {
    "objectID": "files/slides/open.html#navigating-the-garden-of-forking-paths-theoretical-foundations-for-interactive-data-analysis-in-data-driven-science",
    "href": "files/slides/open.html#navigating-the-garden-of-forking-paths-theoretical-foundations-for-interactive-data-analysis-in-data-driven-science",
    "title": "TMCF 2024",
    "section": "Navigating the garden of forking paths  Theoretical foundations for interactive data analysis in data-driven science",
    "text": "Navigating the garden of forking paths  Theoretical foundations for interactive data analysis in data-driven science"
  },
  {
    "objectID": "files/slides/open.html#themes",
    "href": "files/slides/open.html#themes",
    "title": "TMCF 2024",
    "section": "Themes",
    "text": "Themes\n\n\nModelling paradigms: establish what is distinctive about modelling in data-driven science by mapping out archetypal data-driven projects and the analysis practices they use.\nInference and replicability: develop ways of documenting the context under which analytical findings are made – a grammar for structuring exploratory research findings – so that inferences can be more formally reported.\nTools for progressing from analysis to communication: documenting interactive data analysis processes with integrity – balancing claims to knowledge with informational complexity.\n\n\n\ne.g. model-based exploratory analysis that has rigour – and so that allows us to comment on effects, processes, causes and assess generalisability and reliability of things we are observing.\n\nWhat we would really like to do is think, write about, and start working on examples of how this would ‘look’ in our own domains:\n\nhow it might work/be specialised according to domains\nwhether we can systemise any of this and generate tools to support analysts in reasoning and building models that are loaded with rich context\n\nbut that also invite us to check against spurious discovery, consider the context under which observed patterns become unstable etc."
  },
  {
    "objectID": "challenge/modelling/index.html",
    "href": "challenge/modelling/index.html",
    "title": "Modelling Paradigms",
    "section": "",
    "text": "Model-building praxis in data-driven science is quite distinctive. As well as predicting and explaining outcomes, models are often used as containers for structuring information and learning within the data space. We will consider the ways in which models are incorporated into interactive data analysis by asking:\n\nHow relevant models are identified and selected;\nHow they are developed and refined; and\nThe processes through which they are analysed and evaluated.\n\nFrom here, we will document what is distinctive about model building in data-driven science."
  },
  {
    "objectID": "challenge/modelling/index.html#modelling-paradigms-for-data-driven-science-learning-from-example",
    "href": "challenge/modelling/index.html#modelling-paradigms-for-data-driven-science-learning-from-example",
    "title": "Modelling Paradigms",
    "section": "",
    "text": "Model-building praxis in data-driven science is quite distinctive. As well as predicting and explaining outcomes, models are often used as containers for structuring information and learning within the data space. We will consider the ways in which models are incorporated into interactive data analysis by asking:\n\nHow relevant models are identified and selected;\nHow they are developed and refined; and\nThe processes through which they are analysed and evaluated.\n\nFrom here, we will document what is distinctive about model building in data-driven science."
  },
  {
    "objectID": "challenge/modelling/index.html#related-work-challenge-team",
    "href": "challenge/modelling/index.html#related-work-challenge-team",
    "title": "Modelling Paradigms",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nBeecham, R. and Lovelace, R. (2023) A Framework for Inserting Visually Supported Inferences into Geographical Analysis Workflow: Application to Road Safety Research, Geographical Analysis, 55(3): 345–366.\nCook, D., Reid, N., and Tanaka, E. (2021) The Foundation Is Available for Thinking About Data Visualization Inferentially. Harvard Data Science Review, 3(3).\nFranklin, R. (2023) Quantitative methods II: Big theory. Progress in Human Geography, 47(1): 178-186.\nGelman, A. (2004) “Exploratory Data Analysis for Complex Models.” Journal of Computational and Graphical Statistics, 13: 755–79.\nHullman, J., and Gelman, A. (2021) Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference. Harvard Data Science Review, 3(3).\nWolf, L.J. (2022) “The Right to Rule by Thumb: A Comment on Epistemology in A Route Map for Successful Applications of Geographically-Weighted Regression, Geographical Analysis, 55(1): 184–190.\nWood, J., D. Badawood, J. Dykes, and A. Slingsby. (2011). “BallotMaps: Detecting Name Bias in Alphabetically Ordered Ballot Papers.” IEEE Transactions on Visualization and Computer Graphics, 17: 2384–91."
  },
  {
    "objectID": "challenge/index.html",
    "href": "challenge/index.html",
    "title": "Challenge Statement",
    "section": "",
    "text": "An expanding array of observational data now enables social, economic and environmental behaviours to be researched in a large-scale and empirical way. This is exciting as there are increasing opportunities to effect evidence-based decision making in science, government and industry. In order to generate insights and draw conclusions, however, such data-intensive research relies on reasonably informal, interactive data analysis approaches: those that combine data graphics and statistics via computational notebooks (Observable, Quarto, Jupyter) or interactive data analysis tools such as Tableau and PowerBI. Unlike traditional scientific processes, we don’t know what an optimal and rigorous interactive analysis should look like and their success more often than not relies on the expertise and skills of the data scientist.\nOur proposed Theory & Methods Challenge will develop proposals for conducting and reporting data-intensive research in a more formal way. Our hope is that this activity will lay the foundations for interactive data analysis practices and provide guidelines for computational tools that underpin the next generation of analysis platforms. Through the workshop, we aim to explore three sub-challenges:\n\nModelling paradigms for data-driven science: establish what is distinctive about modelling in data-driven science by mapping out archetypal data-driven projects and the analysis practices they use.\nInference and replicability in data-driven science: develop systematic ways of documenting the context under which analytical findings are made – a grammar for structuring exploratory research findings – so that inferences can be more formally reported.\nTools for progressing from analysis to communication: explore tools and technologies for documenting interactive data analysis processes with integrity – balancing claims to knowledge with informational complexity."
  },
  {
    "objectID": "challenge/index.html#the-challenge",
    "href": "challenge/index.html#the-challenge",
    "title": "Challenge Statement",
    "section": "",
    "text": "An expanding array of observational data now enables social, economic and environmental behaviours to be researched in a large-scale and empirical way. This is exciting as there are increasing opportunities to effect evidence-based decision making in science, government and industry. In order to generate insights and draw conclusions, however, such data-intensive research relies on reasonably informal, interactive data analysis approaches: those that combine data graphics and statistics via computational notebooks (Observable, Quarto, Jupyter) or interactive data analysis tools such as Tableau and PowerBI. Unlike traditional scientific processes, we don’t know what an optimal and rigorous interactive analysis should look like and their success more often than not relies on the expertise and skills of the data scientist.\nOur proposed Theory & Methods Challenge will develop proposals for conducting and reporting data-intensive research in a more formal way. Our hope is that this activity will lay the foundations for interactive data analysis practices and provide guidelines for computational tools that underpin the next generation of analysis platforms. Through the workshop, we aim to explore three sub-challenges:\n\nModelling paradigms for data-driven science: establish what is distinctive about modelling in data-driven science by mapping out archetypal data-driven projects and the analysis practices they use.\nInference and replicability in data-driven science: develop systematic ways of documenting the context under which analytical findings are made – a grammar for structuring exploratory research findings – so that inferences can be more formally reported.\nTools for progressing from analysis to communication: explore tools and technologies for documenting interactive data analysis processes with integrity – balancing claims to knowledge with informational complexity."
  },
  {
    "objectID": "challenge/index.html#outcomes-and-potential-for-impact",
    "href": "challenge/index.html#outcomes-and-potential-for-impact",
    "title": "Challenge Statement",
    "section": "Outcomes and potential for impact",
    "text": "Outcomes and potential for impact\nThe Challenge will aim for the following outcomes:\n\nFoundations for theoretically-guided, transparent and replicable interactive data analysis, leading to more rigorous data-driven science.\nA systematic framework – a grammar – for describing interactive data analysis and the context under which inferences are made.\nGuidelines for designing new generation data analysis tools and technologies that operationalise more formal, interactive data analysis practice.\n\nWe anticipate impact on the following:\n\nAdvancing the science of data science – data science increasingly requires better established, formal forms of working. Our findings will contribute to the theoretical literature on how data-driven research could better respond to advances in data science and AI technologies.\nEcologically-valid, justified and plausible data-driven decisions – our work will support better documented analytical choices, e.g. model/assumptions that are justified and recorded along with contextual, background and researcher tacit knowledge. Such forms of practice will advance how data-driven artefacts can inform decisions and policies.\nTools and technologies for transparent, rigorous and reproducible data analysis practices – we will develop blueprints for new generations of analytical tools for rigorous analysis, from hypothesis generation through to modelling and communication."
  },
  {
    "objectID": "challenge/index.html#open-practices-and-dissemination",
    "href": "challenge/index.html#open-practices-and-dissemination",
    "title": "Challenge Statement",
    "section": "Open practices and dissemination",
    "text": "Open practices and dissemination\nWe will document the activities, discussions and outcomes in openly available and transparent channels. All the discussion and working notes will be collated on a github repository, with blog and other items appearing on these webpages providing an accessible “front-end” to our work."
  },
  {
    "objectID": "challenge/inference/index.html",
    "href": "challenge/inference/index.html",
    "title": "Inference",
    "section": "",
    "text": "A familiar challenge in data-driven science is that of inference. Generalising away from a sample to some target population is challenging when datasets are already large and population-level and observations subject to unknown confounding context. Replication is muddled by the number of analysis choices that are to be made and range of alternatives that could be pursued (the forking paths problem). How, then, can we ensure that the inferences and claims we make from exploratory analyses are properly contextualised?\nDrawing on expertise in Statistics, Human Computer Interaction and Spatial Econometrics, we will explore ways of systematically describing context in interactive data analysis. An outcome of this work may be to formulate a grammar for structuring exploratory research findings. For a stated finding, the grammar would require researchers to identify, using theory and prior knowledge: confounding context that is present/absent in their analysis, the spatiotemporal scale at which their stated finding is observed and how it may generalise outside of this; the weight of evidence in support of some theory implied by a finding relative to other plausible theories."
  },
  {
    "objectID": "challenge/inference/index.html#inference-and-replicability-in-data-driven-science-capturing-context",
    "href": "challenge/inference/index.html#inference-and-replicability-in-data-driven-science-capturing-context",
    "title": "Inference",
    "section": "",
    "text": "A familiar challenge in data-driven science is that of inference. Generalising away from a sample to some target population is challenging when datasets are already large and population-level and observations subject to unknown confounding context. Replication is muddled by the number of analysis choices that are to be made and range of alternatives that could be pursued (the forking paths problem). How, then, can we ensure that the inferences and claims we make from exploratory analyses are properly contextualised?\nDrawing on expertise in Statistics, Human Computer Interaction and Spatial Econometrics, we will explore ways of systematically describing context in interactive data analysis. An outcome of this work may be to formulate a grammar for structuring exploratory research findings. For a stated finding, the grammar would require researchers to identify, using theory and prior knowledge: confounding context that is present/absent in their analysis, the spatiotemporal scale at which their stated finding is observed and how it may generalise outside of this; the weight of evidence in support of some theory implied by a finding relative to other plausible theories."
  },
  {
    "objectID": "challenge/inference/index.html#related-work-challenge-team",
    "href": "challenge/inference/index.html#related-work-challenge-team",
    "title": "Inference",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nBeecham, R. and Lovelace, R. (2023) A Framework for Inserting Visually Supported Inferences into Geographical Analysis Workflow: Application to Road Safety Research, Geographical Analysis, 55(3): 345–366.\nBeecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J. Wood. (2017). “Map LineUps: Effects of Spatial Structure on Graphical Inference.” IEEE Transactions on Visualization and Computer Graphics, 23: 391–400.\nCook, D., Reid, N., & Tanaka, E. (2021) The Foundation Is Available for Thinking About Data Visualization Inferentially. Harvard Data Science Review, 3(3).\nFranklin, R. (2023) Quantitative methods II: Big theory. Progress in Human Geography, 47(1), 178-186.\nHullman, J., P. Resnick, and E. Adar. (2015). “Hypothetical Outcome Plots Outperform Error Bars And Violin Plots for Inferences About Reliability of Variable Ordering.” PLoS One, 10: e0142444.\nKale, A., Wu, Y. and Hullman, J. (2022) “Causal Support: Modeling Causal Inferences with Visualization”, IEEE Transactions on Visualization and Computer Graphics, 28(1): 1150-1160.\nH. Wickham, D. Cook, H. Hofmann and A. Buja (2010) “Graphical inference for infovis,” IEEE Transactions on Visualization and Computer Graphics, 16(6): 973–979.\nWolf, L.J. et al. (2021) Quantitative geography III: Future challenges and challenging futures. Progress in Human Geography, 45(3): 596–608.\nZhang, J. and Wolf, L. (2023) Rethinking “causality” in Quantitative Human Geography. doi: 10.31235/osf.io/7yncm."
  },
  {
    "objectID": "challenge/tools/index.html",
    "href": "challenge/tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Interactive data analysis has been made possible by coding ecosystems, such as tidyverse in R and pandas in Python, that support iterative dialogue between data and models. The analysis process leaves behind a diverse trail of pathways explored, decisions made, failed attempts and also conclusions that satisfactory outcomes have been reached, i.e., when to stop an exploration? How can these analysis trails be documented, how can the provenance of these processes be recorded and serve as evidence and eventually as communicative artefacts?\nThis theme will focus on tools and technologies through which exploratory data analysis workflows can be documented, communicated and shared. Here we will explore the space of computational notebook environments – how they can be used to record and communicate interactive data analysis processes. Drawing on work from themes #1 and #2 we will identify examples of interactive data analyses, documented via computational notebooks, that balance complexity of information with claims to knowledge."
  },
  {
    "objectID": "challenge/tools/index.html#from-analysis-to-communication-tools-for-data-driven-science-with-integrity",
    "href": "challenge/tools/index.html#from-analysis-to-communication-tools-for-data-driven-science-with-integrity",
    "title": "Tools",
    "section": "",
    "text": "Interactive data analysis has been made possible by coding ecosystems, such as tidyverse in R and pandas in Python, that support iterative dialogue between data and models. The analysis process leaves behind a diverse trail of pathways explored, decisions made, failed attempts and also conclusions that satisfactory outcomes have been reached, i.e., when to stop an exploration? How can these analysis trails be documented, how can the provenance of these processes be recorded and serve as evidence and eventually as communicative artefacts?\nThis theme will focus on tools and technologies through which exploratory data analysis workflows can be documented, communicated and shared. Here we will explore the space of computational notebook environments – how they can be used to record and communicate interactive data analysis processes. Drawing on work from themes #1 and #2 we will identify examples of interactive data analyses, documented via computational notebooks, that balance complexity of information with claims to knowledge."
  },
  {
    "objectID": "challenge/tools/index.html#related-work-challenge-team",
    "href": "challenge/tools/index.html#related-work-challenge-team",
    "title": "Tools",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nPu, X. and Kay, M. (2018) The garden of forking paths in visualization: A design space for reliable exploratory visual analytics: Position paper. In 2018 IEEE Evaluation and Beyond-Methodological Approaches for Visualization (beliv).\nTurkay, C. et al. (2017) Supporting theoretically-grounded model building in the social sciences through interactive visualisation, Neurocomputing, 268:153–163.\nWickham, H. and Grolemund, G. (2019) R for Data Science, Orielly.\nWood, J., Kachkaev, A. and Dykes, J. (2018) Design exposition with literate visualization. IEEE Transactions on Visualization and Computer Graphics, 25(1): 751–768."
  },
  {
    "objectID": "lecture/index.html",
    "href": "lecture/index.html",
    "title": "Public Lecture",
    "section": "",
    "text": "The steady expansion in the availability and reach of observational data has prompted much-needed introspection into data analysis practice. When using data to answer questions, it is not simply a case of choosing from a set of appropriate statistical procedures, or ‘rolling out’ some research design template. Effective data analysis requires analysts to make decisions within a wide space of analysis options and to engage deeply with the processes and mechanisms being represented through data. This public lecture features three internationally-standout scientists from academia and industry. Talks will cover how to challenge and interrogate in data-based research; techniques for imagining uncertainty and variation in observational data; and how data-driven analyses can be put into production.\n\nDate/Time : Thursday 20th June, 1730-2100\nVenue : King’s College London\nDetails : Drinks reception following talks\n\n\n\n\n\n\n\n\nStandard paradigms for data-based decision making and policy analysis fail, and have led to a replication crisis in science, because they can’t handle uncertainty and variation and because they don’t seriously engage with the quality of evidence. We discuss how this has happened, touching on the piranha problem, the butterfly effect, the magic number 16, the one-way-street fallacy, the backpack fallacy, the Edlin factor, Clarke’s law, the analyst’s paradox, and the greatest trick the default ever pulled. We then discuss ways to go beyond the push-a-button, take-a-pill model to a more active engagement of data in science.\n\n\n\n\n\n\nAndrew Gelman is Professor of Statistics and Political Science at Columbia University. His research in applied statistics is wide-ranging within and beyond Political Science. He has received the Outstanding Statistical Application award three times from the American Statistical Association, the award for best article published in the American Political Science Review, the Mitchell and DeGroot prizes from the International Society of Bayesian Analysis, and the Council of Presidents of Statistical Societies award.\n\n\n\n\n\n\n\n\n\n\n\nLearning from data, whether in exploratory or confirmatory analysis settings, requires one to reason about the likelihood of many competing explanations. However, people are boundedly rational agents who often engage in pattern-finding at the expense of recognizing uncertainty or considering potential sources of heterogeneity and variation in the effects they seek to discover. Taking this seriously motivates new classes of interface tools that help people extend their imagination in hypothesizing and interpreting effects.\n\n\n\n\n\n\nJessica Hullman is Ginni Rometty Associate Professor of Computer Science at Northwestern University. Her research addresses challenges and limitations that arise when people draw inductive inferences from data. Her work has contributed visualization and interaction techniques decision-making and analysis, as well as theoretical frameworks for understanding the role of visualization in statistical workflow. Jessica’s work has been awarded best paper awards at top visualization and HCI venues, a Microsoft Faculty award, and NSF CAREER, Medium, and Small awards as PI, among others.\n\n\n\n\n\n\n\n\n\n\n\nThis talk will discuss what it means to put data science “in production”. In industry, any successful data science project will be run repeatedly for months or years, typically on a server that you can’t work with interactively. This poses an entirely new set of challenges that you won’t encounter in your classes in university, but are vital to overcome if you want to have an impact in your job.\nIn this talk, I’ll discuss three principles that I’ve found useful for understanding data science in production: not just once, not just my computer, and not just by myself. I’ll discuss the challenges associated with each, and where possible, what solutions (both technical and sociological) are currently available.\n\n\n\n\n\n\nHadley Wickham is Chief Scientist at Posit PBC, winner of the 2019 COPSS award, and a member of the R Foundation. He builds tools (both computational and cognitive) to make data science easier, faster, and more fun. His work includes packages for data science (like the tidyverse, which includes ggplot2, dplyr, and tidyr)and principled software development (e.g. roxygen2, testthat, and pkgdown). He is also a writer, educator, and speaker promoting the use of R for data science. Learn more on his website, &lt;http://hadley.nz&gt;."
  },
  {
    "objectID": "lecture/index.html#rigor-imagination-and-production-in-data-driven-science",
    "href": "lecture/index.html#rigor-imagination-and-production-in-data-driven-science",
    "title": "Public Lecture",
    "section": "",
    "text": "The steady expansion in the availability and reach of observational data has prompted much-needed introspection into data analysis practice. When using data to answer questions, it is not simply a case of choosing from a set of appropriate statistical procedures, or ‘rolling out’ some research design template. Effective data analysis requires analysts to make decisions within a wide space of analysis options and to engage deeply with the processes and mechanisms being represented through data. This public lecture features three internationally-standout scientists from academia and industry. Talks will cover how to challenge and interrogate in data-based research; techniques for imagining uncertainty and variation in observational data; and how data-driven analyses can be put into production.\n\nDate/Time : Thursday 20th June, 1730-2100\nVenue : King’s College London\nDetails : Drinks reception following talks\n\n\n\n\n\n\n\n\nStandard paradigms for data-based decision making and policy analysis fail, and have led to a replication crisis in science, because they can’t handle uncertainty and variation and because they don’t seriously engage with the quality of evidence. We discuss how this has happened, touching on the piranha problem, the butterfly effect, the magic number 16, the one-way-street fallacy, the backpack fallacy, the Edlin factor, Clarke’s law, the analyst’s paradox, and the greatest trick the default ever pulled. We then discuss ways to go beyond the push-a-button, take-a-pill model to a more active engagement of data in science.\n\n\n\n\n\n\nAndrew Gelman is Professor of Statistics and Political Science at Columbia University. His research in applied statistics is wide-ranging within and beyond Political Science. He has received the Outstanding Statistical Application award three times from the American Statistical Association, the award for best article published in the American Political Science Review, the Mitchell and DeGroot prizes from the International Society of Bayesian Analysis, and the Council of Presidents of Statistical Societies award.\n\n\n\n\n\n\n\n\n\n\n\nLearning from data, whether in exploratory or confirmatory analysis settings, requires one to reason about the likelihood of many competing explanations. However, people are boundedly rational agents who often engage in pattern-finding at the expense of recognizing uncertainty or considering potential sources of heterogeneity and variation in the effects they seek to discover. Taking this seriously motivates new classes of interface tools that help people extend their imagination in hypothesizing and interpreting effects.\n\n\n\n\n\n\nJessica Hullman is Ginni Rometty Associate Professor of Computer Science at Northwestern University. Her research addresses challenges and limitations that arise when people draw inductive inferences from data. Her work has contributed visualization and interaction techniques decision-making and analysis, as well as theoretical frameworks for understanding the role of visualization in statistical workflow. Jessica’s work has been awarded best paper awards at top visualization and HCI venues, a Microsoft Faculty award, and NSF CAREER, Medium, and Small awards as PI, among others.\n\n\n\n\n\n\n\n\n\n\n\nThis talk will discuss what it means to put data science “in production”. In industry, any successful data science project will be run repeatedly for months or years, typically on a server that you can’t work with interactively. This poses an entirely new set of challenges that you won’t encounter in your classes in university, but are vital to overcome if you want to have an impact in your job.\nIn this talk, I’ll discuss three principles that I’ve found useful for understanding data science in production: not just once, not just my computer, and not just by myself. I’ll discuss the challenges associated with each, and where possible, what solutions (both technical and sociological) are currently available.\n\n\n\n\n\n\nHadley Wickham is Chief Scientist at Posit PBC, winner of the 2019 COPSS award, and a member of the R Foundation. He builds tools (both computational and cognitive) to make data science easier, faster, and more fun. His work includes packages for data science (like the tidyverse, which includes ggplot2, dplyr, and tidyr)and principled software development (e.g. roxygen2, testthat, and pkgdown). He is also a writer, educator, and speaker promoting the use of R for data science. Learn more on his website, &lt;http://hadley.nz&gt;."
  },
  {
    "objectID": "posts/01-models/index.html",
    "href": "posts/01-models/index.html",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "",
    "text": "We have all been there: you have your research question, you’ve gathered your dataset, and you are ready to start digging. Or perhaps you only have a question and are wondering how you could answer it in the first place given all the options available. Ever since the invention of data analysis, the human analyst has held the primary role of thinking, deciding and analyzing. At the same time, there has been a steady progression toward automating parts of statistical workflow that are better done by machines, such as calculation. With impressive recent advancements in general purpose intelligent assistants like chatbots, we are in a better position than ever to imagine what an intelligent assistant who acts as a helpful collaborator during data analysis might look like. These new possibilities provide an opportunity to reflect on the ideal interaction between statistical tools and human knowledge. What are the risks involved with increasing amounts of automation in data analysis steps like problem specification, data collection, model specification and selection, and interpretation of results? What are the opportunities? How can we ensure that the sum of human knowledge and AI prediction in data analysis will be more than its parts?\nTo formalize the problem slightly, one could say that data analysis has traditionally involved applying some sequence of operations on data (e.g., reductions or visualizations) which we shall express as functions \\(f = (f_1, f_2, …, f_n)\\) to data \\(D\\), each of which generates some intermediate output \\(y_i = f_i(D)\\) to produce some ultimate knowledge output or interpretation \\(K = f\\)\\(_{human}(y)\\), where \\(f_{human}\\) represents a human processing \\(y = (y_1, y_2, …, y_n)\\). For example, a conventional visual analytics workflow involves a human selecting and applying to data some set of queries or operations \\(f(D)\\) (which might consist of filters, aggregations, regressions, etc.), then interpreting the output of these functions to produce some interpretation or decision \\(K\\).\nAs we consider how this simple formulation changes with increasing levels of automation, we propose this gives rise to a spectrum. On the one end you have a human-directed extreme, such as in conventional visual analytics (Keim et al. 2008), where the human has chosen the specific functional form of the members of \\(f\\) to apply to the data given their prior knowledge to find \\(K\\), and automation is involved primarily to perform what would otherwise be tedious calculations.\nIn the current age of technological development, where products like ChatGPT are promoted for data analysis, from insight discovery to communication, we are closer to being able to imagine the other extreme of this spectrum: fully automated data analysis. In this case, the human only asks the question or provides a goal and optionally some data \\(D\\). The remainder of the process is automated, concluding with the machine presenting its interpretation \\(K\\) (i.e., \\(K=f_{assistent} (y)\\)). The human is not directly involved in selecting \\(f\\) nor in processing the intermediate outputs of \\(y\\). Consequently, the selected f may not be interpretable to a human in the sense of being explainable via human-meaningful parameters.\nAlthough it may seem to some readers to be immediately apparent that removing the human from data science is a bad idea, it is not at all clear that the current extent of automation in data analysis workflows is sufficient. Integrating more machine intelligence may help address human limitations we have had no choice but to accept in data analysis. For example, the human will be limited by their own experience, and may only be able to see a limited set of all the possible operations or functions that could be applied. Or, information processing biases may lead them to misperceive \\(y\\), resulting in a non-optimal \\(K\\) even if their selection of \\(f\\) was in fact optimal. There would seem to be many ways that using AI could improve analysis outputs in light of the limited knowledge and experience that any human analyst might have.\nWe propose that thinking about the spectrum from fully human-driven to fully machine-driven analysis is helpful in several ways. First, by explicitly reflecting on what the ideal role of the human is in data analysis, we gain insight into what aspects of data analysis we think are truly human or non-automatable, and which are better given to the machine. Second, as research and practice increasingly makes use of predictive modeling, we are in a better position to apply our expertise on statistical modeling to design better futures for interactive data analysis. Below we explore this spectrum from its two extremes."
  },
  {
    "objectID": "posts/01-models/index.html#introduction",
    "href": "posts/01-models/index.html#introduction",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "",
    "text": "We have all been there: you have your research question, you’ve gathered your dataset, and you are ready to start digging. Or perhaps you only have a question and are wondering how you could answer it in the first place given all the options available. Ever since the invention of data analysis, the human analyst has held the primary role of thinking, deciding and analyzing. At the same time, there has been a steady progression toward automating parts of statistical workflow that are better done by machines, such as calculation. With impressive recent advancements in general purpose intelligent assistants like chatbots, we are in a better position than ever to imagine what an intelligent assistant who acts as a helpful collaborator during data analysis might look like. These new possibilities provide an opportunity to reflect on the ideal interaction between statistical tools and human knowledge. What are the risks involved with increasing amounts of automation in data analysis steps like problem specification, data collection, model specification and selection, and interpretation of results? What are the opportunities? How can we ensure that the sum of human knowledge and AI prediction in data analysis will be more than its parts?\nTo formalize the problem slightly, one could say that data analysis has traditionally involved applying some sequence of operations on data (e.g., reductions or visualizations) which we shall express as functions \\(f = (f_1, f_2, …, f_n)\\) to data \\(D\\), each of which generates some intermediate output \\(y_i = f_i(D)\\) to produce some ultimate knowledge output or interpretation \\(K = f\\)\\(_{human}(y)\\), where \\(f_{human}\\) represents a human processing \\(y = (y_1, y_2, …, y_n)\\). For example, a conventional visual analytics workflow involves a human selecting and applying to data some set of queries or operations \\(f(D)\\) (which might consist of filters, aggregations, regressions, etc.), then interpreting the output of these functions to produce some interpretation or decision \\(K\\).\nAs we consider how this simple formulation changes with increasing levels of automation, we propose this gives rise to a spectrum. On the one end you have a human-directed extreme, such as in conventional visual analytics (Keim et al. 2008), where the human has chosen the specific functional form of the members of \\(f\\) to apply to the data given their prior knowledge to find \\(K\\), and automation is involved primarily to perform what would otherwise be tedious calculations.\nIn the current age of technological development, where products like ChatGPT are promoted for data analysis, from insight discovery to communication, we are closer to being able to imagine the other extreme of this spectrum: fully automated data analysis. In this case, the human only asks the question or provides a goal and optionally some data \\(D\\). The remainder of the process is automated, concluding with the machine presenting its interpretation \\(K\\) (i.e., \\(K=f_{assistent} (y)\\)). The human is not directly involved in selecting \\(f\\) nor in processing the intermediate outputs of \\(y\\). Consequently, the selected f may not be interpretable to a human in the sense of being explainable via human-meaningful parameters.\nAlthough it may seem to some readers to be immediately apparent that removing the human from data science is a bad idea, it is not at all clear that the current extent of automation in data analysis workflows is sufficient. Integrating more machine intelligence may help address human limitations we have had no choice but to accept in data analysis. For example, the human will be limited by their own experience, and may only be able to see a limited set of all the possible operations or functions that could be applied. Or, information processing biases may lead them to misperceive \\(y\\), resulting in a non-optimal \\(K\\) even if their selection of \\(f\\) was in fact optimal. There would seem to be many ways that using AI could improve analysis outputs in light of the limited knowledge and experience that any human analyst might have.\nWe propose that thinking about the spectrum from fully human-driven to fully machine-driven analysis is helpful in several ways. First, by explicitly reflecting on what the ideal role of the human is in data analysis, we gain insight into what aspects of data analysis we think are truly human or non-automatable, and which are better given to the machine. Second, as research and practice increasingly makes use of predictive modeling, we are in a better position to apply our expertise on statistical modeling to design better futures for interactive data analysis. Below we explore this spectrum from its two extremes."
  },
  {
    "objectID": "posts/01-models/index.html#the-human-directed-extreme",
    "href": "posts/01-models/index.html#the-human-directed-extreme",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "The human-directed extreme",
    "text": "The human-directed extreme\nAt one end of the spectrum, the human selects the functions to be applied, processes the outputs of those functions more or less manually (i.e., without necessarily using formal decision rules), and maps the beliefs that they form to an interpretation. We assume that the human has access to visual analytics processes and technology (Keim et al., 2008). By this we mean that the human does not perform the calculation themselves (we aren’t worried about mathematical errors in generating the individual \\(i\\)), and they have access to visual analytics systems that perform the calculations and present the answers in a visual form for interpretation through \\(f_{human}\\). We assume the human has some level of understanding of how the particular set of functions work and what they mean. In other words, \\(f\\) consists of interpretable operations, such as parameterized functions \\(f_{theta}\\) where theta is considered meaningful to the human, or simple deterministic functions like finding the maximum of data series. It is worth noting that despite the interpretability of the operations applied, how exactly the human processes the available information (i.e., \\(f_{human}\\)) can be thought of as a black box of sorts, as the generation of \\(y\\) is guided implicitly by the previous experience and domain specific knowledge of the human in addition to the data. It is assumed that the human knows how to best apply their domain knowledge in this scenario.\nWe can loosely analogize the learning problem to Bayesian decision theory for the purpose of identifying failure points. Assume some high level learning goal given some observed dataset; for example, an analyst hired by a large school district might be investigating the question, What factors explain the dip in high school math performance between 2018 and 2020? While normally in applied Bayesian analysis we would focus on how posterior beliefs about the some parameters deemed meaningful are arrived at within the context of some particular model specification, here we will instead think of the knowledge gained from the entire analysis workflow as resulting from some combination of the analyst’s prior knowledge and beliefs, the data at hand, and modeling assumptions. Assume for example that what the analyst finds is intended to inform the school district’s decision about what to invest next year’s budget in. The human analyst will likely bring some relevant domain knowledge (prior beliefs) on which variables are likely to matter and how much, how they should model the data to achieve their goals, what aspects of context should influence their interpretation, etc. For example, maybe they have prior experience on factors that predict high school test scores, biases that exist in available data, contextual knowledge about the specific school district, etc. What they know about such problems will influence the statistical modeling approach they select (e.g., a multiple regression predicting 11th grade standardized math test performance from some particular set of covariates). The output of the models and operations they apply inform their posterior beliefs about which factors influence student performance and how this information should inform a decision.\nIf the analyst were fully Bayesian, they would optimally combine the new information they learn with their prior beliefs to arrive at posterior beliefs, then select the utility maximizing interpretation or decision from a space of possible decisions. But a number of things can go wrong when dealing with boundedly rational agents. The analyst’s prior knowledge itself might be biased or incomplete, leading them to analyze the wrong data, fail to select an appropriate set of operations \\(f\\), etc. They might make bad assumptions in model specification or selection, leading to misleading modeling outputs. They might overconstrain their analysis based on their priors (or discount them too easily) by taking advantage of degrees of freedom, for example if they are predisposed to prefer a certain interpretation leading to biased beliefs they might settle on a particular fitted model because it aligns with these preferences. They might arrive at the wrong interpretation or decision in light of their beliefs.\nOn the other hand, there may be advantages to the fully human case. The human understands exactly what was applied in \\(f\\) and drives the interpretation of \\(y\\), enabling them to apply domain knowledge in a flexible, unscripted way. Their understanding and experience with the functions or models they apply makes it easier for them to debug issues, and they may feel more confident about the interpretation they ultimately arrive at. They have the ability to reformulate the goals and problem upon viewing results at any point, for example if they originally failed to consider some important data. More broadly, if the purpose of most data science is to generate insight from data for human purposes, in the fully human-driven case we are 100% sure that data science is happening as every step was done by and consumed by the human, providing an opportunity for understanding, critique, and reconsideration at each step."
  },
  {
    "objectID": "posts/01-models/index.html#the-fully-automated-extreme",
    "href": "posts/01-models/index.html#the-fully-automated-extreme",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "The fully automated extreme",
    "text": "The fully automated extreme\nAt the fully automated extreme, the human provides an analysis goal and optionally data. When considering the Keim et al. model of visual analytics (Keim et al. 2008), the assistant has complete control over all stages, including automated processing of knowledge. In the extreme, the assistant simply returns the answer, \\(K\\), without any justification or provenance of the analysis. For the purposes of this post, we assume the assistant optimally combines what can be gleaned from all available prior statistical and scientific modeling examples in an autoregressive framework in which its task is to predict the most appropriate sequence of steps given the human’s prompt.\nSeveral risks arise from the training constraints on the assistant. For example, one risk arises from how the assistant’s suggestions will be constrained by what it has seen, which in the best case consists of all prior observed statistical analyses. The model cannot necessarily suggest an altogether new form of analysis unless it in some way represents a combination of previous observed analyses. This raises the question of whether and when we might expect completely new analysis paradigms to emerge which could not somehow be reconstitutions of existing ideas.\nRelatedly, the assistant is constrained to suggesting what is most probable. Ask any statistician if they expect frequencies within a corpus representing all historical examples of statistical analysis to capture the appropriateness of a given analysis path and they will answer with a resounding no. If ritualistic choice of models and interpretation of results is a problem in the fully human directed case, then any agent restricted to optimizing an autoregressive objective over some corpus of training data has the potential to miss something more appropriate but less prevalent in the training data. To some extent modern machine learning pipelines can overcome these biases through processes like fine-tuning, where a small amount of preference data is collected after training an unsupervised foundation model and used to adjust the conditional distribution of the model’s output. However, fine-tuning for particular cases implies a form of interaction that would seem to contradict the extreme fully-automated case: if we expect analyses to improve through interaction to obtain further context- or expert-specific input, we would seem to be advocating for an interactive dialogue between the human user and the assistant.\nRelatedly, the fully automated scenario assumes that the human, at the time of prompting, is able to specify their true analysis goal, and in doing so will know how to customize the prompt to contain any relevant domain knowledge they have. But this begins to sound a lot like our trust that the human knows best how to apply their knowledge in the fully human-driven case. Is this reasonable? Or does the real human analyst require intermediate outputs along the way in order to cue lessons from their prior knowledge and experience? Consider how often real analysis workflows involve a shifting of direction. Upon viewing representations of our data in early Exploratory Data Analysis, we might realize our misconceptions of what it contained. Perhaps the fidelity of information we thought we could capture (and which we need to achieve our analysis goals) is simply not possible, and we must rethink our questions altogether. Or, we might realize upon reviewing model diagnostics that there is little signal in the variables we have collected, but have an idea of what other features we could collect. Can we imagine data analysis without such flexibility? Is it possible that an assistant could anticipate such changes in direction? The answer in the extreme case is simply returned without any reason or process to derive the answer, hence no obvious recourse for the human to take further action. Thus, knowledge is gained in some sense if they choose to trust it, but that might be hard."
  },
  {
    "objectID": "posts/01-models/index.html#charting-a-generalized-path-through-extremes-of-y",
    "href": "posts/01-models/index.html#charting-a-generalized-path-through-extremes-of-y",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "Charting a generalized path through extremes of \\(y\\)",
    "text": "Charting a generalized path through extremes of \\(y\\)\nThe above reflections suggest the ideal amount of machine assistance in data analysis will lie somewhere between the two extremes, and may differ depending on the expertise of the particular human and the specific context. However, we could say that the goal in finding the sweet spot is to identify the points where the human’s imagination holds them back from realizing a better path. If good statistical practice involves judicious use of computation to extend the human’s ability to imagine possible outcomes (e.g., alternative values for a statistic resulting from bias or sampling error, counterfactuals in causal inference, equivalently performing models in machine learning, etc.), a computational assistant who can entertain many models or theories simultaneously provides opportunities to “amplify” human cognition, a stated goal of visualization.\nBut how much should an assistant prompt the human to imagine outside their comfort zone? Attempting to design the optimal human-machine pairing naturally motivates reflection on the extent to which we want data-driven science to be pluralistic, allowing for different beliefs and conventions when it comes to how to best learn from data. Consider a familiar tension in the field of data analysis between inference, theory and explanation, and prediction. Researchers and practitioners in different fields and domains vary in how much they value each. It would seem that an optimal assistant would need to adjust to the specific problem in ways congruent with domain-specific values. In social and natural science applications, the assistant might, for example, behave like a scientist from those traditions, where the generated y (and consequently \\(K\\) derived from \\(y\\)) is grounded in substantive theory. In Operations research where prediction is typically the goal, we might judge possible outputs purely in terms of best out-of-sample performance, leading to a much more quantitative (minimal) inspection of \\(y\\) to arrive at \\(K\\).\nParadigmatic differences leave us with many questions around how the assistant should be constrained or configured to realize the goals of scientific knowledge development. We should think about what level of constraints are tolerable and ultimately desirable for learning in a domain, which is a question about how comfortable we are with letting an assistant push us outside our comfort zones as scientists. If the social or natural scientist fine-tunes or otherwise configures the assistant only in a way that reproduces existing practice, the risk is simply reproducing the status quo. What level of “expanded” imagination is desirable for the purpose of scientific progress? For example, should researchers working in domains that prioritize explanatory approaches be given recommendations informed by predictive modeling, as proposed by Hofman et al. (2021) in advocating for integrative modeling? Are there unified goals, constraints or features of analysis – ways of evaluatin and thinking about the y that gives rise to \\(K\\) – that are independent of domain, or are we ultimately constrained to work only within a particular modeling paradigm because historically boundaries have existed? To what extent is explicit design toward achieving specific features/goals of scientific analysis and different standards of evidence (Hofman et al., 2021) useful in constraining the assistant?\nFor example, to what extent should the ideal pairing of the human and machine for data science seek to prioritize:\n\nSubstantive theory – Extent to which the \\(K\\) that is derived supports, validates and extends knowledge.\nReproducibility and transfer – Extent to which \\(K\\) is reproducible - for example, is it invariant under perturbations we do not believe should substantively change results, such as slight variations in how the assistant is called? Notions of replicability may also matter: when should a closely related analysis on similar datasets in the same domain produce an analogous result?\n\nTransparency – Extent to which \\(K\\) that can be easily understood and interrogated.\nCoverage and generalizability – Extent to which \\(K\\) encompasses a narrow or wide range of settings (contexts, scales, etc.).\nExpansiveness – Extent to which \\(K\\) extends imagination, or that enables the analyst to push beyond bounds of inherited modeling and data paradigms.\n\nDifferent domains and use cases might naturally have different weightings on each of these features, leading to different modeling paradigms. Where do we find ourselves on the spectrum, and to what extent might we expect an intelligent assistant to be informed by knowledge of other points along it, so as to push analysts in a given domain outside of their comfort zone?"
  },
  {
    "objectID": "posts/01-models/index.html#conclusions",
    "href": "posts/01-models/index.html#conclusions",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "Conclusions",
    "text": "Conclusions\nThere are many other risks and benefits of the human-directed and automated extremes than those discussed above. However, several themes arise even from our partial treatment. One is that it seems unlikely that the optimal analysis approach is either fully human-directed (with the primary form of automation being calculation as assumed above) or nearly fully automated, with the human providing only the high level goal and optionally, some data. Without any visibility into how \\(K\\) was produced, the human has no opportunity to apply knowledge that is not contained in the training data to debug operations chosen by the machine. They may not feel confident in their results, and their lack of insight into how they were reached may prevent them from applying the knowledge that is output, leading to a question of whether it is knowledge at all.\nAt the same time, a machine that can run many analyses simultaneously, including approaches the human may not know of or be familiar with, has the potential to result in much more informed interpretations of data. There are many blindspots in human analysis. Forms of model multiplicity (the fact that we can get the same model fit or performance from models that imply very different interpretations of a phenomena) and sources of uncertainty (e.g., about how good our assumptions are) are routinely overlooked. In general, it is unlikely that either extreme will be sufficient, prompting a number of questions about how explicitly an analysis assistant should prompt a human analyst to think more expansively than they otherwise might. Key challenges lie in identifying when, how, and why to elicit human knowledge, so as to show the analyst what they may miss, and how to communicate results from potentially non-human interpretable operations in ways that humans can understand. In the current age of technological advancement with increasingly advanced automated methods we expect the sum will indeed be more than its parts."
  },
  {
    "objectID": "posts/01-models/index.html#references",
    "href": "posts/01-models/index.html#references",
    "title": "Some risks and opportunities of automated data analysis",
    "section": "References",
    "text": "References\nHofman, J.M., Watts, D.J., Athey, S. et al. Integrating explanation and prediction in computational social science. Nature 595, 181–188 (2021). doi: 10.1038/s41586-021-03659-0\nHullman, J., and Gelman, A. (2021). Comparing human to automated statistics. Section 6 of Designing for interactive exploratory data analysis requires theories of graphical inference, Harvard Data Science Review 3 (3).\nHullman, J., Holtzman, A., and Gelman, A. (2023). Artificial intelligence and aesthetic judgment. http://stat.columbia.edu/~gelman/research/unpublished/AI_aesthetic_judgment.pdf\nKeim, D., Andrienko, G., Fekete, JD., Görg, C., Kohlhammer, J., Melançon, G. (2008). Visual Analytics: Definition, Process, and Challenges. In: Kerren, A., Stasko, J.T., Fekete, JD., North, C. (eds) Information Visualization. Lecture Notes in Computer Science, vol 4950. Springer, Berlin, Heidelberg. doi: 10.1007/978-3-540-70956-5_7"
  },
  {
    "objectID": "posts/03-cultures/index.html",
    "href": "posts/03-cultures/index.html",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "",
    "text": "What does data-driven science that is critical, reflexive, and methodologically robust look like? How can we operationalise such a mindset through structured methods and practices to foster different expectations for data-driven science and scientists? Interactive data analysis1 by definition offers approaches that encourage an iterative dialogue between analyst, models, and data, making the most of machine capability and human participation and facilitating the interrogation of context and situatedness of data and analytical workflows. Used mindfully and deliberately, interactivity is a strength—not a threat—for validity, robustness, and value. In this blog, we highlight reasons why analysis cannot be left solely to the machines and provide concrete suggestions for guided interactivity: ways in which analysts can probe themselves and their analytical findings to become habituated to asking: How can I be sure I have found something? How can I be more certain what I have found is notable?"
  },
  {
    "objectID": "posts/03-cultures/index.html#interaction-taking-time-to-look-around",
    "href": "posts/03-cultures/index.html#interaction-taking-time-to-look-around",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "Interaction — Taking time to look around",
    "text": "Interaction — Taking time to look around\nWithin a broader data analysis framework, interaction is a moment of communication—a touchpoint—that happens via a graphic, table, metric, summary statistic, an estimated parameter or a prediction. It’s usually facilitated by coding (as in literate programming), direct manipulation using graphics (as in multiple coordinated views), or through a user-interface. As such, _interactive data analysis _encourages an ongoing dialogue between analyst, analysis, and data. Each touchpoint presents an opportunity to stop and reflect: What have we learned? What might we still learn? How does this extend our imagination, understanding, and vision of the world we inhabit? Amidst pressure to find answers and make decisions, interaction prompts deliberation, curiosity, and reflection."
  },
  {
    "objectID": "posts/03-cultures/index.html#data-driven-science-starts-with-data",
    "href": "posts/03-cultures/index.html#data-driven-science-starts-with-data",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "Data-driven science starts with data",
    "text": "Data-driven science starts with data\nData is a powerful but imperfect lens for making sense, and increasing our understanding, of the world around us23. Human decisions determine (and constrain) the who, what, when, where, and why4 of data—as well as the mechanics of how phenomena and characteristics are measured. The way data is collected substantially determines what inferences can be made, which is sometimes ignored by the human analyst. Humans occasionally apply subjective filters to collected data, as well. For example, the widely publicized gender gap in math, measured with PISA data, is not universal among 15 year olds in numerous countries (however a_ reversed _reading gap is observed)—and the effect is exaggerated from an average gap of 0–20 points out of a full numerical range of 0–1000. The effect is miniscule, and individual-to-individual variation dominates5.\n2 Donoho, D. (2017). 50 Years of Data Science. Journal of Computational and Graphical Statistics, 26(4), 745–766. https://doi.org/10.1080/10618600.2017.1384734.3 Wickham, H., Çetinkaya-Rundel, M. & Grolemund, G. (2023). R for Data Science (2e). O’Reilly Media.4 Çetinkaya-Rundel, M., Dogucu, M. & Rummerfield, W. (2022) The 5Ws and 1H of Term Projects in the Introductory Data Science Classroom, https://doi.org/10.52041/serj.v21i2.37.5 https://www.oecd-ilibrary.org/sites/f56f8c26-en/index.html?itemId=/content/component/f56f8c26-en6 Broman, K. W., & Woo, K. H. (2018). Data Organization in Spreadsheets. The American Statistician, 72(1), 2–10. https://doi.org/10.1080/00031305.2017.1375989.7 Swinscow, T. D. V. (2009) Statistics at Square One (11e), BMJ Books. (Chapter 3).8 Belbin L, Wallis E, Hobern D, Zerger A (2021) The Atlas of Living Australia: History, current state and future directions. Biodiversity Data Journal 9: e65023. https://doi.org/10.3897/BDJ.9.e650239 MacColl, C., Leseberg, N. P., Seaton, R., Murphy, S. A., & Watson, J. E. M. (2023). Rapid and recent range collapse of Australia’s Red Goshawk Erythrotriorchis radiatus. Emu - Austral Ornithology, 123(2), 93–104. https://doi.org/10.1080/01584197.2023.217273510 Malički M, Aalbersberg IJ, Bouter L, Mulligan A, Ter Riet G. (2023) Transparency in Conducting and Reporting Research: A survey of authors, reviewers, and editors across scholarly disciplines. PLoS One. 18(3):e0270054. doi: 10.1371/journal.pone.0270054.Automated validation6 is commonly implemented to prevent mistakes, but, at the root, human decisions guide what to check and what constitutes a mistake. Moreover, omissions in the original data collection can inhibit inference. For example, making population-level references from data requires specifying the population7. It’s not easy, and it may be uncomfortable, to spell out the population. It may also result in awkward realizations, like that half the population was not measured. Other times, for example with animal sightings, values are only recorded when a human is present even though most events happened with no humans looking8 and substantial adjusting is required to provide some assurance of reliability of the inferences made9. Transparency10 in data collection and application helps illuminate these issues.\nThere are multiple analytical starting points when working with data. A data scientist or analyst might commence directly from the data11, exploring potential relationships and patterns to determine what stories or narratives can be uncovered. More traditionally, one might come to the data with hypotheses in hand, seeking to test whether theory-based expectations can be supported. To infer from data and analysis, self-critique encourages questioning. Who and what is captured by the data and what is missing? How do method choices and presentation of results reinforce, obfuscate, or illuminate findings? When are results robust and ready to be shared? When should we stop?\n11 Wickham, H., Lawrence, M., Cook, D. et al. (2009) The Plumbing of Interactive Graphics. Computational Statistics 24, 207–215, https://doi.org/10.1007/s00180-008-0116-x"
  },
  {
    "objectID": "posts/03-cultures/index.html#humans-all-the-way-down",
    "href": "posts/03-cultures/index.html#humans-all-the-way-down",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "Humans all the way down",
    "text": "Humans all the way down\nHuman-in-the-loop interactive data analysis is a helpful framework for data science- and AI-driven research, encouraging those explicit touchpoints between researcher and computer in which expert knowledge can shape and feed back into an iterative analytical process guided by real-world understanding. Although an appealing solution to existing problems, the human-in-the-loop proposition relies on an implicit null model of human-not-in-the-loop: that without deliberate touchpoints, data-driven research is somehow insulated from human fallibility (and expertise). This is patently not the case. Human fingerprints are all over data science applications, even when machines have guided the process from start to finish.\n\nHumans are in the data loop\nAlthough some shortcomings of data are commonly acknowledged, like bias, representation, or missingness, there is less acknowledgement that, particularly for surveys or administrative data, data are created by humans. Humans decide what questions to ask, what the answer space looks like, how to convert raw answers into variables, measures, and indices, and what will be the unit of observation. The example at the top of this post illustrates how a massive and ongoing data collection mechanism like the US decennial census failed to count same-sex households in the late 20th century. The option not only didn’t appear as an element in the answer space, but responses were changed for those households where both adults reported the same sex, on the assumption that error was involved12. In this case, human researchers not only failed to conceptualise the full space of real household behaviours, but also amended the data to fit a set of assumptions. Similar blinders or constraints exist for household structures that assume marriage, or ethnicity and race categories that force respondents into predefined categories, or sex/gender questions that fail to reflect actual lived identities.\n12 DeMaio, T., Bates, N., and O’Connell, M. (2013). Exploring Measurement Error Issues In the Reporting of Same-Sex Couples. Public Opinion Quarterly 77(S1): 145-158. https://doi.org/10.1093/poq/nfs066\n\nHumans are in the theory loop\nTheory is often assumed to be less relevant for data-driven research—the idea that big data (and big code) renders theory obsolete13. This can also be read as another form of blindness to humans in the loop, as theory is, without argument, created by humans. Wait, you say, how can theory impact data-driven research in which theory plays no explicit role? First, theory is what determines the content of our data. We know to collect data about race, gender, age, educational attainment, and geographical location (as just a few examples) precisely because theory tells us these are important for understanding outcomes, behaviors, and preferences. Second, the questions we ask of our data—even in exploration—may not be testing theories, but are certainly theory-driven. When handed wage or income data, it is theory that whispers in our ear, “how does this compare across genders?”\n13 https://www.wired.com/2008/06/pb-theory/\n\nAnd humans are in the methods loop, too\nOk, but surely the methods we employ—software packages, models, and statistics—are free of humans? Not quite. Methods are created and implemented by humans in order to solve problems and answer questions generated by humans. This necessarily means that the types of analysis (and therefore eventual answers or findings) are constrained by the tools we have at our disposal. From the development of statistics to compare groups to innovative types of data visualisation (like hexagon maps14 over traditional choropleth maps), what at first glance might appear to be an off-the-shelf machine procedure is actually a human-in-the-loop solution.\n14 https://srkobakian.github.io/sugarbag/index.html"
  },
  {
    "objectID": "posts/03-cultures/index.html#why-human-involvement-matters",
    "href": "posts/03-cultures/index.html#why-human-involvement-matters",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "Why human involvement matters",
    "text": "Why human involvement matters\nIn a nutshell, robust inference is difficult. One explanation for this is the “garden of forking paths” metaphor15 which illustrates the challenges of interactive hypothesis testing and the blurry boundaries between explanatory and confirmatory analysis. The challenge is, however, much more extensive: implicit choices embedded in theory, data, and methods also hamper inference. Moreover, the wider contexts in which data analysis takes place, whether academic, governmental, industry, or media, privilege speed and certainty of findings. Academic systems, including publication and promotion, reward quantity, innovation, and conformity, meaning there is little if any incentive for introspection, replication, or any other approach that might reinforce confidence in findings.\n15 http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdfThe assumption is that humans get in the way of robust inference—that, consciously and subconsciously, we make choices that facilitate the (occasionally spurious) detection of patterns, signals, and answers. We know to shine a spotlight on model specification and interpretation, whether exploratory or confirmatory—obvious touchpoints where humans and machines are in contact—as areas vulnerable to analyst bias. To go further and argue that human touchpoints exist at every stage of research, from theory to data to methods, is to invite capitulation. Perhaps robust analysis is impossible. Surrender is not what we are proposing, however! We are confident analysts can respond creatively to challenges, once they’re identified. To do this, though, we need to become better at academic self-reflection and also take greater advantage of a wide range of inference support approaches. We have some suggestions:\n\nBe a “social” scientist: talk to people!\nOne of the easiest ways to stress-test data analysis is to talk to people about it! Getting feedback is central to improvement. Write a working paper, post it on ArXiV/OSF, publish code as an open-source software package, and talk about the work at conferences and on social media. Indeed, the things that we worry about in analysis or research are often quite different from the stuff that other people might find concerning. It’s useful to note here that just posting (or talking) about the work is often not enough to actually attract feedback; teaching people how to use one’s open source code might be necessary in order to get feedback on the ideas it executes. More informal “brown-bag” or “workshop”-style events are often designed for this kind of structured feedback, and tools like Margins/Librarian from Fermats Library16 or Hypothesis17 can help facilitate online input.\n16 https://fermatslibrary.com/17 https://web.hypothes.is/18 https://thesiswhisperer.com/2023/07/10/academicenshittification/19 Whitmer, J. M. You are your brand: self-branding and the marketization of self. Sociology Compass 13(3): e12662. https://doi.org/10.1111/soc4.12662At the same time, it is important to acknowledge that social media and conferences are not a cure-all for gaining perspective. Travel to conferences can be expensive and time-consuming, with clear disparities in access. Furthermore, conferences can be competitive, high-stakes environments for some, making it difficult to both give and receive effective and constructive critique. Social media also has some serious drawbacks18, in part due to the imperative to develop and maintain a ‘personal brand’ on those platforms.19 At a higher level, seeking input from others can make it difficult to find a balance between cultivating an individual voice and learning to adopt the perspectives and norms within a research community. This is especially the case for early career researchers. Thus, it can be helpful to cultivate methods to support internal self-reflection.\n\n\nInternalize a critical perspective\nOne common practice is to imagine how someone else might react to our work. This might be a PhD or work supervisor (or another mentor) voicing common questions or routine self-checks: _Have you plotted the data first? Have you tried to solve the problem yourself before reaching for something off the shelf? Has someone else solved it differently? Why did the previous work on this topic miss what you’re proposing? _This also might come in the form of conceptualizing a researcher persona, a fictional person who works from a given research background (like an “experimentalist” or a “data scientist”), who critiques work from a particular angle. This can help identify weaknesses in research design, or identify places where analytical choices might merit more justification. This can be quite useful as a mode of self-reflection, and has its place in developing an analytical voice and sense of self.\nNot everybody has had such a mentor or, if they have one, it might be hard to internalize their voice positively. Mentoring is quite important, but it can be difficult to cultivate good mentor-mentee relationships.20 When thinking about research personas, critiques might be too total to effectively improve a draft. An old joke comes to mind:\n20 https://www.science.org/content/article/improving-mentoring-academia-requires-collective-effort.A driver, tired and lost, pulls over to the side of the road and asks a shepherd:\n\n“How do I get to town?” _\n\nThe shepherd responds,\n\n“Well, I wouldn’t start from here!”\n\nIndeed, if an analyst knew that analysis was problematic, they might ostensibly change how they executed that part of the research in the first place. Of course, the analyst doesn’t know what they don’t know until they try to gain perspective. This suggests that surprise is essential to effective self-reflection—learning about the ways you can improve will involve some kind of realization, a recognition of something previously unknown, and is fundamental to effective self-reflection.\n\n\nLeverage randomness and randomization\nAnother way to elicit new perspectives is to invoke randomness21, like an Oblique Strategies22 for researchers. In its original form, this_ _is a deck of cards intended to help artists get over creative blocks. Each card has a different short statement used as a prompt for engagement23. The card might suggest to “Discover the recipes you are using and abandon them” or “Honor thy error as a hidden intention”. These are useful provocations to escape a creative block. They are unlikely to directly help determine a best course of action during a data analysis, or decide on a better model, but are an effective technique to imagine what this relationship might look like if there was really nothing going on2425, or vary if a different sample was taken. Invoking randomness includes permuting values26, bootstrapping27 the sample, creating cross-validation folds2829, simulating30 from a distribution—all mindsets and tools that enable the development of perspective on data patterns and provide confidence in interpretations.\n21 http://library.isical.ac.in:8080/jspui/bitstream/10263/5519/1/C%20R%20Rao%20speech.pdf.22 https://en.wikipedia.org/wiki/Oblique_Strategies.23 You can try drawing a few of these yourself here: http://stoney.sb.org/eno/oblique.html.24 Buja A., Cook D., Hofmann H., Lawrence M., Lee E.-K., Swayne D. F. and Wickham H. (2009) Statistical inference for exploratory data analysis and model diagnostics, Phil. Trans. R. Soc. A.3674361–4383 http://doi.org/10.1098/rsta.2009.0120.25 Wickham, H., Cook, D., Hofmann, H. and Buja, A., “Graphical inference for infovis,” in IEEE Transactions on Visualization and Computer Graphics, vol. 16, no. 6, pp. 973-979, Nov.-Dec. 2010, doi: 10.1109/TVCG.2010.161.26 Good, P. (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses, Springer.27 Efron, B. (1979) Bootstrap Methods: Another Look at the Jackknife. Ann. Statist. 7 (1) 1 - 26, January, 1979. https://doi.org/10.1214/aos/1176344552.28 Allen, D. M. (1974). The Relationship between Variable Selection and Data Augmentation and a Method for Prediction, Technometrics. 16 (1): 125–127. doi:10.2307/1267500.29 Stone, M. (1974). Cross-Validatory Choice and Assessment of Statistical Predictions, Journal of the Royal Statistical Society, Series B (Methodological). 36 (2): 111–147. doi:10.1111/j.2517-6161.1974.tb00994.x.30 Sokolowski, J. A., Banks, C. M. (2009). Principles of Modeling and Simulation. John Wiley & Son. p. 6. ISBN 978-0-470-28943-3.\n\nConsider granularity\nOne pattern of thought might be to use more fine-grained explanations: what specific observations are necessary for a result to hold? There are a few new statistical approaches that can help identify, for example, how much of a dataset is needed in order to preserve a core finding31 or how “serious” an omitted confounder would have to be before an effect disappears32. In either case, these statistical tools help identify why a result arises from a given dataset, characterize the conditions under which they might not, and find what new data might be needed (or analyses conducted) in order to make the work more robust.\n31 Broderick, T., Giordano, R., and Meager, R. (2023). An automatic finite-sample robustness metric: when can dropping a little data make a big difference. ArXiV preprint: https://arxiv.org/abs/2011.1499932 Cinelli, C. and Hazlett, C. (2019). Making sense of sensitivity: extending omitted variable bias. Journal of the Royal Statistical Society, Series B: Statistical Methodology 82(1), 39-67. https://doi.org/10.1111/rssb.12348\n\nIdentify saturation\nHow much analysis is enough? The question of when to stop collecting data, when to stop estimating models, or when a question has been answered—when to stop fiddling around with analytical details, in short—is one faced across the entire research continuum. An approach widely adopted in qualitative research is that of saturation33: research can stop when no new information or insight is elicited from additional data collection or analysis. This framework is also generally (but cautiously!) applicable to data-driven science. Here the idea is less that analysis stops as soon as a satisfactory answer is found (bad; don’t do this), but rather that a variety of approaches is tried until broadly the same findings emerge across the board. At that point saturation is reached.\n33 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5993836/\n\nUse counterfactuals and what-if scenarios\nRobust inference depends on stress-testing analytical approaches, or theoretical assumptions34, and the ability to interpret black-box models35. Across research domains, the observed analytical relationships between variables can result from a range of underlying processes. Student test scores that increase as class size shrinks36, for example, may result from a direct relationship between the two variables, but might also be attributable to the wealth of the school district, characteristics of students, or a host of other intervening/underlying factors. From a policy standpoint, this is a problem. Where and how should interventions be designed? A counterfactual37 mindset encourages the analyst to consider alternative stories or narratives and to assess whether, and how, analytical findings change when parts of the story are changed. This also relates to explainable artificial intelligence (XAI) methods for complex models38, which articulate how a model makes an individual prediction.\n34 Greco, L., Ventura, L. (2011) Robust inference for the stress–strength reliability. Stat Papers 52, 773–788, https://doi.org/10.1007/s00362-009-0286-9.35 Molnar, C. (2024) Interpretable Machine Learning, https://christophm.github.io/interpretable-ml-book/36 Cho, H., Glewwe, P., Whitler, M. (2012) Do reductions in class size raise students’ test scores? Economics of Education Review, Volume 31, Issue 3, Pages 77-95, https://doi.org/10.1016/j.econedurev.2012.01.004.37 DeMartno, G.F. (2021) The specter of irreparable ignorance: counterfactuals and causality in economics. Review of Evolutionary Political Economy 2: 253-276. https://doi.org/10.1007/s43253-020-00029-w38 Biecek P (2018). “DALEX: Explainers for Complex Predictive Models in R.” Journal of Machine Learning Research, 19(84), 1-5. https://jmlr.org/papers/v19/18-416.html.\n\nSee it to believe it\nData visualisation employs the human visual system to interpret relationships, enabling summaries that are more elaborate than is capable from simple numerical statistics. It is useful in every stage of interactive data analysis, from data exploration to diagnosing and summarising model fit. New statistical thinking in visualisation methods lifts this area into the domain of making inference for unconventional data scenarios and evaluating robustness of findings. As a baseline, the grammar of graphics3940 provides a framework that bridges statistical thinking and data graphics. From this basis, new methods are emerging. Line-ups4142 nudge the viewer to check whether observed relationships are really there. Causal quartets43, which provide a visual “check” on the mechanics of average treatment effects, can help analysts understand the different combinations of individual effects that produce average effects. Hypothetical outcome plots44 enable the analyst to assess what the plot might look like under different samples. Rashomon sets45 are useful for instances in which predictive models offer similar performance, but alternative underlying explanations. One element common to these visual approaches is that they serve to encourage reflection, multiplicity of perspectives, and nuanced interpretation, supporting and cautioning interpretation of results and aiding inference where classical statistical methods do not apply.\n39 Wilkinson, L. (2010), The grammar of graphics. WIREs Comp Stat, 2: 673-677. https://doi.org/10.1002/wics.11840 Hadley Wickham. (2010) A layered grammar of graphics.Journal of Computational and Graphical Statistics, vol. 19, no. 1, pp. 3–28.41 Buja A., Cook D., Hofmann H., Lawrence M., Lee E.-K., Swayne D. F. and Wickham H. (2009) Statistical inference for exploratory data analysis and model diagnostics, Phil. Trans. R. Soc. A.3674361–4383 http://doi.org/10.1098/rsta.2009.0120.42 Wickham, H., Cook, D., Hofmann, H. and Buja, A., “Graphical inference for infovis,” in IEEE Transactions on Visualization and Computer Graphics, vol. 16, no. 6, pp. 973-979, Nov.-Dec. 2010, doi: 10.1109/TVCG.2010.161.43 Gelman, A., Hullman, J. and Kennedy, L. (2023) Causal quartets: Different ways to attain the same average treatment effect, https://arxiv.org/abs/2302.12878.44 A. Kale, F. Nguyen, M. Kay and J. Hullman, Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data in IEEE Transactions on Visualization & Computer Graphics, vol. 25, no. 01, pp. 892-902, 2019. doi: 10.1109/TVCG.2018.2864909.45 Biecek, P., Baniecki, H., Krzyziński, M., & Cook, D. (2024). Performance Is Not Enough: The Story Told by a Rashomon Quartet. Journal of Computational and Graphical Statistics, 1–4. https://doi.org/10.1080/10618600.2024.2344616\n\nEmbrace uncertainty\nModelling is an uncertain endeavour: data are uncertain; models provide (uncertain) estimates of one potential version of reality; and individual point estimates are uncertain. Researchers and analysts, however, work in truths and certainties, generating inevitable tension between data-driven analysis and real-world applications. What to do? Formally, it’s important to embrace opportunities to foreground uncertainty of findings and this can be done visually, numerically, and narratively. More informally, practicing talking about findings as one plausible version of a story out of many performs a wider service to all communities producing and consuming data-driven analysis.\n\n\nRecord provenance and document analytic journeys\nThe human-machine partnership underlying interactive data analysis needs to be documented as a means to ensure reproducibility, facilitate validation, and increase trust—not only in the findings but in the journey itself46. The documentation and provenance of these interactions can be performed at different levels of detail, including with data and methods, as well as how results are validated and reported. The documentation of the analytic journey needs to happen at multiple levels of detail, from coarse high-level representations of the journey to fine-grained, low-level, representations of the analyst’s journey. Ensuring that these are explored in context is also important.\n46 Becker, G., Barr, C., Gentleman, R., & Lawrence, M. (2017). Enhancing Reproducibility and Collaboration via Management of R Package Cohorts. Journal of Statistical Software, 82(1), 1–18. https://doi.org/10.18637/jss.v082.i01"
  },
  {
    "objectID": "posts/03-cultures/index.html#closing-reflections",
    "href": "posts/03-cultures/index.html#closing-reflections",
    "title": "Humans all the way down: statistical reflection and self-critique for interactive data analysis",
    "section": "Closing reflections",
    "text": "Closing reflections\nDespite the rise of artificial intelligence, human intelligence (and fallibility) are evergreen. Human actions are ubiquitous in every part of data analysis, and humans are active players in this game. With the increasing prevalence of open-source tools and open research, everyone has the power to do data analysis. Under these conditions making reliable inferences presents ongoing challenges. Creating spaces in which to try out alternatives facilitates creativity, interactivity, and surprise in data analysis—and, in the end, more robust inference. Building in techniques for self-reflection and critique is more necessary than ever to help ensure confidence in data analysis results, allowing the data to say what it means."
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "We have assembled an interdisciplinary team of 12 world-leaders working in Information Visualization, Statistical Modelling, Human Computer Interaction, Communication Studies and Geography.\nChallenge Leads:\n\nCagatay Turkay, Professor of Visual Data Science, Centre for Interdisciplinary Methodologies, University of Warwick\nRoger Beecham, Associate Professor of Visual Data Science, School of Geography, University of Leeds\n\nChallenge Team:\n\nDaniel Archambault, Professor of Data Science/Visualisation, School of Computing, Newcastle University\nBenjamin Bach, Reader in Design Informatics and Visualization, School of Informatics, University of Edingburgh\nDi Cook, Professor of Business Analytics, Econometrics and Business Statistics, Monash University\nRachel Franklin, Professor of Geographical Analysis, School of Geography, Politics and Sociology, Newcastle University\nAndrew Gelman, Professor of Statistics and Political Science, Department of Statistics, Columbia University\nJessica Hullman, Ginni Rometty Associate Professor, Computer Science and Communication Studies, Northwestern University\nEdwin Pos, Associate Professor, Utrecht University\nMari-Cruz Villa-Uriol, Senior Lecturer, University of Sheffield\nHadley Wickham, Chief Scientist, Posit\nLevi Wolf, Senior Lecturer in Quantitative Human Geography, School of Geographical Sciences, University of Bristol\nJo Wood, Professor of Visual Analytics, Department of Computing, City, University of London\nKai Xu, Associate Professor, School of Computer Science, University of Nottingham"
  }
]