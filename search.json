[
  {
    "objectID": "files/notes.html",
    "href": "files/notes.html",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Commentary paper (e.g. multi-author in RSS Stats and Society)\nAn edited special issue, possibly also RSS Stats and Society)\nSome ambitions for funded activity\nOthers\n\n\n\n\n\nStructured activities rather than free-flow discussion / “talks”.\nTMCF for writing outputs rather than open discussion / networking.\nPre-meeting activities that we ask participants to perform, and communications, should establish above, and enable participants to come with ideas for writing.\n\n\n\n\nNote this is an initial plan to be iterated over.\n\n\n\nGive each participant an extreme position that they have to argue / test the limits of.\nThey elaborate on position, identify what it brings, its consequences and its future. Get people to write provocations relating to position. An example, the famous Anderson quote: “Big Data makes theory obsolete [sic]”\nThey will present these positions on day 1, via 20 minute presentations.\nThey should argue for that position with reference to their own work (this is a way of doing introductions).\nCT and RB to formulate these positions, and slightly informed by the work that individuals do.\n\n\n\n\n\nFrom these positions participants will be allocated into groups. These might fall into disciplinary divides (whether we want to encourage / discourage this is up for discussion).\nAs a group, they write up position with evidence and examples. These documents should be ?? word blog posts.\nWe could impose a common structure on these blog posts (to be discussed)\nBlog posts are shared with participants\n\n\n\n\n\nHaving read/understood positions, each participant identifies consequences for their own research. E.g. what position x,y,z would mean for their work/community.\nThis would be via quick presentations (little prep).\nRest of day, entire group works up a structure for the position paper.\nGroups of individuals attached to sections.\n\n\n\n\n\nGroups of individuals write sections to which they are attached.\n\n\n\n\n\nOpen discussion of Research Agenda, given work done\nPlans for second week of TMCF for local people\n\n\n\n\n\n\nHeuristics trumps theory in data-driven research. Model-building in data-driven research is best approached by consulting data and domain praxis and commonsense rather than isolated theory/ theoretical mechanisms.\nKnowledge claims can only be made through out-of-sample significance tests (extreme pre-registration).\nExploratory analysis induces p-hacking\nData graphics + visual analysis methods induce false discovery via unchecked multiple comparisons\nData graphics + visual analysis methods are free from pitfalls of statistical analysis since they encourage informal calibration and expectation on inferences.\nAutomated analysis is the best hope for doing bias-free research (e.g. removing human from scientific method)\nVisualization as data artefact (e.g. Simulation from null hypothesis – Hops and lineups – for exploring possible outcomes that might have been produced by data, not for marking out confidence intervals)\nFor research to have value, it must be underpinned by crisply stated causal claims.\nThe only interesting/useful hypothesis tests are those where the “null hypothesis” is not rejected (where p&gt;.05).\n\nTensions that above try to expose:\n\nautomation / human-in-loop / human-mediated\npre-registration versus free-form exploration versus heuristics (recipes for doing data-driven research)\nlimits to modelling / diagnostics\npractice of data-driven science / tools\n\n\n\n\n\nPre-registration for Predictive Modeling https://arxiv.org/abs/2311.18807\nWhy We (Usually) Don’t Have to Worry About Multiple Comparisons https://doi.org/10.1080/19345747.2011.618213"
  },
  {
    "objectID": "files/notes.html#challenge-outcomes",
    "href": "files/notes.html#challenge-outcomes",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Commentary paper (e.g. multi-author in RSS Stats and Society)\nAn edited special issue, possibly also RSS Stats and Society)\nSome ambitions for funded activity\nOthers"
  },
  {
    "objectID": "files/notes.html#constraints-checks-for-planning",
    "href": "files/notes.html#constraints-checks-for-planning",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Structured activities rather than free-flow discussion / “talks”.\nTMCF for writing outputs rather than open discussion / networking.\nPre-meeting activities that we ask participants to perform, and communications, should establish above, and enable participants to come with ideas for writing."
  },
  {
    "objectID": "files/notes.html#structure-for-5-days",
    "href": "files/notes.html#structure-for-5-days",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Note this is an initial plan to be iterated over.\n\n\n\nGive each participant an extreme position that they have to argue / test the limits of.\nThey elaborate on position, identify what it brings, its consequences and its future. Get people to write provocations relating to position. An example, the famous Anderson quote: “Big Data makes theory obsolete [sic]”\nThey will present these positions on day 1, via 20 minute presentations.\nThey should argue for that position with reference to their own work (this is a way of doing introductions).\nCT and RB to formulate these positions, and slightly informed by the work that individuals do.\n\n\n\n\n\nFrom these positions participants will be allocated into groups. These might fall into disciplinary divides (whether we want to encourage / discourage this is up for discussion).\nAs a group, they write up position with evidence and examples. These documents should be ?? word blog posts.\nWe could impose a common structure on these blog posts (to be discussed)\nBlog posts are shared with participants\n\n\n\n\n\nHaving read/understood positions, each participant identifies consequences for their own research. E.g. what position x,y,z would mean for their work/community.\nThis would be via quick presentations (little prep).\nRest of day, entire group works up a structure for the position paper.\nGroups of individuals attached to sections.\n\n\n\n\n\nGroups of individuals write sections to which they are attached.\n\n\n\n\n\nOpen discussion of Research Agenda, given work done\nPlans for second week of TMCF for local people"
  },
  {
    "objectID": "files/notes.html#some-themes-that-might-form-positions",
    "href": "files/notes.html#some-themes-that-might-form-positions",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Heuristics trumps theory in data-driven research. Model-building in data-driven research is best approached by consulting data and domain praxis and commonsense rather than isolated theory/ theoretical mechanisms.\nKnowledge claims can only be made through out-of-sample significance tests (extreme pre-registration).\nExploratory analysis induces p-hacking\nData graphics + visual analysis methods induce false discovery via unchecked multiple comparisons\nData graphics + visual analysis methods are free from pitfalls of statistical analysis since they encourage informal calibration and expectation on inferences.\nAutomated analysis is the best hope for doing bias-free research (e.g. removing human from scientific method)\nVisualization as data artefact (e.g. Simulation from null hypothesis – Hops and lineups – for exploring possible outcomes that might have been produced by data, not for marking out confidence intervals)\nFor research to have value, it must be underpinned by crisply stated causal claims.\nThe only interesting/useful hypothesis tests are those where the “null hypothesis” is not rejected (where p&gt;.05).\n\nTensions that above try to expose:\n\nautomation / human-in-loop / human-mediated\npre-registration versus free-form exploration versus heuristics (recipes for doing data-driven research)\nlimits to modelling / diagnostics\npractice of data-driven science / tools"
  },
  {
    "objectID": "files/notes.html#some-additional-reading",
    "href": "files/notes.html#some-additional-reading",
    "title": "TMCF 2024 Planning",
    "section": "",
    "text": "Pre-registration for Predictive Modeling https://arxiv.org/abs/2311.18807\nWhy We (Usually) Don’t Have to Worry About Multiple Comparisons https://doi.org/10.1080/19345747.2011.618213"
  },
  {
    "objectID": "programme/index.html",
    "href": "programme/index.html",
    "title": "Conference Programme",
    "section": "",
    "text": "Monday 17th June\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n1100-1115\n Arrival and networking\n\n\n1115-1130\n TMCF and Turing– Ioannis Kosmidis, TMCF Lead\n\n\n1130-1145\n Turing Overview– Daniel Lovelock, Head of Academic Engagement\n\n\n1145--1200\n This TMCF– Cagatay Turkay, Roger Beecham\n\n\nProvocations and positions\n\n\n1200--1300\n Participants present their provocations, w/ group review– All\n\n\n1300-1400\n Lunch\n\n\n1400--1530\n Participants present their provocations, w/ group review– All\n\n\n1530-1700\n Affinity mapping exercise– All\n\n\n\n\n\n\n\n\n\n\nTuesday 18th June\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n Arrival and networking\n\n\n0930-0945\n Update and plans for day– Cagatay Turkay, Roger Beecham\n\n\nDocumenting positions\n\n\n0945-1230\n Groups co-write 1500-word blog w/ example and evidence– All\n\n\n1230-1330\n Lunch\n\n\n1330-1600\n Groupwork on commentaries– All\n\n\n1600-1630\n De-brief and planning– Cagatay Turkay, Roger Beecham\n\n\n\n\n\n\n\n\n\n\nWednesday 19th June\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n Arrival and networking\n\n\n0930-0945\n Update and plans for day– Cagatay Turkay, Roger Beecham\n\n\n0945-1100\n Lightning talks from Turing PIs on probabilistic programming– TBC\n\n\nGrounding positions\n\n\n1100-1230\n Groupwork on blogs– All\n\n\n1230-1330\n Lunch\n\n\n1330-1600\n Groupwork on blogs– All\n\n\n\n\n\n\n\n\n\n\nThursday 20th June\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n Arrival and networking\n\n\n0930-0945\n Update and plans for day– Cagatay Turkay, Roger Beecham\n\n\nConsolidate\n\n\n0945-1230\n Prioritising and grounding activity– All\n\n\n1130-1230\n Paper planning– All\n\n\n1230-1330\n Lunch\n\n\n1330-1530\n Paper planning + material gathering– All\n\n\n1545-1600\n Progress and next steps– All\n\n\nPublic lecture\n\n\n1730-2100\n Public lectures + drinks reception– Andrew Gelman, Jess Hullman, Hadley Wickham\n\n\n\n\n\n\n\n\n\n\nFriday 21st June\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen\nActivity\n\n\n\n\nWelcome\n\n\n0900-0930\n Arrival and networking\n\n\n0930-0945\n Briefing from Turing Director for Fundamental DS&AI– Andrew Duncan \n\n\nOutput planning\n\n\n0945-1230\n Commentary + SI paper mapping, software, toolkits, etc.– All\n\n\n1230-1400\n Lunch and depart"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "",
    "text": "Theory & Methods Challenge Fortnights are intensive, two-week events where teams of 12 experts from across the Turing university partner network and external institutions worldwide collaborate to initiate work on tackling a foundational challenge in data science and artificial intelligence. Each event focuses on a specific theoretical or methodological challenge that has the potential to impact data science and artificial intelligence at a fundamental level.\nvia turing.ac.uk.\n\nWelcome to our upcoming Turing Theory & Methods Challenge Fortnight (TMCF). Our challenge will take place in the British Library in summer 2024.\nThe ambition behind TMCFs is nicely laid out on the Alan Turing Institute’s website (above). Ours seeks to a address a problem familiar to data-driven research: the garden of forking paths fallacy (Gelman and Loken 2013). We have assembled a stellar team to work on the challenge – world-leaders in data science engineering and practice. Alongside the challenge we intend to host public lectures and seminars – more news on this to follow.\nWe will update these pages through 2024 as planning and other preparatory work develops.\n\n\nCagatay Turkay and Roger Beecham, Challenge Leads"
  },
  {
    "objectID": "index.html#our-tmcf",
    "href": "index.html#our-tmcf",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "",
    "text": "Theory & Methods Challenge Fortnights are intensive, two-week events where teams of 12 experts from across the Turing university partner network and external institutions worldwide collaborate to initiate work on tackling a foundational challenge in data science and artificial intelligence. Each event focuses on a specific theoretical or methodological challenge that has the potential to impact data science and artificial intelligence at a fundamental level.\nvia turing.ac.uk.\n\nWelcome to our upcoming Turing Theory & Methods Challenge Fortnight (TMCF). Our challenge will take place in the British Library in summer 2024.\nThe ambition behind TMCFs is nicely laid out on the Alan Turing Institute’s website (above). Ours seeks to a address a problem familiar to data-driven research: the garden of forking paths fallacy (Gelman and Loken 2013). We have assembled a stellar team to work on the challenge – world-leaders in data science engineering and practice. Alongside the challenge we intend to host public lectures and seminars – more news on this to follow.\nWe will update these pages through 2024 as planning and other preparatory work develops.\n\n\nCagatay Turkay and Roger Beecham, Challenge Leads"
  },
  {
    "objectID": "index.html#key-dates",
    "href": "index.html#key-dates",
    "title": "Navigating the garden of forking paths: theoretical foundations for interactive data analysis in data-driven science",
    "section": "Key Dates",
    "text": "Key Dates\n\nWeek 1 (full team) : Monday 17th June – Friday 21st June 2024\nWeek 2 (UK-based team) : Monday 8th July – Friday 12th July 2024"
  },
  {
    "objectID": "challenge/modelling/index.html",
    "href": "challenge/modelling/index.html",
    "title": "Modelling Paradigms",
    "section": "",
    "text": "Model-building praxis in data-driven science is quite distinctive. As well as predicting and explaining outcomes, models are often used as containers for structuring information and learning within the data space. We will consider the ways in which models are incorporated into interactive data analysis by asking:\n\nHow relevant models are identified and selected;\nHow they are developed and refined; and\nThe processes through which they are analysed and evaluated.\n\nFrom here, we will document what is distinctive about model building in data-driven science."
  },
  {
    "objectID": "challenge/modelling/index.html#modelling-paradigms-for-data-driven-science-learning-from-example",
    "href": "challenge/modelling/index.html#modelling-paradigms-for-data-driven-science-learning-from-example",
    "title": "Modelling Paradigms",
    "section": "",
    "text": "Model-building praxis in data-driven science is quite distinctive. As well as predicting and explaining outcomes, models are often used as containers for structuring information and learning within the data space. We will consider the ways in which models are incorporated into interactive data analysis by asking:\n\nHow relevant models are identified and selected;\nHow they are developed and refined; and\nThe processes through which they are analysed and evaluated.\n\nFrom here, we will document what is distinctive about model building in data-driven science."
  },
  {
    "objectID": "challenge/modelling/index.html#related-work-challenge-team",
    "href": "challenge/modelling/index.html#related-work-challenge-team",
    "title": "Modelling Paradigms",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nBeecham, R. and Lovelace, R. (2023) A Framework for Inserting Visually Supported Inferences into Geographical Analysis Workflow: Application to Road Safety Research, Geographical Analysis, 55(3): 345–366.\nCook, D., Reid, N., and Tanaka, E. (2021) The Foundation Is Available for Thinking About Data Visualization Inferentially. Harvard Data Science Review, 3(3).\nFranklin, R. (2023) Quantitative methods II: Big theory. Progress in Human Geography, 47(1): 178-186.\nGelman, A. (2004) “Exploratory Data Analysis for Complex Models.” Journal of Computational and Graphical Statistics, 13: 755–79.\nHullman, J., and Gelman, A. (2021) Designing for Interactive Exploratory Data Analysis Requires Theories of Graphical Inference. Harvard Data Science Review, 3(3).\nWolf, L.J. (2022) “The Right to Rule by Thumb: A Comment on Epistemology in A Route Map for Successful Applications of Geographically-Weighted Regression, Geographical Analysis, 55(1): 184–190.\nWood, J., D. Badawood, J. Dykes, and A. Slingsby. (2011). “BallotMaps: Detecting Name Bias in Alphabetically Ordered Ballot Papers.” IEEE Transactions on Visualization and Computer Graphics, 17: 2384–91."
  },
  {
    "objectID": "challenge/index.html",
    "href": "challenge/index.html",
    "title": "Challenge Statement",
    "section": "",
    "text": "An expanding array of observational data now enables social, economic and environmental behaviours to be researched in a large-scale and empirical way. This is exciting as there are increasing opportunities to effect evidence-based decision making in science, government and industry. In order to generate insights and draw conclusions, however, such data-intensive research relies on reasonably informal, interactive data analysis approaches: those that combine data graphics and statistics via computational notebooks (Observable, Quarto, Jupyter) or interactive data analysis tools such as Tableau and PowerBI. Unlike traditional scientific processes, we don’t know what an optimal and rigorous interactive analysis should look like and their success more often than not relies on the expertise and skills of the data scientist.\nOur proposed Theory & Methods Challenge will develop proposals for conducting and reporting data-intensive research in a more formal way. Our hope is that this activity will lay the foundations for interactive data analysis practices and provide guidelines for computational tools that underpin the next generation of analysis platforms. Through the workshop, we aim to explore three sub-challenges:\n\nModelling paradigms for data-driven science: establish what is distinctive about modelling in data-driven science by mapping out archetypal data-driven projects and the analysis practices they use.\nInference and replicability in data-driven science: develop systematic ways of documenting the context under which analytical findings are made – a grammar for structuring exploratory research findings – so that inferences can be more formally reported.\nTools for progressing from analysis to communication: explore tools and technologies for documenting interactive data analysis processes with integrity – balancing claims to knowledge with informational complexity."
  },
  {
    "objectID": "challenge/index.html#the-challenge",
    "href": "challenge/index.html#the-challenge",
    "title": "Challenge Statement",
    "section": "",
    "text": "An expanding array of observational data now enables social, economic and environmental behaviours to be researched in a large-scale and empirical way. This is exciting as there are increasing opportunities to effect evidence-based decision making in science, government and industry. In order to generate insights and draw conclusions, however, such data-intensive research relies on reasonably informal, interactive data analysis approaches: those that combine data graphics and statistics via computational notebooks (Observable, Quarto, Jupyter) or interactive data analysis tools such as Tableau and PowerBI. Unlike traditional scientific processes, we don’t know what an optimal and rigorous interactive analysis should look like and their success more often than not relies on the expertise and skills of the data scientist.\nOur proposed Theory & Methods Challenge will develop proposals for conducting and reporting data-intensive research in a more formal way. Our hope is that this activity will lay the foundations for interactive data analysis practices and provide guidelines for computational tools that underpin the next generation of analysis platforms. Through the workshop, we aim to explore three sub-challenges:\n\nModelling paradigms for data-driven science: establish what is distinctive about modelling in data-driven science by mapping out archetypal data-driven projects and the analysis practices they use.\nInference and replicability in data-driven science: develop systematic ways of documenting the context under which analytical findings are made – a grammar for structuring exploratory research findings – so that inferences can be more formally reported.\nTools for progressing from analysis to communication: explore tools and technologies for documenting interactive data analysis processes with integrity – balancing claims to knowledge with informational complexity."
  },
  {
    "objectID": "challenge/index.html#outcomes-and-potential-for-impact",
    "href": "challenge/index.html#outcomes-and-potential-for-impact",
    "title": "Challenge Statement",
    "section": "Outcomes and potential for impact",
    "text": "Outcomes and potential for impact\nThe Challenge will aim for the following outcomes:\n\nFoundations for theoretically-guided, transparent and replicable interactive data analysis, leading to more rigorous data-driven science.\nA systematic framework – a grammar – for describing interactive data analysis and the context under which inferences are made.\nGuidelines for designing new generation data analysis tools and technologies that operationalise more formal, interactive data analysis practice.\n\nWe anticipate impact on the following:\n\nAdvancing the science of data science – data science increasingly requires better established, formal forms of working. Our findings will contribute to the theoretical literature on how data-driven research could better respond to advances in data science and AI technologies.\nEcologically-valid, justified and plausible data-driven decisions – our work will support better documented analytical choices, e.g. model/assumptions that are justified and recorded along with contextual, background and researcher tacit knowledge. Such forms of practice will advance how data-driven artefacts can inform decisions and policies.\nTools and technologies for transparent, rigorous and reproducible data analysis practices – we will develop blueprints for new generations of analytical tools for rigorous analysis, from hypothesis generation through to modelling and communication."
  },
  {
    "objectID": "challenge/index.html#open-practices-and-dissemination",
    "href": "challenge/index.html#open-practices-and-dissemination",
    "title": "Challenge Statement",
    "section": "Open practices and dissemination",
    "text": "Open practices and dissemination\nWe will document the activities, discussions and outcomes in openly available and transparent channels. All the discussion and working notes will be collated on a github repository, with blog and other items appearing on these webpages providing an accessible “front-end” to our work."
  },
  {
    "objectID": "challenge/inference/index.html",
    "href": "challenge/inference/index.html",
    "title": "Inference",
    "section": "",
    "text": "A familiar challenge in data-driven science is that of inference. Generalising away from a sample to some target population is challenging when datasets are already large and population-level and observations subject to unknown confounding context. Replication is muddled by the number of analysis choices that are to be made and range of alternatives that could be pursued (the forking paths problem). How, then, can we ensure that the inferences and claims we make from exploratory analyses are properly contextualised?\nDrawing on expertise in Statistics, Human Computer Interaction and Spatial Econometrics, we will explore ways of systematically describing context in interactive data analysis. An outcome of this work may be to formulate a grammar for structuring exploratory research findings. For a stated finding, the grammar would require researchers to identify, using theory and prior knowledge: confounding context that is present/absent in their analysis, the spatiotemporal scale at which their stated finding is observed and how it may generalise outside of this; the weight of evidence in support of some theory implied by a finding relative to other plausible theories."
  },
  {
    "objectID": "challenge/inference/index.html#inference-and-replicability-in-data-driven-science-capturing-context",
    "href": "challenge/inference/index.html#inference-and-replicability-in-data-driven-science-capturing-context",
    "title": "Inference",
    "section": "",
    "text": "A familiar challenge in data-driven science is that of inference. Generalising away from a sample to some target population is challenging when datasets are already large and population-level and observations subject to unknown confounding context. Replication is muddled by the number of analysis choices that are to be made and range of alternatives that could be pursued (the forking paths problem). How, then, can we ensure that the inferences and claims we make from exploratory analyses are properly contextualised?\nDrawing on expertise in Statistics, Human Computer Interaction and Spatial Econometrics, we will explore ways of systematically describing context in interactive data analysis. An outcome of this work may be to formulate a grammar for structuring exploratory research findings. For a stated finding, the grammar would require researchers to identify, using theory and prior knowledge: confounding context that is present/absent in their analysis, the spatiotemporal scale at which their stated finding is observed and how it may generalise outside of this; the weight of evidence in support of some theory implied by a finding relative to other plausible theories."
  },
  {
    "objectID": "challenge/inference/index.html#related-work-challenge-team",
    "href": "challenge/inference/index.html#related-work-challenge-team",
    "title": "Inference",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nBeecham, R. and Lovelace, R. (2023) A Framework for Inserting Visually Supported Inferences into Geographical Analysis Workflow: Application to Road Safety Research, Geographical Analysis, 55(3): 345–366.\nBeecham, R., J. Dykes, W. Meulemans, A. Slingsby, C. Turkay, and J. Wood. (2017). “Map LineUps: Effects of Spatial Structure on Graphical Inference.” IEEE Transactions on Visualization and Computer Graphics, 23: 391–400.\nCook, D., Reid, N., & Tanaka, E. (2021) The Foundation Is Available for Thinking About Data Visualization Inferentially. Harvard Data Science Review, 3(3).\nFranklin, R. (2023) Quantitative methods II: Big theory. Progress in Human Geography, 47(1), 178-186.\nHullman, J., P. Resnick, and E. Adar. (2015). “Hypothetical Outcome Plots Outperform Error Bars And Violin Plots for Inferences About Reliability of Variable Ordering.” PLoS One, 10: e0142444.\nKale, A., Wu, Y. and Hullman, J. (2022) “Causal Support: Modeling Causal Inferences with Visualization”, IEEE Transactions on Visualization and Computer Graphics, 28(1): 1150-1160.\nH. Wickham, D. Cook, H. Hofmann and A. Buja (2010) “Graphical inference for infovis,” IEEE Transactions on Visualization and Computer Graphics, 16(6): 973–979.\nWolf, L.J. et al. (2021) Quantitative geography III: Future challenges and challenging futures. Progress in Human Geography, 45(3): 596–608.\nZhang, J. and Wolf, L. (2023) Rethinking “causality” in Quantitative Human Geography. doi: 10.31235/osf.io/7yncm."
  },
  {
    "objectID": "challenge/tools/index.html",
    "href": "challenge/tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Interactive data analysis has been made possible by coding ecosystems, such as tidyverse in R and pandas in Python, that support iterative dialogue between data and models. The analysis process leaves behind a diverse trail of pathways explored, decisions made, failed attempts and also conclusions that satisfactory outcomes have been reached, i.e., when to stop an exploration? How can these analysis trails be documented, how can the provenance of these processes be recorded and serve as evidence and eventually as communicative artefacts?\nThis theme will focus on tools and technologies through which exploratory data analysis workflows can be documented, communicated and shared. Here we will explore the space of computational notebook environments – how they can be used to record and communicate interactive data analysis processes. Drawing on work from themes #1 and #2 we will identify examples of interactive data analyses, documented via computational notebooks, that balance complexity of information with claims to knowledge."
  },
  {
    "objectID": "challenge/tools/index.html#from-analysis-to-communication-tools-for-data-driven-science-with-integrity",
    "href": "challenge/tools/index.html#from-analysis-to-communication-tools-for-data-driven-science-with-integrity",
    "title": "Tools",
    "section": "",
    "text": "Interactive data analysis has been made possible by coding ecosystems, such as tidyverse in R and pandas in Python, that support iterative dialogue between data and models. The analysis process leaves behind a diverse trail of pathways explored, decisions made, failed attempts and also conclusions that satisfactory outcomes have been reached, i.e., when to stop an exploration? How can these analysis trails be documented, how can the provenance of these processes be recorded and serve as evidence and eventually as communicative artefacts?\nThis theme will focus on tools and technologies through which exploratory data analysis workflows can be documented, communicated and shared. Here we will explore the space of computational notebook environments – how they can be used to record and communicate interactive data analysis processes. Drawing on work from themes #1 and #2 we will identify examples of interactive data analyses, documented via computational notebooks, that balance complexity of information with claims to knowledge."
  },
  {
    "objectID": "challenge/tools/index.html#related-work-challenge-team",
    "href": "challenge/tools/index.html#related-work-challenge-team",
    "title": "Tools",
    "section": "Related work (challenge team)",
    "text": "Related work (challenge team)\n\nPu, X. and Kay, M. (2018) The garden of forking paths in visualization: A design space for reliable exploratory visual analytics: Position paper. In 2018 IEEE Evaluation and Beyond-Methodological Approaches for Visualization (beliv).\nTurkay, C. et al. (2017) Supporting theoretically-grounded model building in the social sciences through interactive visualisation, Neurocomputing, 268:153–163.\nWickham, H. and Grolemund, G. (2019) R for Data Science, Orielly.\nWood, J., Kachkaev, A. and Dykes, J. (2018) Design exposition with literate visualization. IEEE Transactions on Visualization and Computer Graphics, 25(1): 751–768."
  },
  {
    "objectID": "lecture/index.html",
    "href": "lecture/index.html",
    "title": "Public Lecture",
    "section": "",
    "text": "The steady expansion in the availability and reach of observational data has prompted much-needed introspection into data analysis practice. When using data to answer questions, it is not simply a case of choosing from a set of appropriate statistical procedures, or ‘rolling out’ some research design template. Effective data analysis requires analysts to make decisions within a wide space of analysis options and to engage deeply with the processes and mechanisms being represented through data. This public lecture features three internationally-standout scientists from academia and industry. Talks will cover how to challenge and interrogate in data-based research; techniques for imagining uncertainty and variation in observational data; and how data-driven analyses can be put into production.\n\nDate/Time : Thursday 20th June, 1730-2100\nVenue : King’s College London\nDetails : Drinks reception following talks\n\n\n\n\n\n\n\n\nStandard paradigms for data-based decision making and policy analysis fail, and have led to a replication crisis in science, because they can’t handle uncertainty and variation and because they don’t seriously engage with the quality of evidence. We discuss how this has happened, touching on the piranha problem, the butterfly effect, the magic number 16, the one-way-street fallacy, the backpack fallacy, the Edlin factor, Clarke’s law, the analyst’s paradox, and the greatest trick the default ever pulled. We then discuss ways to go beyond the push-a-button, take-a-pill model to a more active engagement of data in science.\n\n\n\n\n\n\nAndrew Gelman is Professor of Statistics and Political Science at Columbia University. His research in applied statistics is wide-ranging within and beyond Political Science. He has received the Outstanding Statistical Application award three times from the American Statistical Association, the award for best article published in the American Political Science Review, the Mitchell and DeGroot prizes from the International Society of Bayesian Analysis, and the Council of Presidents of Statistical Societies award.\n\n\n\n\n\n\n\n\n\n\n\nLearning from data, whether in exploratory or confirmatory analysis settings, requires one to reason about the likelihood of many competing explanations. However, people are boundedly rational agents who often engage in pattern-finding at the expense of recognizing uncertainty or considering potential sources of heterogeneity and variation in the effects they seek to discover. Taking this seriously motivates new classes of interface tools that help people extend their imagination in hypothesizing and interpreting effects.\n\n\n\n\n\n\nJessica Hullman is Ginni Rometty Associate Professor of Computer Science at Northwestern University. Her research addresses challenges and limitations that arise when people draw inductive inferences from data. Her work has contributed visualization and interaction techniques decision-making and analysis, as well as theoretical frameworks for understanding the role of visualization in statistical workflow. Jessica’s work has been awarded best paper awards at top visualization and HCI venues, a Microsoft Faculty award, and NSF CAREER, Medium, and Small awards as PI, among others.\n\n\n\n\n\n\n\n\n\n\n\nThis talk will discuss what it means to put data science “in production”. In industry, any successful data science project will be run repeatedly for months or years, typically on a server that you can’t work with interactively. This poses an entirely new set of challenges that you won’t encounter in your classes in university, but are vital to overcome if you want to have an impact in your job.\nIn this talk, I’ll discuss three principles that I’ve found useful for understanding data science in production: not just once, not just my computer, and not just by myself. I’ll discuss the challenges associated with each, and where possible, what solutions (both technical and sociological) are currently available.\n\n\n\n\n\n\nHadley Wickham is Chief Scientist at Posit PBC, winner of the 2019 COPSS award, and a member of the R Foundation. He builds tools (both computational and cognitive) to make data science easier, faster, and more fun. His work includes packages for data science (like the tidyverse, which includes ggplot2, dplyr, and tidyr)and principled software development (e.g. roxygen2, testthat, and pkgdown). He is also a writer, educator, and speaker promoting the use of R for data science. Learn more on his website, &lt;http://hadley.nz&gt;."
  },
  {
    "objectID": "lecture/index.html#rigor-imagination-and-production-in-data-driven-science",
    "href": "lecture/index.html#rigor-imagination-and-production-in-data-driven-science",
    "title": "Public Lecture",
    "section": "",
    "text": "The steady expansion in the availability and reach of observational data has prompted much-needed introspection into data analysis practice. When using data to answer questions, it is not simply a case of choosing from a set of appropriate statistical procedures, or ‘rolling out’ some research design template. Effective data analysis requires analysts to make decisions within a wide space of analysis options and to engage deeply with the processes and mechanisms being represented through data. This public lecture features three internationally-standout scientists from academia and industry. Talks will cover how to challenge and interrogate in data-based research; techniques for imagining uncertainty and variation in observational data; and how data-driven analyses can be put into production.\n\nDate/Time : Thursday 20th June, 1730-2100\nVenue : King’s College London\nDetails : Drinks reception following talks\n\n\n\n\n\n\n\n\nStandard paradigms for data-based decision making and policy analysis fail, and have led to a replication crisis in science, because they can’t handle uncertainty and variation and because they don’t seriously engage with the quality of evidence. We discuss how this has happened, touching on the piranha problem, the butterfly effect, the magic number 16, the one-way-street fallacy, the backpack fallacy, the Edlin factor, Clarke’s law, the analyst’s paradox, and the greatest trick the default ever pulled. We then discuss ways to go beyond the push-a-button, take-a-pill model to a more active engagement of data in science.\n\n\n\n\n\n\nAndrew Gelman is Professor of Statistics and Political Science at Columbia University. His research in applied statistics is wide-ranging within and beyond Political Science. He has received the Outstanding Statistical Application award three times from the American Statistical Association, the award for best article published in the American Political Science Review, the Mitchell and DeGroot prizes from the International Society of Bayesian Analysis, and the Council of Presidents of Statistical Societies award.\n\n\n\n\n\n\n\n\n\n\n\nLearning from data, whether in exploratory or confirmatory analysis settings, requires one to reason about the likelihood of many competing explanations. However, people are boundedly rational agents who often engage in pattern-finding at the expense of recognizing uncertainty or considering potential sources of heterogeneity and variation in the effects they seek to discover. Taking this seriously motivates new classes of interface tools that help people extend their imagination in hypothesizing and interpreting effects.\n\n\n\n\n\n\nJessica Hullman is Ginni Rometty Associate Professor of Computer Science at Northwestern University. Her research addresses challenges and limitations that arise when people draw inductive inferences from data. Her work has contributed visualization and interaction techniques decision-making and analysis, as well as theoretical frameworks for understanding the role of visualization in statistical workflow. Jessica’s work has been awarded best paper awards at top visualization and HCI venues, a Microsoft Faculty award, and NSF CAREER, Medium, and Small awards as PI, among others.\n\n\n\n\n\n\n\n\n\n\n\nThis talk will discuss what it means to put data science “in production”. In industry, any successful data science project will be run repeatedly for months or years, typically on a server that you can’t work with interactively. This poses an entirely new set of challenges that you won’t encounter in your classes in university, but are vital to overcome if you want to have an impact in your job.\nIn this talk, I’ll discuss three principles that I’ve found useful for understanding data science in production: not just once, not just my computer, and not just by myself. I’ll discuss the challenges associated with each, and where possible, what solutions (both technical and sociological) are currently available.\n\n\n\n\n\n\nHadley Wickham is Chief Scientist at Posit PBC, winner of the 2019 COPSS award, and a member of the R Foundation. He builds tools (both computational and cognitive) to make data science easier, faster, and more fun. His work includes packages for data science (like the tidyverse, which includes ggplot2, dplyr, and tidyr)and principled software development (e.g. roxygen2, testthat, and pkgdown). He is also a writer, educator, and speaker promoting the use of R for data science. Learn more on his website, &lt;http://hadley.nz&gt;."
  },
  {
    "objectID": "team/index.html",
    "href": "team/index.html",
    "title": "Team",
    "section": "",
    "text": "We have assembled an interdisciplinary team of 12 world-leaders working in Information Visualization, Statistical Modelling, Human Computer Interaction, Communication Studies and Geography.\nChallenge Leads:\n\nCagatay Turkay, Professor of Visual Data Science, Centre for Interdisciplinary Methodologies, University of Warwick\nRoger Beecham, Associate Professor of Visual Data Science, School of Geography, University of Leeds\n\nChallenge Team:\n\nDaniel Archambault, Professor of Data Science/Visualisation, School of Computing, Newcastle University\nBenjamin Bach, Reader in Design Informatics and Visualization, School of Informatics, University of Edingburgh\nDi Cook, Professor of Business Analytics, Econometrics and Business Statistics, Monash University\nRachel Franklin, Professor of Geographical Analysis, School of Geography, Politics and Sociology, Newcastle University\nAndrew Gelman, Professor of Statistics and Political Science, Department of Statistics, Columbia University\nJessica Hullman, Ginni Rometty Associate Professor, Computer Science and Communication Studies, Northwestern University\nEdwin Pos, Associate Professor, Utrecht University\nMari-Cruz Villa-Uriol, Senior Lecturer, University of Sheffield\nHadley Wickham, Chief Scientist, Posit\nLevi Wolf, Senior Lecturer in Quantitative Human Geography, School of Geographical Sciences, University of Bristol\nJo Wood, Professor of Visual Analytics, Department of Computing, City, University of London\nKai Xu, Associate Professor, School of Computer Science, University of Nottingham"
  }
]